{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae268c2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f3188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msttaseen\u001b[0m (\u001b[33mcares\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sadat/Desktop/csn-sign-autoencoder/wandb/run-20230202_155359-25khgkge</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cares/autoencoder-experiments/runs/25khgkge\" target=\"_blank\">wlasl</a></strong> to <a href=\"https://wandb.ai/cares/autoencoder-experiments\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/cares/autoencoder-experiments/runs/25khgkge?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f01f2cf82e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from custom_dataset import VideoFrameDataset, ImglistToTensor\n",
    "from cls_head import ClassifierHead\n",
    "from pytorch_csn import create_csn\n",
    "\n",
    "wandb.init(entity=\"cares\", project=\"autoencoder-experiments\", group=\"cls-pytorch\", name=\"wlasl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1cad2",
   "metadata": {},
   "source": [
    "## Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1125627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "except:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e41de",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d97582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_root = os.path.join(os.getcwd(), 'data/wlasl/rawframes') \n",
    "ann_file_train = os.path.join(os.getcwd(), 'data/wlasl/train_annotations.txt') \n",
    "ann_file_test = os.path.join(os.getcwd(), 'data/wlasl/test_annotations.txt')\n",
    "work_dir = 'work_dirs/wlasl/sampleframes/'\n",
    "batch_size = 3\n",
    "\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Setting up data augments\n",
    "train_pipeline = transforms.Compose([\n",
    "        ImglistToTensor(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop((256, 256), scale=(0.6, 1.0)),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375]),\n",
    "    ])\n",
    "\n",
    "test_pipeline = transforms.Compose([\n",
    "        ImglistToTensor(), # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        transforms.Resize((256, 256)),  # image batch, resize smaller edge to 256\n",
    "        transforms.CenterCrop((224, 224)),  # image batch, center crop to square 224x224\n",
    "        transforms.Normalize(mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375]),\n",
    "    ])\n",
    "\n",
    "# Setting up datasets\n",
    "train_dataset = VideoFrameDataset(\n",
    "    root_path=data_root,\n",
    "    annotationfile_path=ann_file_train,\n",
    "    clip_len=32,\n",
    "    frame_interval=2,\n",
    "    num_clips=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=train_pipeline,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "test_dataset = VideoFrameDataset(\n",
    "    root_path=data_root,\n",
    "    annotationfile_path=ann_file_test,\n",
    "    clip_len=32,\n",
    "    frame_interval=2,\n",
    "    num_clips=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=test_pipeline,\n",
    "    test_mode=True\n",
    ")\n",
    "\n",
    "# Setting up dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=4,\n",
    "                                    pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=4,\n",
    "                                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b86989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing\n",
    "# dataiter = iter(test_loader)\n",
    "# get = next(dataiter)\n",
    "# reshape = get[0].permute(0,2,1,3,4)\n",
    "# video = iter(reshape[0][0])\n",
    "# plt.imshow(next(video))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9152cae",
   "metadata": {},
   "source": [
    "## Set up model, loss and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3aeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder = create_csn(input_channel=3,\n",
    "                   model_depth=50,\n",
    "                   model_num_class=400,\n",
    "                   dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3d2b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        code = self.encoder(x)\n",
    "        return self.decoder(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b4d4efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create a CSN model\n",
    "# encoder = ResNet3dCSN(\n",
    "#     pretrained2d=False,\n",
    "#     pretrained='https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth',\n",
    "#     depth=50,\n",
    "#     with_pool2=False,\n",
    "#     bottleneck_mode='ir',\n",
    "#     norm_eval=True,\n",
    "#     zero_init_residual=False,\n",
    "#     bn_frozen=True\n",
    "# )\n",
    "# encoder.init_weights()\n",
    "\n",
    "# decoder = ClassifierHead()\n",
    "# decoder.init_weights()\n",
    "\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.000025, momentum=0.9, weight_decay=0)\n",
    "\n",
    "# Specify learning rate scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75,130])\n",
    "\n",
    "# Setup wandb\n",
    "wandb.watch(model, log_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a511b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints = torch.load(work_dir+'latest.pth')\n",
    "# model.load_state_dict(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00bed402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(scores, labels, topk=(1, )):\n",
    "    \"\"\"Calculate top k accuracy score.\n",
    "    Args:\n",
    "        scores (list[np.ndarray]): Prediction scores for each class.\n",
    "        labels (list[int]): Ground truth labels.\n",
    "        topk (tuple[int]): K value for top_k_accuracy. Default: (1, ).\n",
    "    Returns:\n",
    "        list[float]: Top k accuracy score for each k.\n",
    "    \"\"\"\n",
    "    res = np.zeros(len(topk))\n",
    "    labels = np.array(labels)[:, np.newaxis]\n",
    "    for i, k in enumerate(topk):\n",
    "        max_k_preds = np.argsort(scores, axis=1)[:, -k:][:, ::-1]\n",
    "        match_array = np.logical_or.reduce(max_k_preds == labels, axis=1)\n",
    "        topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "        res[i] = topk_acc_score\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff077de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, interval=5):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        images, targets = images.to(device).permute(0,2,1,3,4), targets.to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=40, norm_type=2.0)\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         lr: {scheduler.get_last_lr()[0]:.5e}\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % interval == interval-1:\n",
    "            last_loss = running_loss / interval # loss per batch     \n",
    "            print(f'Epoch [{epoch_index}][{i+1}/{len(train_loader)}], loss: {last_loss:.5}')\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7db16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    running_vloss = 0.0\n",
    "    running_vacc = np.zeros(2)\n",
    "    \n",
    "    print('Evaluating top_k_accuracy...')\n",
    "    \n",
    "    with torch.inference_mode():   \n",
    "        for i, (vimages, vtargets) in enumerate(test_loader):\n",
    "            vimages, vtargets = vimages.to(device), vtargets.to(device)\n",
    "            \n",
    "            voutputs = model(vimages.permute(0,2,1,3,4))\n",
    "            \n",
    "            vloss = loss_fn(voutputs, vtargets)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            running_vacc += top_k_accuracy(voutputs.detach().cpu().numpy(), vtargets.detach().cpu().numpy(), topk=(1,5))\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "\n",
    "    acc = running_vacc/len(test_loader)\n",
    "    top1_acc = acc[0].item()\n",
    "    top5_acc = acc[1].item()\n",
    "    \n",
    "    return (avg_vloss, top1_acc, top5_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eb60833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1][5/594], loss: 6.0058\n",
      "Epoch [1][10/594], loss: 5.9491\n",
      "Epoch [1][15/594], loss: 5.9023\n",
      "Epoch [1][20/594], loss: 5.9649\n",
      "Epoch [1][25/594], loss: 5.9265\n",
      "Epoch [1][30/594], loss: 6.0411\n",
      "Epoch [1][35/594], loss: 5.9891\n",
      "Epoch [1][40/594], loss: 6.0152\n",
      "Epoch [1][45/594], loss: 5.9568\n",
      "Epoch [1][50/594], loss: 6.0365\n",
      "Epoch [1][55/594], loss: 5.9752\n",
      "Epoch [1][60/594], loss: 6.0357\n",
      "Epoch [1][65/594], loss: 5.9352\n",
      "Epoch [1][70/594], loss: 5.9114\n",
      "Epoch [1][75/594], loss: 5.9198\n",
      "Epoch [1][80/594], loss: 5.973\n",
      "Epoch [1][85/594], loss: 6.013\n",
      "Epoch [1][90/594], loss: 5.8491\n",
      "Epoch [1][95/594], loss: 5.9884\n",
      "Epoch [1][100/594], loss: 6.0162\n",
      "Epoch [1][105/594], loss: 5.9667\n",
      "Epoch [1][110/594], loss: 5.9901\n",
      "Epoch [1][115/594], loss: 5.9462\n",
      "Epoch [1][120/594], loss: 5.9734\n",
      "Epoch [1][125/594], loss: 5.9203\n",
      "Epoch [1][130/594], loss: 5.9957\n",
      "Epoch [1][135/594], loss: 5.9638\n",
      "Epoch [1][140/594], loss: 5.954\n",
      "Epoch [1][145/594], loss: 5.935\n",
      "Epoch [1][150/594], loss: 5.9567\n",
      "Epoch [1][155/594], loss: 5.8664\n",
      "Epoch [1][160/594], loss: 5.9082\n",
      "Epoch [1][165/594], loss: 5.8486\n",
      "Epoch [1][170/594], loss: 5.917\n",
      "Epoch [1][175/594], loss: 5.9153\n",
      "Epoch [1][180/594], loss: 5.8564\n",
      "Epoch [1][185/594], loss: 5.9963\n",
      "Epoch [1][190/594], loss: 5.9179\n",
      "Epoch [1][195/594], loss: 5.8884\n",
      "Epoch [1][200/594], loss: 6.0096\n",
      "Epoch [1][205/594], loss: 5.9675\n",
      "Epoch [1][210/594], loss: 6.003\n",
      "Epoch [1][215/594], loss: 5.9332\n",
      "Epoch [1][220/594], loss: 5.9196\n",
      "Epoch [1][225/594], loss: 5.878\n",
      "Epoch [1][230/594], loss: 5.7931\n",
      "Epoch [1][235/594], loss: 5.9142\n",
      "Epoch [1][240/594], loss: 5.8443\n",
      "Epoch [1][245/594], loss: 5.8178\n",
      "Epoch [1][250/594], loss: 5.9053\n",
      "Epoch [1][255/594], loss: 5.9258\n",
      "Epoch [1][260/594], loss: 5.8855\n",
      "Epoch [1][265/594], loss: 5.8615\n",
      "Epoch [1][270/594], loss: 5.9343\n",
      "Epoch [1][275/594], loss: 5.8538\n",
      "Epoch [1][280/594], loss: 5.8822\n",
      "Epoch [1][285/594], loss: 5.8617\n",
      "Epoch [1][290/594], loss: 5.8811\n",
      "Epoch [1][295/594], loss: 5.7921\n",
      "Epoch [1][300/594], loss: 5.849\n",
      "Epoch [1][305/594], loss: 5.9341\n",
      "Epoch [1][310/594], loss: 5.928\n",
      "Epoch [1][315/594], loss: 5.8209\n",
      "Epoch [1][320/594], loss: 5.8714\n",
      "Epoch [1][325/594], loss: 5.9228\n",
      "Epoch [1][330/594], loss: 5.8781\n",
      "Epoch [1][335/594], loss: 5.8535\n",
      "Epoch [1][340/594], loss: 5.8646\n",
      "Epoch [1][345/594], loss: 5.8248\n",
      "Epoch [1][350/594], loss: 5.8315\n",
      "Epoch [1][355/594], loss: 5.8848\n",
      "Epoch [1][360/594], loss: 5.8128\n",
      "Epoch [1][365/594], loss: 5.7882\n",
      "Epoch [1][370/594], loss: 5.8675\n",
      "Epoch [1][375/594], loss: 5.7605\n",
      "Epoch [1][380/594], loss: 5.8989\n",
      "Epoch [1][385/594], loss: 5.731\n",
      "Epoch [1][390/594], loss: 5.8375\n",
      "Epoch [1][395/594], loss: 5.8922\n",
      "Epoch [1][400/594], loss: 5.8479\n",
      "Epoch [1][405/594], loss: 5.8559\n",
      "Epoch [1][410/594], loss: 5.9618\n",
      "Epoch [1][415/594], loss: 5.8686\n",
      "Epoch [1][420/594], loss: 5.7987\n",
      "Epoch [1][425/594], loss: 5.8479\n",
      "Epoch [1][430/594], loss: 5.8645\n",
      "Epoch [1][435/594], loss: 5.792\n",
      "Epoch [1][440/594], loss: 5.7881\n",
      "Epoch [1][445/594], loss: 5.7965\n",
      "Epoch [1][450/594], loss: 5.78\n",
      "Epoch [1][455/594], loss: 5.8877\n",
      "Epoch [1][460/594], loss: 5.8343\n",
      "Epoch [1][465/594], loss: 5.8822\n",
      "Epoch [1][470/594], loss: 5.7504\n",
      "Epoch [1][475/594], loss: 5.7855\n",
      "Epoch [1][480/594], loss: 5.7727\n",
      "Epoch [1][485/594], loss: 5.8138\n",
      "Epoch [1][490/594], loss: 5.8873\n",
      "Epoch [1][495/594], loss: 5.8398\n",
      "Epoch [1][500/594], loss: 5.8482\n",
      "Epoch [1][505/594], loss: 5.7969\n",
      "Epoch [1][510/594], loss: 5.7537\n",
      "Epoch [1][515/594], loss: 5.771\n",
      "Epoch [1][520/594], loss: 5.7683\n",
      "Epoch [1][525/594], loss: 5.778\n",
      "Epoch [1][530/594], loss: 5.6829\n",
      "Epoch [1][535/594], loss: 5.6999\n",
      "Epoch [1][540/594], loss: 5.8526\n",
      "Epoch [1][545/594], loss: 5.8631\n",
      "Epoch [1][550/594], loss: 5.706\n",
      "Epoch [1][555/594], loss: 5.7412\n",
      "Epoch [1][560/594], loss: 5.7349\n",
      "Epoch [1][565/594], loss: 5.682\n",
      "Epoch [1][570/594], loss: 5.7712\n",
      "Epoch [1][575/594], loss: 5.7194\n",
      "Epoch [1][580/594], loss: 5.8484\n",
      "Epoch [1][585/594], loss: 5.7648\n",
      "Epoch [1][590/594], loss: 5.8334\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.007752, top5_acc: 0.04264, train_loss: 5.8334, val_loss: 5.7867\n",
      "Saving checkpoint at 1 epochs...\n",
      "Epoch [2][5/594], loss: 5.7514\n",
      "Epoch [2][10/594], loss: 5.7483\n",
      "Epoch [2][15/594], loss: 5.8564\n",
      "Epoch [2][20/594], loss: 5.723\n",
      "Epoch [2][25/594], loss: 5.6946\n",
      "Epoch [2][30/594], loss: 5.6999\n",
      "Epoch [2][35/594], loss: 5.8077\n",
      "Epoch [2][40/594], loss: 5.6942\n",
      "Epoch [2][45/594], loss: 5.7864\n",
      "Epoch [2][50/594], loss: 5.6936\n",
      "Epoch [2][55/594], loss: 5.7465\n",
      "Epoch [2][60/594], loss: 5.5966\n",
      "Epoch [2][65/594], loss: 5.7219\n",
      "Epoch [2][70/594], loss: 5.651\n",
      "Epoch [2][75/594], loss: 5.7764\n",
      "Epoch [2][80/594], loss: 5.7201\n",
      "Epoch [2][85/594], loss: 5.697\n",
      "Epoch [2][90/594], loss: 5.7419\n",
      "Epoch [2][95/594], loss: 5.6866\n",
      "Epoch [2][100/594], loss: 5.6556\n",
      "Epoch [2][105/594], loss: 5.7144\n",
      "Epoch [2][110/594], loss: 5.6876\n",
      "Epoch [2][115/594], loss: 5.7418\n",
      "Epoch [2][120/594], loss: 5.6955\n",
      "Epoch [2][125/594], loss: 5.6863\n",
      "Epoch [2][130/594], loss: 5.7485\n",
      "Epoch [2][135/594], loss: 5.842\n",
      "Epoch [2][140/594], loss: 5.6817\n",
      "Epoch [2][145/594], loss: 5.6685\n",
      "Epoch [2][150/594], loss: 5.8344\n",
      "Epoch [2][155/594], loss: 5.6994\n",
      "Epoch [2][160/594], loss: 5.7029\n",
      "Epoch [2][165/594], loss: 5.6433\n",
      "Epoch [2][170/594], loss: 5.6509\n",
      "Epoch [2][175/594], loss: 5.7417\n",
      "Epoch [2][180/594], loss: 5.6322\n",
      "Epoch [2][185/594], loss: 5.6413\n",
      "Epoch [2][190/594], loss: 5.6495\n",
      "Epoch [2][195/594], loss: 5.5806\n",
      "Epoch [2][200/594], loss: 5.7531\n",
      "Epoch [2][205/594], loss: 5.7575\n",
      "Epoch [2][210/594], loss: 5.6585\n",
      "Epoch [2][215/594], loss: 5.7088\n",
      "Epoch [2][220/594], loss: 5.7963\n",
      "Epoch [2][225/594], loss: 5.6844\n",
      "Epoch [2][230/594], loss: 5.6143\n",
      "Epoch [2][235/594], loss: 5.6595\n",
      "Epoch [2][240/594], loss: 5.7041\n",
      "Epoch [2][245/594], loss: 5.6758\n",
      "Epoch [2][250/594], loss: 5.6448\n",
      "Epoch [2][255/594], loss: 5.6647\n",
      "Epoch [2][260/594], loss: 5.5544\n",
      "Epoch [2][265/594], loss: 5.6374\n",
      "Epoch [2][270/594], loss: 5.565\n",
      "Epoch [2][275/594], loss: 5.682\n",
      "Epoch [2][280/594], loss: 5.5767\n",
      "Epoch [2][285/594], loss: 5.6431\n",
      "Epoch [2][290/594], loss: 5.5999\n",
      "Epoch [2][295/594], loss: 5.5959\n",
      "Epoch [2][300/594], loss: 5.6232\n",
      "Epoch [2][305/594], loss: 5.7072\n",
      "Epoch [2][310/594], loss: 5.6011\n",
      "Epoch [2][315/594], loss: 5.6569\n",
      "Epoch [2][320/594], loss: 5.6502\n",
      "Epoch [2][325/594], loss: 5.6061\n",
      "Epoch [2][330/594], loss: 5.649\n",
      "Epoch [2][335/594], loss: 5.6774\n",
      "Epoch [2][340/594], loss: 5.6242\n",
      "Epoch [2][345/594], loss: 5.604\n",
      "Epoch [2][350/594], loss: 5.5929\n",
      "Epoch [2][355/594], loss: 5.6918\n",
      "Epoch [2][360/594], loss: 5.7154\n",
      "Epoch [2][365/594], loss: 5.681\n",
      "Epoch [2][370/594], loss: 5.6516\n",
      "Epoch [2][375/594], loss: 5.5659\n",
      "Epoch [2][380/594], loss: 5.5975\n",
      "Epoch [2][385/594], loss: 5.6155\n",
      "Epoch [2][390/594], loss: 5.6209\n",
      "Epoch [2][395/594], loss: 5.5318\n",
      "Epoch [2][400/594], loss: 5.6349\n",
      "Epoch [2][405/594], loss: 5.4825\n",
      "Epoch [2][410/594], loss: 5.5805\n",
      "Epoch [2][415/594], loss: 5.6955\n",
      "Epoch [2][420/594], loss: 5.5263\n",
      "Epoch [2][425/594], loss: 5.622\n",
      "Epoch [2][430/594], loss: 5.6835\n",
      "Epoch [2][435/594], loss: 5.6554\n",
      "Epoch [2][440/594], loss: 5.7186\n",
      "Epoch [2][445/594], loss: 5.6659\n",
      "Epoch [2][450/594], loss: 5.547\n",
      "Epoch [2][455/594], loss: 5.6064\n",
      "Epoch [2][460/594], loss: 5.6386\n",
      "Epoch [2][465/594], loss: 5.5353\n",
      "Epoch [2][470/594], loss: 5.6327\n",
      "Epoch [2][475/594], loss: 5.4785\n",
      "Epoch [2][480/594], loss: 5.5298\n",
      "Epoch [2][485/594], loss: 5.5764\n",
      "Epoch [2][490/594], loss: 5.6152\n",
      "Epoch [2][495/594], loss: 5.6004\n",
      "Epoch [2][500/594], loss: 5.5148\n",
      "Epoch [2][505/594], loss: 5.5822\n",
      "Epoch [2][510/594], loss: 5.5716\n",
      "Epoch [2][515/594], loss: 5.5328\n",
      "Epoch [2][520/594], loss: 5.7009\n",
      "Epoch [2][525/594], loss: 5.4827\n",
      "Epoch [2][530/594], loss: 5.539\n",
      "Epoch [2][535/594], loss: 5.5216\n",
      "Epoch [2][540/594], loss: 5.5832\n",
      "Epoch [2][545/594], loss: 5.6198\n",
      "Epoch [2][550/594], loss: 5.4947\n",
      "Epoch [2][555/594], loss: 5.6213\n",
      "Epoch [2][560/594], loss: 5.5859\n",
      "Epoch [2][565/594], loss: 5.6031\n",
      "Epoch [2][570/594], loss: 5.5772\n",
      "Epoch [2][575/594], loss: 5.5017\n",
      "Epoch [2][580/594], loss: 5.6557\n",
      "Epoch [2][585/594], loss: 5.6413\n",
      "Epoch [2][590/594], loss: 5.4722\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.06589, train_loss: 5.4722, val_loss: 5.5537\n",
      "Saving checkpoint at 2 epochs...\n",
      "Epoch [3][5/594], loss: 5.5763\n",
      "Epoch [3][10/594], loss: 5.3905\n",
      "Epoch [3][15/594], loss: 5.549\n",
      "Epoch [3][20/594], loss: 5.479\n",
      "Epoch [3][25/594], loss: 5.4059\n",
      "Epoch [3][30/594], loss: 5.5546\n",
      "Epoch [3][35/594], loss: 5.465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3][40/594], loss: 5.5579\n",
      "Epoch [3][45/594], loss: 5.5999\n",
      "Epoch [3][50/594], loss: 5.4887\n",
      "Epoch [3][55/594], loss: 5.5633\n",
      "Epoch [3][60/594], loss: 5.4102\n",
      "Epoch [3][65/594], loss: 5.4436\n",
      "Epoch [3][70/594], loss: 5.5512\n",
      "Epoch [3][75/594], loss: 5.5037\n",
      "Epoch [3][80/594], loss: 5.5342\n",
      "Epoch [3][85/594], loss: 5.5074\n",
      "Epoch [3][90/594], loss: 5.4414\n",
      "Epoch [3][95/594], loss: 5.4988\n",
      "Epoch [3][100/594], loss: 5.491\n",
      "Epoch [3][105/594], loss: 5.4824\n",
      "Epoch [3][110/594], loss: 5.4786\n",
      "Epoch [3][115/594], loss: 5.536\n",
      "Epoch [3][120/594], loss: 5.5402\n",
      "Epoch [3][125/594], loss: 5.4989\n",
      "Epoch [3][130/594], loss: 5.5182\n",
      "Epoch [3][135/594], loss: 5.494\n",
      "Epoch [3][140/594], loss: 5.5525\n",
      "Epoch [3][145/594], loss: 5.5411\n",
      "Epoch [3][150/594], loss: 5.4337\n",
      "Epoch [3][155/594], loss: 5.4176\n",
      "Epoch [3][160/594], loss: 5.4631\n",
      "Epoch [3][165/594], loss: 5.4814\n",
      "Epoch [3][170/594], loss: 5.4941\n",
      "Epoch [3][175/594], loss: 5.4851\n",
      "Epoch [3][180/594], loss: 5.4867\n",
      "Epoch [3][185/594], loss: 5.5133\n",
      "Epoch [3][190/594], loss: 5.498\n",
      "Epoch [3][195/594], loss: 5.4284\n",
      "Epoch [3][200/594], loss: 5.4909\n",
      "Epoch [3][205/594], loss: 5.5522\n",
      "Epoch [3][210/594], loss: 5.5405\n",
      "Epoch [3][215/594], loss: 5.3787\n",
      "Epoch [3][220/594], loss: 5.5281\n",
      "Epoch [3][225/594], loss: 5.414\n",
      "Epoch [3][230/594], loss: 5.5344\n",
      "Epoch [3][235/594], loss: 5.4489\n",
      "Epoch [3][240/594], loss: 5.5467\n",
      "Epoch [3][245/594], loss: 5.4785\n",
      "Epoch [3][250/594], loss: 5.4615\n",
      "Epoch [3][255/594], loss: 5.4195\n",
      "Epoch [3][260/594], loss: 5.483\n",
      "Epoch [3][265/594], loss: 5.394\n",
      "Epoch [3][270/594], loss: 5.4106\n",
      "Epoch [3][275/594], loss: 5.3845\n",
      "Epoch [3][280/594], loss: 5.4672\n",
      "Epoch [3][285/594], loss: 5.4529\n",
      "Epoch [3][290/594], loss: 5.48\n",
      "Epoch [3][295/594], loss: 5.5701\n",
      "Epoch [3][300/594], loss: 5.3615\n",
      "Epoch [3][305/594], loss: 5.3583\n",
      "Epoch [3][310/594], loss: 5.4164\n",
      "Epoch [3][315/594], loss: 5.4052\n",
      "Epoch [3][320/594], loss: 5.4571\n",
      "Epoch [3][325/594], loss: 5.4056\n",
      "Epoch [3][330/594], loss: 5.4526\n",
      "Epoch [3][335/594], loss: 5.5271\n",
      "Epoch [3][340/594], loss: 5.4216\n",
      "Epoch [3][345/594], loss: 5.3756\n",
      "Epoch [3][350/594], loss: 5.4213\n",
      "Epoch [3][355/594], loss: 5.3808\n",
      "Epoch [3][360/594], loss: 5.4507\n",
      "Epoch [3][365/594], loss: 5.4466\n",
      "Epoch [3][370/594], loss: 5.3805\n",
      "Epoch [3][375/594], loss: 5.4713\n",
      "Epoch [3][380/594], loss: 5.4565\n",
      "Epoch [3][385/594], loss: 5.3965\n",
      "Epoch [3][390/594], loss: 5.3642\n",
      "Epoch [3][395/594], loss: 5.3517\n",
      "Epoch [3][400/594], loss: 5.4818\n",
      "Epoch [3][405/594], loss: 5.3978\n",
      "Epoch [3][410/594], loss: 5.4979\n",
      "Epoch [3][415/594], loss: 5.4391\n",
      "Epoch [3][420/594], loss: 5.3591\n",
      "Epoch [3][425/594], loss: 5.4492\n",
      "Epoch [3][430/594], loss: 5.4208\n",
      "Epoch [3][435/594], loss: 5.4368\n",
      "Epoch [3][440/594], loss: 5.4016\n",
      "Epoch [3][445/594], loss: 5.4216\n",
      "Epoch [3][450/594], loss: 5.4577\n",
      "Epoch [3][455/594], loss: 5.5268\n",
      "Epoch [3][460/594], loss: 5.4568\n",
      "Epoch [3][465/594], loss: 5.3626\n",
      "Epoch [3][470/594], loss: 5.4407\n",
      "Epoch [3][475/594], loss: 5.5475\n",
      "Epoch [3][480/594], loss: 5.3565\n",
      "Epoch [3][485/594], loss: 5.3654\n",
      "Epoch [3][490/594], loss: 5.396\n",
      "Epoch [3][495/594], loss: 5.3994\n",
      "Epoch [3][500/594], loss: 5.4058\n",
      "Epoch [3][505/594], loss: 5.2469\n",
      "Epoch [3][510/594], loss: 5.4571\n",
      "Epoch [3][515/594], loss: 5.3335\n",
      "Epoch [3][520/594], loss: 5.4197\n",
      "Epoch [3][525/594], loss: 5.3694\n",
      "Epoch [3][530/594], loss: 5.4204\n",
      "Epoch [3][535/594], loss: 5.3797\n",
      "Epoch [3][540/594], loss: 5.3344\n",
      "Epoch [3][545/594], loss: 5.4211\n",
      "Epoch [3][550/594], loss: 5.4242\n",
      "Epoch [3][555/594], loss: 5.3802\n",
      "Epoch [3][560/594], loss: 5.3813\n",
      "Epoch [3][565/594], loss: 5.2281\n",
      "Epoch [3][570/594], loss: 5.3992\n",
      "Epoch [3][575/594], loss: 5.4609\n",
      "Epoch [3][580/594], loss: 5.3158\n",
      "Epoch [3][585/594], loss: 5.4199\n",
      "Epoch [3][590/594], loss: 5.4164\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.06589, train_loss: 5.4164, val_loss: 5.3726\n",
      "Saving checkpoint at 3 epochs...\n",
      "Epoch [4][5/594], loss: 5.2806\n",
      "Epoch [4][10/594], loss: 5.3724\n",
      "Epoch [4][15/594], loss: 5.4408\n",
      "Epoch [4][20/594], loss: 5.3236\n",
      "Epoch [4][25/594], loss: 5.3301\n",
      "Epoch [4][30/594], loss: 5.396\n",
      "Epoch [4][35/594], loss: 5.3295\n",
      "Epoch [4][40/594], loss: 5.2637\n",
      "Epoch [4][45/594], loss: 5.4334\n",
      "Epoch [4][50/594], loss: 5.3342\n",
      "Epoch [4][55/594], loss: 5.358\n",
      "Epoch [4][60/594], loss: 5.2284\n",
      "Epoch [4][65/594], loss: 5.3824\n",
      "Epoch [4][70/594], loss: 5.2506\n",
      "Epoch [4][75/594], loss: 5.4073\n",
      "Epoch [4][80/594], loss: 5.3639\n",
      "Epoch [4][85/594], loss: 5.3013\n",
      "Epoch [4][90/594], loss: 5.2986\n",
      "Epoch [4][95/594], loss: 5.3736\n",
      "Epoch [4][100/594], loss: 5.3913\n",
      "Epoch [4][105/594], loss: 5.3031\n",
      "Epoch [4][110/594], loss: 5.438\n",
      "Epoch [4][115/594], loss: 5.2871\n",
      "Epoch [4][120/594], loss: 5.4504\n",
      "Epoch [4][125/594], loss: 5.2363\n",
      "Epoch [4][130/594], loss: 5.4099\n",
      "Epoch [4][135/594], loss: 5.2581\n",
      "Epoch [4][140/594], loss: 5.2248\n",
      "Epoch [4][145/594], loss: 5.4026\n",
      "Epoch [4][150/594], loss: 5.2756\n",
      "Epoch [4][155/594], loss: 5.2498\n",
      "Epoch [4][160/594], loss: 5.323\n",
      "Epoch [4][165/594], loss: 5.1501\n",
      "Epoch [4][170/594], loss: 5.4047\n",
      "Epoch [4][175/594], loss: 5.3286\n",
      "Epoch [4][180/594], loss: 5.2759\n",
      "Epoch [4][185/594], loss: 5.2675\n",
      "Epoch [4][190/594], loss: 5.3828\n",
      "Epoch [4][195/594], loss: 5.3322\n",
      "Epoch [4][200/594], loss: 5.2758\n",
      "Epoch [4][205/594], loss: 5.3127\n",
      "Epoch [4][210/594], loss: 5.2728\n",
      "Epoch [4][215/594], loss: 5.2935\n",
      "Epoch [4][220/594], loss: 5.3625\n",
      "Epoch [4][225/594], loss: 5.3607\n",
      "Epoch [4][230/594], loss: 5.3023\n",
      "Epoch [4][235/594], loss: 5.3703\n",
      "Epoch [4][240/594], loss: 5.3629\n",
      "Epoch [4][245/594], loss: 5.3427\n",
      "Epoch [4][250/594], loss: 5.3596\n",
      "Epoch [4][255/594], loss: 5.2652\n",
      "Epoch [4][260/594], loss: 5.3467\n",
      "Epoch [4][265/594], loss: 5.2649\n",
      "Epoch [4][270/594], loss: 5.2955\n",
      "Epoch [4][275/594], loss: 5.3311\n",
      "Epoch [4][280/594], loss: 5.3759\n",
      "Epoch [4][285/594], loss: 5.3246\n",
      "Epoch [4][290/594], loss: 5.2575\n",
      "Epoch [4][295/594], loss: 5.2181\n",
      "Epoch [4][300/594], loss: 5.3498\n",
      "Epoch [4][305/594], loss: 5.2869\n",
      "Epoch [4][310/594], loss: 5.1679\n",
      "Epoch [4][315/594], loss: 5.1885\n",
      "Epoch [4][320/594], loss: 5.3695\n",
      "Epoch [4][325/594], loss: 5.2805\n",
      "Epoch [4][330/594], loss: 5.2594\n",
      "Epoch [4][335/594], loss: 5.3153\n",
      "Epoch [4][340/594], loss: 5.1707\n",
      "Epoch [4][345/594], loss: 5.1963\n",
      "Epoch [4][350/594], loss: 5.3204\n",
      "Epoch [4][355/594], loss: 5.3065\n",
      "Epoch [4][360/594], loss: 5.2634\n",
      "Epoch [4][365/594], loss: 5.3115\n",
      "Epoch [4][370/594], loss: 5.3271\n",
      "Epoch [4][375/594], loss: 5.2551\n",
      "Epoch [4][380/594], loss: 5.2788\n",
      "Epoch [4][385/594], loss: 5.2325\n",
      "Epoch [4][390/594], loss: 5.3187\n",
      "Epoch [4][395/594], loss: 5.3226\n",
      "Epoch [4][400/594], loss: 5.1324\n",
      "Epoch [4][405/594], loss: 5.283\n",
      "Epoch [4][410/594], loss: 5.2072\n",
      "Epoch [4][415/594], loss: 5.1202\n",
      "Epoch [4][420/594], loss: 5.3758\n",
      "Epoch [4][425/594], loss: 5.2508\n",
      "Epoch [4][430/594], loss: 5.2965\n",
      "Epoch [4][435/594], loss: 5.3225\n",
      "Epoch [4][440/594], loss: 5.1568\n",
      "Epoch [4][445/594], loss: 5.2927\n",
      "Epoch [4][450/594], loss: 5.2811\n",
      "Epoch [4][455/594], loss: 5.2494\n",
      "Epoch [4][460/594], loss: 5.2057\n",
      "Epoch [4][465/594], loss: 5.1624\n",
      "Epoch [4][470/594], loss: 5.2653\n",
      "Epoch [4][475/594], loss: 5.2569\n",
      "Epoch [4][480/594], loss: 5.2147\n",
      "Epoch [4][485/594], loss: 5.2025\n",
      "Epoch [4][490/594], loss: 5.2522\n",
      "Epoch [4][495/594], loss: 5.3397\n",
      "Epoch [4][500/594], loss: 5.2344\n",
      "Epoch [4][505/594], loss: 5.178\n",
      "Epoch [4][510/594], loss: 5.2697\n",
      "Epoch [4][515/594], loss: 5.1806\n",
      "Epoch [4][520/594], loss: 5.2906\n",
      "Epoch [4][525/594], loss: 5.3029\n",
      "Epoch [4][530/594], loss: 5.1471\n",
      "Epoch [4][535/594], loss: 5.1703\n",
      "Epoch [4][540/594], loss: 5.1422\n",
      "Epoch [4][545/594], loss: 5.2165\n",
      "Epoch [4][550/594], loss: 5.2452\n",
      "Epoch [4][555/594], loss: 5.2795\n",
      "Epoch [4][560/594], loss: 5.3201\n",
      "Epoch [4][565/594], loss: 5.2575\n",
      "Epoch [4][570/594], loss: 5.2474\n",
      "Epoch [4][575/594], loss: 5.2268\n",
      "Epoch [4][580/594], loss: 5.1827\n",
      "Epoch [4][585/594], loss: 5.2504\n",
      "Epoch [4][590/594], loss: 5.2426\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 5.2426, val_loss: 5.2383\n",
      "Saving checkpoint at 4 epochs...\n",
      "Epoch [5][5/594], loss: 5.1593\n",
      "Epoch [5][10/594], loss: 5.2183\n",
      "Epoch [5][15/594], loss: 5.3576\n",
      "Epoch [5][20/594], loss: 5.1554\n",
      "Epoch [5][25/594], loss: 5.2321\n",
      "Epoch [5][30/594], loss: 5.1874\n",
      "Epoch [5][35/594], loss: 5.1766\n",
      "Epoch [5][40/594], loss: 5.262\n",
      "Epoch [5][45/594], loss: 5.2948\n",
      "Epoch [5][50/594], loss: 5.2284\n",
      "Epoch [5][55/594], loss: 5.1826\n",
      "Epoch [5][60/594], loss: 5.1964\n",
      "Epoch [5][65/594], loss: 5.2541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5][70/594], loss: 5.2254\n",
      "Epoch [5][75/594], loss: 5.2039\n",
      "Epoch [5][80/594], loss: 5.2239\n",
      "Epoch [5][85/594], loss: 5.1362\n",
      "Epoch [5][90/594], loss: 5.0971\n",
      "Epoch [5][95/594], loss: 5.3176\n",
      "Epoch [5][100/594], loss: 5.1831\n",
      "Epoch [5][105/594], loss: 5.207\n",
      "Epoch [5][110/594], loss: 5.1156\n",
      "Epoch [5][115/594], loss: 5.0658\n",
      "Epoch [5][120/594], loss: 5.0794\n",
      "Epoch [5][125/594], loss: 5.2174\n",
      "Epoch [5][130/594], loss: 5.0592\n",
      "Epoch [5][135/594], loss: 5.1758\n",
      "Epoch [5][140/594], loss: 5.2095\n",
      "Epoch [5][145/594], loss: 5.2077\n",
      "Epoch [5][150/594], loss: 5.1249\n",
      "Epoch [5][155/594], loss: 5.1762\n",
      "Epoch [5][160/594], loss: 5.2431\n",
      "Epoch [5][165/594], loss: 5.177\n",
      "Epoch [5][170/594], loss: 5.2044\n",
      "Epoch [5][175/594], loss: 5.2281\n",
      "Epoch [5][180/594], loss: 5.1184\n",
      "Epoch [5][185/594], loss: 5.1737\n",
      "Epoch [5][190/594], loss: 5.1666\n",
      "Epoch [5][195/594], loss: 5.2585\n",
      "Epoch [5][200/594], loss: 5.2812\n",
      "Epoch [5][205/594], loss: 5.1545\n",
      "Epoch [5][210/594], loss: 5.0984\n",
      "Epoch [5][215/594], loss: 5.1843\n",
      "Epoch [5][220/594], loss: 5.157\n",
      "Epoch [5][225/594], loss: 5.1949\n",
      "Epoch [5][230/594], loss: 5.25\n",
      "Epoch [5][235/594], loss: 5.1692\n",
      "Epoch [5][240/594], loss: 5.1901\n",
      "Epoch [5][245/594], loss: 5.101\n",
      "Epoch [5][250/594], loss: 5.2423\n",
      "Epoch [5][255/594], loss: 5.1942\n",
      "Epoch [5][260/594], loss: 5.1833\n",
      "Epoch [5][265/594], loss: 5.1504\n",
      "Epoch [5][270/594], loss: 5.1354\n",
      "Epoch [5][275/594], loss: 5.0136\n",
      "Epoch [5][280/594], loss: 5.193\n",
      "Epoch [5][285/594], loss: 5.1607\n",
      "Epoch [5][290/594], loss: 5.2733\n",
      "Epoch [5][295/594], loss: 5.1749\n",
      "Epoch [5][300/594], loss: 5.1285\n",
      "Epoch [5][305/594], loss: 5.0225\n",
      "Epoch [5][310/594], loss: 5.1665\n",
      "Epoch [5][315/594], loss: 5.1424\n",
      "Epoch [5][320/594], loss: 5.2263\n",
      "Epoch [5][325/594], loss: 5.0756\n",
      "Epoch [5][330/594], loss: 5.2438\n",
      "Epoch [5][335/594], loss: 5.18\n",
      "Epoch [5][340/594], loss: 5.1071\n",
      "Epoch [5][345/594], loss: 5.1663\n",
      "Epoch [5][350/594], loss: 5.0281\n",
      "Epoch [5][355/594], loss: 5.1686\n",
      "Epoch [5][360/594], loss: 5.2202\n",
      "Epoch [5][365/594], loss: 5.1133\n",
      "Epoch [5][370/594], loss: 5.2325\n",
      "Epoch [5][375/594], loss: 5.0635\n",
      "Epoch [5][380/594], loss: 5.1226\n",
      "Epoch [5][385/594], loss: 5.0156\n",
      "Epoch [5][390/594], loss: 5.2425\n",
      "Epoch [5][395/594], loss: 4.8967\n",
      "Epoch [5][400/594], loss: 5.0515\n",
      "Epoch [5][405/594], loss: 5.1421\n",
      "Epoch [5][410/594], loss: 5.0828\n",
      "Epoch [5][415/594], loss: 5.1513\n",
      "Epoch [5][420/594], loss: 5.1589\n",
      "Epoch [5][425/594], loss: 5.1737\n",
      "Epoch [5][430/594], loss: 5.2701\n",
      "Epoch [5][435/594], loss: 5.2228\n",
      "Epoch [5][440/594], loss: 5.1443\n",
      "Epoch [5][445/594], loss: 5.18\n",
      "Epoch [5][450/594], loss: 4.9892\n",
      "Epoch [5][455/594], loss: 5.1429\n",
      "Epoch [5][460/594], loss: 5.1054\n",
      "Epoch [5][465/594], loss: 5.1175\n",
      "Epoch [5][470/594], loss: 5.052\n",
      "Epoch [5][475/594], loss: 5.2309\n",
      "Epoch [5][480/594], loss: 5.1896\n",
      "Epoch [5][485/594], loss: 5.0127\n",
      "Epoch [5][490/594], loss: 5.1694\n",
      "Epoch [5][495/594], loss: 5.1496\n",
      "Epoch [5][500/594], loss: 5.2061\n",
      "Epoch [5][505/594], loss: 5.0907\n",
      "Epoch [5][510/594], loss: 5.2087\n",
      "Epoch [5][515/594], loss: 5.1865\n",
      "Epoch [5][520/594], loss: 5.2437\n",
      "Epoch [5][525/594], loss: 5.1046\n",
      "Epoch [5][530/594], loss: 5.0434\n",
      "Epoch [5][535/594], loss: 5.0841\n",
      "Epoch [5][540/594], loss: 5.1876\n",
      "Epoch [5][545/594], loss: 5.1106\n",
      "Epoch [5][550/594], loss: 5.1832\n",
      "Epoch [5][555/594], loss: 5.1228\n",
      "Epoch [5][560/594], loss: 5.0942\n",
      "Epoch [5][565/594], loss: 5.1852\n",
      "Epoch [5][570/594], loss: 5.0906\n",
      "Epoch [5][575/594], loss: 5.1533\n",
      "Epoch [5][580/594], loss: 5.1762\n",
      "Epoch [5][585/594], loss: 5.1881\n",
      "Epoch [5][590/594], loss: 5.0722\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07752, train_loss: 5.0722, val_loss: 5.1147\n",
      "Saving checkpoint at 5 epochs...\n",
      "Epoch [6][5/594], loss: 5.1762\n",
      "Epoch [6][10/594], loss: 5.0995\n",
      "Epoch [6][15/594], loss: 5.1547\n",
      "Epoch [6][20/594], loss: 5.1416\n",
      "Epoch [6][25/594], loss: 5.1023\n",
      "Epoch [6][30/594], loss: 5.1106\n",
      "Epoch [6][35/594], loss: 5.1155\n",
      "Epoch [6][40/594], loss: 5.1369\n",
      "Epoch [6][45/594], loss: 5.0929\n",
      "Epoch [6][50/594], loss: 4.9507\n",
      "Epoch [6][55/594], loss: 5.1213\n",
      "Epoch [6][60/594], loss: 5.1186\n",
      "Epoch [6][65/594], loss: 5.1425\n",
      "Epoch [6][70/594], loss: 5.0374\n",
      "Epoch [6][75/594], loss: 5.0304\n",
      "Epoch [6][80/594], loss: 5.073\n",
      "Epoch [6][85/594], loss: 4.9804\n",
      "Epoch [6][90/594], loss: 5.0558\n",
      "Epoch [6][95/594], loss: 5.2164\n",
      "Epoch [6][100/594], loss: 5.1432\n",
      "Epoch [6][105/594], loss: 5.2069\n",
      "Epoch [6][110/594], loss: 5.1793\n",
      "Epoch [6][115/594], loss: 5.069\n",
      "Epoch [6][120/594], loss: 5.1461\n",
      "Epoch [6][125/594], loss: 5.0056\n",
      "Epoch [6][130/594], loss: 5.1077\n",
      "Epoch [6][135/594], loss: 5.1016\n",
      "Epoch [6][140/594], loss: 5.1051\n",
      "Epoch [6][145/594], loss: 5.1484\n",
      "Epoch [6][150/594], loss: 5.1175\n",
      "Epoch [6][155/594], loss: 5.0033\n",
      "Epoch [6][160/594], loss: 5.0238\n",
      "Epoch [6][165/594], loss: 5.1132\n",
      "Epoch [6][170/594], loss: 5.1346\n",
      "Epoch [6][175/594], loss: 5.1252\n",
      "Epoch [6][180/594], loss: 5.0612\n",
      "Epoch [6][185/594], loss: 5.1025\n",
      "Epoch [6][190/594], loss: 5.1464\n",
      "Epoch [6][195/594], loss: 5.0439\n",
      "Epoch [6][200/594], loss: 5.0693\n",
      "Epoch [6][205/594], loss: 5.1812\n",
      "Epoch [6][210/594], loss: 5.012\n",
      "Epoch [6][215/594], loss: 5.1235\n",
      "Epoch [6][220/594], loss: 4.9554\n",
      "Epoch [6][225/594], loss: 5.1028\n",
      "Epoch [6][230/594], loss: 5.1178\n",
      "Epoch [6][235/594], loss: 5.0542\n",
      "Epoch [6][240/594], loss: 5.1066\n",
      "Epoch [6][245/594], loss: 4.9588\n",
      "Epoch [6][250/594], loss: 5.0235\n",
      "Epoch [6][255/594], loss: 5.095\n",
      "Epoch [6][260/594], loss: 4.8833\n",
      "Epoch [6][265/594], loss: 4.9315\n",
      "Epoch [6][270/594], loss: 4.9895\n",
      "Epoch [6][275/594], loss: 5.0581\n",
      "Epoch [6][280/594], loss: 5.0578\n",
      "Epoch [6][285/594], loss: 5.0345\n",
      "Epoch [6][290/594], loss: 5.0877\n",
      "Epoch [6][295/594], loss: 5.0026\n",
      "Epoch [6][300/594], loss: 5.0159\n",
      "Epoch [6][305/594], loss: 5.1217\n",
      "Epoch [6][310/594], loss: 4.9878\n",
      "Epoch [6][315/594], loss: 5.0861\n",
      "Epoch [6][320/594], loss: 5.0619\n",
      "Epoch [6][325/594], loss: 5.1981\n",
      "Epoch [6][330/594], loss: 5.0322\n",
      "Epoch [6][335/594], loss: 5.0958\n",
      "Epoch [6][340/594], loss: 5.1398\n",
      "Epoch [6][345/594], loss: 5.0665\n",
      "Epoch [6][350/594], loss: 5.0145\n",
      "Epoch [6][355/594], loss: 5.0136\n",
      "Epoch [6][360/594], loss: 5.1147\n",
      "Epoch [6][365/594], loss: 5.2094\n",
      "Epoch [6][370/594], loss: 4.9644\n",
      "Epoch [6][375/594], loss: 5.0904\n",
      "Epoch [6][380/594], loss: 5.0456\n",
      "Epoch [6][385/594], loss: 5.0406\n",
      "Epoch [6][390/594], loss: 5.1002\n",
      "Epoch [6][395/594], loss: 5.1194\n",
      "Epoch [6][400/594], loss: 5.0674\n",
      "Epoch [6][405/594], loss: 5.168\n",
      "Epoch [6][410/594], loss: 5.1166\n",
      "Epoch [6][415/594], loss: 4.9538\n",
      "Epoch [6][420/594], loss: 5.0724\n",
      "Epoch [6][425/594], loss: 5.1175\n",
      "Epoch [6][430/594], loss: 4.9105\n",
      "Epoch [6][435/594], loss: 5.1162\n",
      "Epoch [6][440/594], loss: 4.9791\n",
      "Epoch [6][445/594], loss: 4.9823\n",
      "Epoch [6][450/594], loss: 5.0056\n",
      "Epoch [6][455/594], loss: 5.0163\n",
      "Epoch [6][460/594], loss: 4.9894\n",
      "Epoch [6][465/594], loss: 5.0489\n",
      "Epoch [6][470/594], loss: 5.0446\n",
      "Epoch [6][475/594], loss: 5.17\n",
      "Epoch [6][480/594], loss: 4.9931\n",
      "Epoch [6][485/594], loss: 5.0316\n",
      "Epoch [6][490/594], loss: 5.065\n",
      "Epoch [6][495/594], loss: 4.9726\n",
      "Epoch [6][500/594], loss: 5.0743\n",
      "Epoch [6][505/594], loss: 5.099\n",
      "Epoch [6][510/594], loss: 4.9677\n",
      "Epoch [6][515/594], loss: 4.9209\n",
      "Epoch [6][520/594], loss: 4.9917\n",
      "Epoch [6][525/594], loss: 4.9955\n",
      "Epoch [6][530/594], loss: 5.0361\n",
      "Epoch [6][535/594], loss: 5.112\n",
      "Epoch [6][540/594], loss: 5.0642\n",
      "Epoch [6][545/594], loss: 5.0347\n",
      "Epoch [6][550/594], loss: 4.9478\n",
      "Epoch [6][555/594], loss: 4.9503\n",
      "Epoch [6][560/594], loss: 5.0105\n",
      "Epoch [6][565/594], loss: 4.9419\n",
      "Epoch [6][570/594], loss: 4.927\n",
      "Epoch [6][575/594], loss: 5.0996\n",
      "Epoch [6][580/594], loss: 4.9804\n",
      "Epoch [6][585/594], loss: 5.0032\n",
      "Epoch [6][590/594], loss: 5.0237\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 5.0237, val_loss: 5.0247\n",
      "Saving checkpoint at 6 epochs...\n",
      "Epoch [7][5/594], loss: 5.0117\n",
      "Epoch [7][10/594], loss: 5.0196\n",
      "Epoch [7][15/594], loss: 4.9845\n",
      "Epoch [7][20/594], loss: 4.995\n",
      "Epoch [7][25/594], loss: 5.0497\n",
      "Epoch [7][30/594], loss: 4.9994\n",
      "Epoch [7][35/594], loss: 5.0384\n",
      "Epoch [7][40/594], loss: 4.9758\n",
      "Epoch [7][45/594], loss: 4.9703\n",
      "Epoch [7][50/594], loss: 5.0804\n",
      "Epoch [7][55/594], loss: 5.0367\n",
      "Epoch [7][60/594], loss: 4.9456\n",
      "Epoch [7][65/594], loss: 5.0102\n",
      "Epoch [7][70/594], loss: 5.0217\n",
      "Epoch [7][75/594], loss: 4.9967\n",
      "Epoch [7][80/594], loss: 4.9452\n",
      "Epoch [7][85/594], loss: 5.0204\n",
      "Epoch [7][90/594], loss: 4.9266\n",
      "Epoch [7][95/594], loss: 5.048\n",
      "Epoch [7][100/594], loss: 4.9066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7][105/594], loss: 5.1082\n",
      "Epoch [7][110/594], loss: 4.9737\n",
      "Epoch [7][115/594], loss: 5.0726\n",
      "Epoch [7][120/594], loss: 4.948\n",
      "Epoch [7][125/594], loss: 4.9014\n",
      "Epoch [7][130/594], loss: 4.9712\n",
      "Epoch [7][135/594], loss: 4.9405\n",
      "Epoch [7][140/594], loss: 4.9929\n",
      "Epoch [7][145/594], loss: 4.885\n",
      "Epoch [7][150/594], loss: 4.9534\n",
      "Epoch [7][155/594], loss: 5.1089\n",
      "Epoch [7][160/594], loss: 5.0324\n",
      "Epoch [7][165/594], loss: 5.1138\n",
      "Epoch [7][170/594], loss: 5.0262\n",
      "Epoch [7][175/594], loss: 5.0377\n",
      "Epoch [7][180/594], loss: 5.0343\n",
      "Epoch [7][185/594], loss: 5.1131\n",
      "Epoch [7][190/594], loss: 5.1108\n",
      "Epoch [7][195/594], loss: 5.0574\n",
      "Epoch [7][200/594], loss: 4.9712\n",
      "Epoch [7][205/594], loss: 5.025\n",
      "Epoch [7][210/594], loss: 5.0486\n",
      "Epoch [7][215/594], loss: 4.8697\n",
      "Epoch [7][220/594], loss: 5.021\n",
      "Epoch [7][225/594], loss: 5.026\n",
      "Epoch [7][230/594], loss: 4.9785\n",
      "Epoch [7][235/594], loss: 5.0567\n",
      "Epoch [7][240/594], loss: 5.0947\n",
      "Epoch [7][245/594], loss: 5.0879\n",
      "Epoch [7][250/594], loss: 5.0134\n",
      "Epoch [7][255/594], loss: 5.0153\n",
      "Epoch [7][260/594], loss: 4.9542\n",
      "Epoch [7][265/594], loss: 5.0013\n",
      "Epoch [7][270/594], loss: 4.9349\n",
      "Epoch [7][275/594], loss: 5.0014\n",
      "Epoch [7][280/594], loss: 4.9627\n",
      "Epoch [7][285/594], loss: 4.9929\n",
      "Epoch [7][290/594], loss: 4.9674\n",
      "Epoch [7][295/594], loss: 5.0478\n",
      "Epoch [7][300/594], loss: 5.0281\n",
      "Epoch [7][305/594], loss: 5.017\n",
      "Epoch [7][310/594], loss: 4.9354\n",
      "Epoch [7][315/594], loss: 5.0313\n",
      "Epoch [7][320/594], loss: 4.9357\n",
      "Epoch [7][325/594], loss: 4.9708\n",
      "Epoch [7][330/594], loss: 4.9713\n",
      "Epoch [7][335/594], loss: 4.9586\n",
      "Epoch [7][340/594], loss: 5.0124\n",
      "Epoch [7][345/594], loss: 4.8719\n",
      "Epoch [7][350/594], loss: 4.9616\n",
      "Epoch [7][355/594], loss: 5.0584\n",
      "Epoch [7][360/594], loss: 4.981\n",
      "Epoch [7][365/594], loss: 4.9914\n",
      "Epoch [7][370/594], loss: 4.9575\n",
      "Epoch [7][375/594], loss: 4.8875\n",
      "Epoch [7][380/594], loss: 4.9045\n",
      "Epoch [7][385/594], loss: 5.0077\n",
      "Epoch [7][390/594], loss: 5.0305\n",
      "Epoch [7][395/594], loss: 5.0316\n",
      "Epoch [7][400/594], loss: 5.0964\n",
      "Epoch [7][405/594], loss: 4.8255\n",
      "Epoch [7][410/594], loss: 5.0639\n",
      "Epoch [7][415/594], loss: 5.0332\n",
      "Epoch [7][420/594], loss: 4.9181\n",
      "Epoch [7][425/594], loss: 4.944\n",
      "Epoch [7][430/594], loss: 4.9709\n",
      "Epoch [7][435/594], loss: 4.9435\n",
      "Epoch [7][440/594], loss: 4.9717\n",
      "Epoch [7][445/594], loss: 5.0643\n",
      "Epoch [7][450/594], loss: 4.8734\n",
      "Epoch [7][455/594], loss: 4.8638\n",
      "Epoch [7][460/594], loss: 4.9976\n",
      "Epoch [7][465/594], loss: 5.0122\n",
      "Epoch [7][470/594], loss: 5.0364\n",
      "Epoch [7][475/594], loss: 5.0631\n",
      "Epoch [7][480/594], loss: 4.9052\n",
      "Epoch [7][485/594], loss: 4.9079\n",
      "Epoch [7][490/594], loss: 5.0013\n",
      "Epoch [7][495/594], loss: 4.921\n",
      "Epoch [7][500/594], loss: 4.8378\n",
      "Epoch [7][505/594], loss: 4.9027\n",
      "Epoch [7][510/594], loss: 4.9859\n",
      "Epoch [7][515/594], loss: 4.9487\n",
      "Epoch [7][520/594], loss: 4.9985\n",
      "Epoch [7][525/594], loss: 4.9879\n",
      "Epoch [7][530/594], loss: 4.9947\n",
      "Epoch [7][535/594], loss: 4.9743\n",
      "Epoch [7][540/594], loss: 4.9567\n",
      "Epoch [7][545/594], loss: 4.8715\n",
      "Epoch [7][550/594], loss: 4.8383\n",
      "Epoch [7][555/594], loss: 4.8387\n",
      "Epoch [7][560/594], loss: 5.0568\n",
      "Epoch [7][565/594], loss: 4.9981\n",
      "Epoch [7][570/594], loss: 4.9107\n",
      "Epoch [7][575/594], loss: 4.9982\n",
      "Epoch [7][580/594], loss: 4.9939\n",
      "Epoch [7][585/594], loss: 4.9295\n",
      "Epoch [7][590/594], loss: 4.9949\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07752, train_loss: 4.9949, val_loss: 4.9591\n",
      "Saving checkpoint at 7 epochs...\n",
      "Epoch [8][5/594], loss: 4.936\n",
      "Epoch [8][10/594], loss: 4.9614\n",
      "Epoch [8][15/594], loss: 4.9253\n",
      "Epoch [8][20/594], loss: 4.9821\n",
      "Epoch [8][25/594], loss: 4.8473\n",
      "Epoch [8][30/594], loss: 4.9164\n",
      "Epoch [8][35/594], loss: 4.8633\n",
      "Epoch [8][40/594], loss: 4.9018\n",
      "Epoch [8][45/594], loss: 4.8936\n",
      "Epoch [8][50/594], loss: 4.9161\n",
      "Epoch [8][55/594], loss: 4.9094\n",
      "Epoch [8][60/594], loss: 4.9481\n",
      "Epoch [8][65/594], loss: 4.8762\n",
      "Epoch [8][70/594], loss: 4.9417\n",
      "Epoch [8][75/594], loss: 4.9807\n",
      "Epoch [8][80/594], loss: 4.9544\n",
      "Epoch [8][85/594], loss: 4.8174\n",
      "Epoch [8][90/594], loss: 5.0211\n",
      "Epoch [8][95/594], loss: 5.0177\n",
      "Epoch [8][100/594], loss: 4.9525\n",
      "Epoch [8][105/594], loss: 5.0074\n",
      "Epoch [8][110/594], loss: 4.8973\n",
      "Epoch [8][115/594], loss: 5.025\n",
      "Epoch [8][120/594], loss: 4.9974\n",
      "Epoch [8][125/594], loss: 4.968\n",
      "Epoch [8][130/594], loss: 4.9553\n",
      "Epoch [8][135/594], loss: 4.8496\n",
      "Epoch [8][140/594], loss: 5.0066\n",
      "Epoch [8][145/594], loss: 4.8706\n",
      "Epoch [8][150/594], loss: 5.0153\n",
      "Epoch [8][155/594], loss: 5.0364\n",
      "Epoch [8][160/594], loss: 4.8677\n",
      "Epoch [8][165/594], loss: 4.7589\n",
      "Epoch [8][170/594], loss: 4.9293\n",
      "Epoch [8][175/594], loss: 4.9599\n",
      "Epoch [8][180/594], loss: 4.94\n",
      "Epoch [8][185/594], loss: 4.8644\n",
      "Epoch [8][190/594], loss: 4.95\n",
      "Epoch [8][195/594], loss: 4.8506\n",
      "Epoch [8][200/594], loss: 4.9823\n",
      "Epoch [8][205/594], loss: 4.9677\n",
      "Epoch [8][210/594], loss: 4.9221\n",
      "Epoch [8][215/594], loss: 4.9672\n",
      "Epoch [8][220/594], loss: 4.9001\n",
      "Epoch [8][225/594], loss: 4.9141\n",
      "Epoch [8][230/594], loss: 4.9914\n",
      "Epoch [8][235/594], loss: 4.7984\n",
      "Epoch [8][240/594], loss: 4.9516\n",
      "Epoch [8][245/594], loss: 4.9239\n",
      "Epoch [8][250/594], loss: 4.9477\n",
      "Epoch [8][255/594], loss: 4.8757\n",
      "Epoch [8][260/594], loss: 4.9551\n",
      "Epoch [8][265/594], loss: 4.9877\n",
      "Epoch [8][270/594], loss: 4.9461\n",
      "Epoch [8][275/594], loss: 5.0038\n",
      "Epoch [8][280/594], loss: 4.8824\n",
      "Epoch [8][285/594], loss: 4.9934\n",
      "Epoch [8][290/594], loss: 4.9738\n",
      "Epoch [8][295/594], loss: 4.8857\n",
      "Epoch [8][300/594], loss: 4.8357\n",
      "Epoch [8][305/594], loss: 5.0452\n",
      "Epoch [8][310/594], loss: 4.9943\n",
      "Epoch [8][315/594], loss: 4.968\n",
      "Epoch [8][320/594], loss: 4.9101\n",
      "Epoch [8][325/594], loss: 4.9839\n",
      "Epoch [8][330/594], loss: 4.8312\n",
      "Epoch [8][335/594], loss: 4.8499\n",
      "Epoch [8][340/594], loss: 4.9183\n",
      "Epoch [8][345/594], loss: 4.8299\n",
      "Epoch [8][350/594], loss: 5.0041\n",
      "Epoch [8][355/594], loss: 4.9145\n",
      "Epoch [8][360/594], loss: 4.9523\n",
      "Epoch [8][365/594], loss: 4.9525\n",
      "Epoch [8][370/594], loss: 4.9832\n",
      "Epoch [8][375/594], loss: 4.8542\n",
      "Epoch [8][380/594], loss: 4.8534\n",
      "Epoch [8][385/594], loss: 4.9664\n",
      "Epoch [8][390/594], loss: 4.9801\n",
      "Epoch [8][395/594], loss: 4.8488\n",
      "Epoch [8][400/594], loss: 4.9981\n",
      "Epoch [8][405/594], loss: 4.858\n",
      "Epoch [8][410/594], loss: 4.9814\n",
      "Epoch [8][415/594], loss: 4.7228\n",
      "Epoch [8][420/594], loss: 4.9947\n",
      "Epoch [8][425/594], loss: 4.9978\n",
      "Epoch [8][430/594], loss: 4.8794\n",
      "Epoch [8][435/594], loss: 4.9654\n",
      "Epoch [8][440/594], loss: 4.8503\n",
      "Epoch [8][445/594], loss: 4.8381\n",
      "Epoch [8][450/594], loss: 4.862\n",
      "Epoch [8][455/594], loss: 5.0159\n",
      "Epoch [8][460/594], loss: 4.8979\n",
      "Epoch [8][465/594], loss: 4.9671\n",
      "Epoch [8][470/594], loss: 5.0402\n",
      "Epoch [8][475/594], loss: 4.9755\n",
      "Epoch [8][480/594], loss: 4.9876\n",
      "Epoch [8][485/594], loss: 4.9243\n",
      "Epoch [8][490/594], loss: 5.0143\n",
      "Epoch [8][495/594], loss: 5.0252\n",
      "Epoch [8][500/594], loss: 4.8377\n",
      "Epoch [8][505/594], loss: 4.9115\n",
      "Epoch [8][510/594], loss: 4.9045\n",
      "Epoch [8][515/594], loss: 4.9843\n",
      "Epoch [8][520/594], loss: 4.8925\n",
      "Epoch [8][525/594], loss: 4.9322\n",
      "Epoch [8][530/594], loss: 4.9194\n",
      "Epoch [8][535/594], loss: 5.025\n",
      "Epoch [8][540/594], loss: 4.8738\n",
      "Epoch [8][545/594], loss: 4.8613\n",
      "Epoch [8][550/594], loss: 4.9243\n",
      "Epoch [8][555/594], loss: 4.9038\n",
      "Epoch [8][560/594], loss: 4.8856\n",
      "Epoch [8][565/594], loss: 4.8642\n",
      "Epoch [8][570/594], loss: 4.8381\n",
      "Epoch [8][575/594], loss: 4.9517\n",
      "Epoch [8][580/594], loss: 4.9399\n",
      "Epoch [8][585/594], loss: 4.9316\n",
      "Epoch [8][590/594], loss: 4.8447\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.8447, val_loss: 4.9064\n",
      "Saving checkpoint at 8 epochs...\n",
      "Epoch [9][5/594], loss: 4.8834\n",
      "Epoch [9][10/594], loss: 4.9061\n",
      "Epoch [9][15/594], loss: 4.8004\n",
      "Epoch [9][20/594], loss: 4.8616\n",
      "Epoch [9][25/594], loss: 4.8226\n",
      "Epoch [9][30/594], loss: 4.7452\n",
      "Epoch [9][35/594], loss: 4.9489\n",
      "Epoch [9][40/594], loss: 4.8418\n",
      "Epoch [9][45/594], loss: 4.9536\n",
      "Epoch [9][50/594], loss: 4.814\n",
      "Epoch [9][55/594], loss: 4.8536\n",
      "Epoch [9][60/594], loss: 4.9345\n",
      "Epoch [9][65/594], loss: 4.8843\n",
      "Epoch [9][70/594], loss: 4.9232\n",
      "Epoch [9][75/594], loss: 4.8881\n",
      "Epoch [9][80/594], loss: 4.9252\n",
      "Epoch [9][85/594], loss: 4.8698\n",
      "Epoch [9][90/594], loss: 4.8606\n",
      "Epoch [9][95/594], loss: 4.9004\n",
      "Epoch [9][100/594], loss: 4.793\n",
      "Epoch [9][105/594], loss: 4.9377\n",
      "Epoch [9][110/594], loss: 4.8849\n",
      "Epoch [9][115/594], loss: 4.8038\n",
      "Epoch [9][120/594], loss: 4.7869\n",
      "Epoch [9][125/594], loss: 4.9438\n",
      "Epoch [9][130/594], loss: 4.818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9][135/594], loss: 4.9727\n",
      "Epoch [9][140/594], loss: 4.9296\n",
      "Epoch [9][145/594], loss: 4.9225\n",
      "Epoch [9][150/594], loss: 4.9409\n",
      "Epoch [9][155/594], loss: 4.9031\n",
      "Epoch [9][160/594], loss: 4.8805\n",
      "Epoch [9][165/594], loss: 4.911\n",
      "Epoch [9][170/594], loss: 4.9003\n",
      "Epoch [9][175/594], loss: 4.9045\n",
      "Epoch [9][180/594], loss: 4.8521\n",
      "Epoch [9][185/594], loss: 4.9601\n",
      "Epoch [9][190/594], loss: 4.8351\n",
      "Epoch [9][195/594], loss: 4.9088\n",
      "Epoch [9][200/594], loss: 4.9292\n",
      "Epoch [9][205/594], loss: 4.9612\n",
      "Epoch [9][210/594], loss: 4.946\n",
      "Epoch [9][215/594], loss: 4.9225\n",
      "Epoch [9][220/594], loss: 4.9165\n",
      "Epoch [9][225/594], loss: 4.908\n",
      "Epoch [9][230/594], loss: 4.9327\n",
      "Epoch [9][235/594], loss: 4.7893\n",
      "Epoch [9][240/594], loss: 4.9432\n",
      "Epoch [9][245/594], loss: 4.9158\n",
      "Epoch [9][250/594], loss: 4.8527\n",
      "Epoch [9][255/594], loss: 4.9032\n",
      "Epoch [9][260/594], loss: 4.902\n",
      "Epoch [9][265/594], loss: 4.9324\n",
      "Epoch [9][270/594], loss: 4.8322\n",
      "Epoch [9][275/594], loss: 4.844\n",
      "Epoch [9][280/594], loss: 4.8142\n",
      "Epoch [9][285/594], loss: 4.9809\n",
      "Epoch [9][290/594], loss: 4.9189\n",
      "Epoch [9][295/594], loss: 4.8122\n",
      "Epoch [9][300/594], loss: 4.8642\n",
      "Epoch [9][305/594], loss: 4.8664\n",
      "Epoch [9][310/594], loss: 4.784\n",
      "Epoch [9][315/594], loss: 4.9188\n",
      "Epoch [9][320/594], loss: 4.7737\n",
      "Epoch [9][325/594], loss: 4.8511\n",
      "Epoch [9][330/594], loss: 4.7866\n",
      "Epoch [9][335/594], loss: 4.909\n",
      "Epoch [9][340/594], loss: 4.9271\n",
      "Epoch [9][345/594], loss: 4.9681\n",
      "Epoch [9][350/594], loss: 4.9127\n",
      "Epoch [9][355/594], loss: 4.9538\n",
      "Epoch [9][360/594], loss: 4.9385\n",
      "Epoch [9][365/594], loss: 4.9847\n",
      "Epoch [9][370/594], loss: 4.8265\n",
      "Epoch [9][375/594], loss: 4.7917\n",
      "Epoch [9][380/594], loss: 4.8379\n",
      "Epoch [9][385/594], loss: 4.8851\n",
      "Epoch [9][390/594], loss: 4.9318\n",
      "Epoch [9][395/594], loss: 4.8606\n",
      "Epoch [9][400/594], loss: 4.8751\n",
      "Epoch [9][405/594], loss: 4.8195\n",
      "Epoch [9][410/594], loss: 4.7956\n",
      "Epoch [9][415/594], loss: 4.8753\n",
      "Epoch [9][420/594], loss: 4.8542\n",
      "Epoch [9][425/594], loss: 4.8349\n",
      "Epoch [9][430/594], loss: 4.9156\n",
      "Epoch [9][435/594], loss: 4.9535\n",
      "Epoch [9][440/594], loss: 4.8875\n",
      "Epoch [9][445/594], loss: 4.8184\n",
      "Epoch [9][450/594], loss: 4.9094\n",
      "Epoch [9][455/594], loss: 4.944\n",
      "Epoch [9][460/594], loss: 4.8453\n",
      "Epoch [9][465/594], loss: 4.8108\n",
      "Epoch [9][470/594], loss: 4.9718\n",
      "Epoch [9][475/594], loss: 4.7916\n",
      "Epoch [9][480/594], loss: 4.7869\n",
      "Epoch [9][485/594], loss: 4.9379\n",
      "Epoch [9][490/594], loss: 4.9095\n",
      "Epoch [9][495/594], loss: 4.8693\n",
      "Epoch [9][500/594], loss: 4.91\n",
      "Epoch [9][505/594], loss: 4.8859\n",
      "Epoch [9][510/594], loss: 4.8345\n",
      "Epoch [9][515/594], loss: 4.9525\n",
      "Epoch [9][520/594], loss: 4.9372\n",
      "Epoch [9][525/594], loss: 4.8629\n",
      "Epoch [9][530/594], loss: 4.9373\n",
      "Epoch [9][535/594], loss: 4.9327\n",
      "Epoch [9][540/594], loss: 4.8516\n",
      "Epoch [9][545/594], loss: 4.7949\n",
      "Epoch [9][550/594], loss: 4.8544\n",
      "Epoch [9][555/594], loss: 4.8867\n",
      "Epoch [9][560/594], loss: 4.8568\n",
      "Epoch [9][565/594], loss: 4.9229\n",
      "Epoch [9][570/594], loss: 4.93\n",
      "Epoch [9][575/594], loss: 4.9678\n",
      "Epoch [9][580/594], loss: 4.8414\n",
      "Epoch [9][585/594], loss: 4.8529\n",
      "Epoch [9][590/594], loss: 4.8094\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.8094, val_loss: 4.8599\n",
      "Saving checkpoint at 9 epochs...\n",
      "Epoch [10][5/594], loss: 4.7374\n",
      "Epoch [10][10/594], loss: 4.8879\n",
      "Epoch [10][15/594], loss: 4.7278\n",
      "Epoch [10][20/594], loss: 4.9038\n",
      "Epoch [10][25/594], loss: 4.8941\n",
      "Epoch [10][30/594], loss: 4.9057\n",
      "Epoch [10][35/594], loss: 4.9489\n",
      "Epoch [10][40/594], loss: 4.7996\n",
      "Epoch [10][45/594], loss: 4.9109\n",
      "Epoch [10][50/594], loss: 4.8991\n",
      "Epoch [10][55/594], loss: 4.7867\n",
      "Epoch [10][60/594], loss: 4.8953\n",
      "Epoch [10][65/594], loss: 4.9746\n",
      "Epoch [10][70/594], loss: 4.9222\n",
      "Epoch [10][75/594], loss: 4.8785\n",
      "Epoch [10][80/594], loss: 4.8768\n",
      "Epoch [10][85/594], loss: 4.9105\n",
      "Epoch [10][90/594], loss: 4.8625\n",
      "Epoch [10][95/594], loss: 4.851\n",
      "Epoch [10][100/594], loss: 4.8109\n",
      "Epoch [10][105/594], loss: 4.8999\n",
      "Epoch [10][110/594], loss: 4.7895\n",
      "Epoch [10][115/594], loss: 4.8039\n",
      "Epoch [10][120/594], loss: 4.8909\n",
      "Epoch [10][125/594], loss: 4.808\n",
      "Epoch [10][130/594], loss: 4.7798\n",
      "Epoch [10][135/594], loss: 4.8083\n",
      "Epoch [10][140/594], loss: 4.7973\n",
      "Epoch [10][145/594], loss: 4.87\n",
      "Epoch [10][150/594], loss: 4.9431\n",
      "Epoch [10][155/594], loss: 4.9135\n",
      "Epoch [10][160/594], loss: 4.8119\n",
      "Epoch [10][165/594], loss: 4.8008\n",
      "Epoch [10][170/594], loss: 4.9319\n",
      "Epoch [10][175/594], loss: 4.7847\n",
      "Epoch [10][180/594], loss: 4.8795\n",
      "Epoch [10][185/594], loss: 4.8522\n",
      "Epoch [10][190/594], loss: 4.8574\n",
      "Epoch [10][195/594], loss: 4.7933\n",
      "Epoch [10][200/594], loss: 4.8866\n",
      "Epoch [10][205/594], loss: 4.8207\n",
      "Epoch [10][210/594], loss: 4.851\n",
      "Epoch [10][215/594], loss: 4.859\n",
      "Epoch [10][220/594], loss: 4.6956\n",
      "Epoch [10][225/594], loss: 4.8683\n",
      "Epoch [10][230/594], loss: 4.8183\n",
      "Epoch [10][235/594], loss: 4.9244\n",
      "Epoch [10][240/594], loss: 4.8052\n",
      "Epoch [10][245/594], loss: 4.9067\n",
      "Epoch [10][250/594], loss: 4.9004\n",
      "Epoch [10][255/594], loss: 4.9231\n",
      "Epoch [10][260/594], loss: 4.8696\n",
      "Epoch [10][265/594], loss: 4.7058\n",
      "Epoch [10][270/594], loss: 4.7903\n",
      "Epoch [10][275/594], loss: 4.8125\n",
      "Epoch [10][280/594], loss: 4.8739\n",
      "Epoch [10][285/594], loss: 4.8298\n",
      "Epoch [10][290/594], loss: 4.8698\n",
      "Epoch [10][295/594], loss: 4.8687\n",
      "Epoch [10][300/594], loss: 4.7746\n",
      "Epoch [10][305/594], loss: 4.9361\n",
      "Epoch [10][310/594], loss: 4.9141\n",
      "Epoch [10][315/594], loss: 4.9201\n",
      "Epoch [10][320/594], loss: 4.832\n",
      "Epoch [10][325/594], loss: 4.8808\n",
      "Epoch [10][330/594], loss: 4.8066\n",
      "Epoch [10][335/594], loss: 4.6631\n",
      "Epoch [10][340/594], loss: 4.7548\n",
      "Epoch [10][345/594], loss: 4.8397\n",
      "Epoch [10][350/594], loss: 4.9032\n",
      "Epoch [10][355/594], loss: 4.6814\n",
      "Epoch [10][360/594], loss: 4.8505\n",
      "Epoch [10][365/594], loss: 4.8457\n",
      "Epoch [10][370/594], loss: 4.8659\n",
      "Epoch [10][375/594], loss: 4.8755\n",
      "Epoch [10][380/594], loss: 4.8216\n",
      "Epoch [10][385/594], loss: 4.7862\n",
      "Epoch [10][390/594], loss: 4.803\n",
      "Epoch [10][395/594], loss: 4.8684\n",
      "Epoch [10][400/594], loss: 4.8977\n",
      "Epoch [10][405/594], loss: 4.8621\n",
      "Epoch [10][410/594], loss: 4.8236\n",
      "Epoch [10][415/594], loss: 4.8563\n",
      "Epoch [10][420/594], loss: 4.8542\n",
      "Epoch [10][425/594], loss: 4.8291\n",
      "Epoch [10][430/594], loss: 4.7936\n",
      "Epoch [10][435/594], loss: 4.8656\n",
      "Epoch [10][440/594], loss: 4.8549\n",
      "Epoch [10][445/594], loss: 4.8103\n",
      "Epoch [10][450/594], loss: 4.834\n",
      "Epoch [10][455/594], loss: 4.8852\n",
      "Epoch [10][460/594], loss: 4.8658\n",
      "Epoch [10][465/594], loss: 4.8358\n",
      "Epoch [10][470/594], loss: 4.8178\n",
      "Epoch [10][475/594], loss: 4.7703\n",
      "Epoch [10][480/594], loss: 4.8634\n",
      "Epoch [10][485/594], loss: 4.786\n",
      "Epoch [10][490/594], loss: 4.9082\n",
      "Epoch [10][495/594], loss: 4.7562\n",
      "Epoch [10][500/594], loss: 4.855\n",
      "Epoch [10][505/594], loss: 4.8286\n",
      "Epoch [10][510/594], loss: 4.8046\n",
      "Epoch [10][515/594], loss: 4.776\n",
      "Epoch [10][520/594], loss: 4.9006\n",
      "Epoch [10][525/594], loss: 4.937\n",
      "Epoch [10][530/594], loss: 4.8322\n",
      "Epoch [10][535/594], loss: 4.8117\n",
      "Epoch [10][540/594], loss: 4.8115\n",
      "Epoch [10][545/594], loss: 4.8294\n",
      "Epoch [10][550/594], loss: 4.9109\n",
      "Epoch [10][555/594], loss: 4.8109\n",
      "Epoch [10][560/594], loss: 4.9089\n",
      "Epoch [10][565/594], loss: 4.9141\n",
      "Epoch [10][570/594], loss: 4.8609\n",
      "Epoch [10][575/594], loss: 4.8328\n",
      "Epoch [10][580/594], loss: 4.8924\n",
      "Epoch [10][585/594], loss: 4.9577\n",
      "Epoch [10][590/594], loss: 4.7858\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.06589, train_loss: 4.7858, val_loss: 4.8332\n",
      "Saving checkpoint at 10 epochs...\n",
      "Epoch [11][5/594], loss: 4.7549\n",
      "Epoch [11][10/594], loss: 4.7545\n",
      "Epoch [11][15/594], loss: 4.774\n",
      "Epoch [11][20/594], loss: 4.8587\n",
      "Epoch [11][25/594], loss: 4.9294\n",
      "Epoch [11][30/594], loss: 4.8098\n",
      "Epoch [11][35/594], loss: 4.8538\n",
      "Epoch [11][40/594], loss: 4.7763\n",
      "Epoch [11][45/594], loss: 4.7998\n",
      "Epoch [11][50/594], loss: 4.7899\n",
      "Epoch [11][55/594], loss: 4.7845\n",
      "Epoch [11][60/594], loss: 4.7737\n",
      "Epoch [11][65/594], loss: 4.8154\n",
      "Epoch [11][70/594], loss: 4.8828\n",
      "Epoch [11][75/594], loss: 4.8388\n",
      "Epoch [11][80/594], loss: 4.8299\n",
      "Epoch [11][85/594], loss: 4.7991\n",
      "Epoch [11][90/594], loss: 4.8913\n",
      "Epoch [11][95/594], loss: 4.6756\n",
      "Epoch [11][100/594], loss: 4.8104\n",
      "Epoch [11][105/594], loss: 4.8558\n",
      "Epoch [11][110/594], loss: 4.7977\n",
      "Epoch [11][115/594], loss: 4.9091\n",
      "Epoch [11][120/594], loss: 4.7539\n",
      "Epoch [11][125/594], loss: 4.8596\n",
      "Epoch [11][130/594], loss: 4.7536\n",
      "Epoch [11][135/594], loss: 4.8\n",
      "Epoch [11][140/594], loss: 4.7978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11][145/594], loss: 4.7302\n",
      "Epoch [11][150/594], loss: 4.7782\n",
      "Epoch [11][155/594], loss: 4.8207\n",
      "Epoch [11][160/594], loss: 4.8328\n",
      "Epoch [11][165/594], loss: 4.8511\n",
      "Epoch [11][170/594], loss: 4.8853\n",
      "Epoch [11][175/594], loss: 4.8618\n",
      "Epoch [11][180/594], loss: 4.8833\n",
      "Epoch [11][185/594], loss: 4.8216\n",
      "Epoch [11][190/594], loss: 4.8791\n",
      "Epoch [11][195/594], loss: 4.8689\n",
      "Epoch [11][200/594], loss: 4.769\n",
      "Epoch [11][205/594], loss: 4.7537\n",
      "Epoch [11][210/594], loss: 4.8725\n",
      "Epoch [11][215/594], loss: 4.8572\n",
      "Epoch [11][220/594], loss: 4.8571\n",
      "Epoch [11][225/594], loss: 4.7562\n",
      "Epoch [11][230/594], loss: 4.8091\n",
      "Epoch [11][235/594], loss: 4.792\n",
      "Epoch [11][240/594], loss: 4.789\n",
      "Epoch [11][245/594], loss: 4.7712\n",
      "Epoch [11][250/594], loss: 4.7918\n",
      "Epoch [11][255/594], loss: 4.8627\n",
      "Epoch [11][260/594], loss: 4.8903\n",
      "Epoch [11][265/594], loss: 4.8442\n",
      "Epoch [11][270/594], loss: 4.8338\n",
      "Epoch [11][275/594], loss: 4.7444\n",
      "Epoch [11][280/594], loss: 4.7794\n",
      "Epoch [11][285/594], loss: 4.7738\n",
      "Epoch [11][290/594], loss: 4.8807\n",
      "Epoch [11][295/594], loss: 4.8032\n",
      "Epoch [11][300/594], loss: 4.7897\n",
      "Epoch [11][305/594], loss: 4.8628\n",
      "Epoch [11][310/594], loss: 4.8768\n",
      "Epoch [11][315/594], loss: 4.8157\n",
      "Epoch [11][320/594], loss: 4.849\n",
      "Epoch [11][325/594], loss: 4.8188\n",
      "Epoch [11][330/594], loss: 4.864\n",
      "Epoch [11][335/594], loss: 4.8695\n",
      "Epoch [11][340/594], loss: 4.8675\n",
      "Epoch [11][345/594], loss: 4.7614\n",
      "Epoch [11][350/594], loss: 4.87\n",
      "Epoch [11][355/594], loss: 4.7991\n",
      "Epoch [11][360/594], loss: 4.9027\n",
      "Epoch [11][365/594], loss: 4.7919\n",
      "Epoch [11][370/594], loss: 4.91\n",
      "Epoch [11][375/594], loss: 4.7702\n",
      "Epoch [11][380/594], loss: 4.8199\n",
      "Epoch [11][385/594], loss: 4.8753\n",
      "Epoch [11][390/594], loss: 4.7767\n",
      "Epoch [11][395/594], loss: 4.8235\n",
      "Epoch [11][400/594], loss: 4.8199\n",
      "Epoch [11][405/594], loss: 4.7213\n",
      "Epoch [11][410/594], loss: 4.9135\n",
      "Epoch [11][415/594], loss: 4.7923\n",
      "Epoch [11][420/594], loss: 4.7925\n",
      "Epoch [11][425/594], loss: 4.7358\n",
      "Epoch [11][430/594], loss: 4.8866\n",
      "Epoch [11][435/594], loss: 4.7434\n",
      "Epoch [11][440/594], loss: 4.7795\n",
      "Epoch [11][445/594], loss: 4.7068\n",
      "Epoch [11][450/594], loss: 4.7712\n",
      "Epoch [11][455/594], loss: 4.8774\n",
      "Epoch [11][460/594], loss: 4.8147\n",
      "Epoch [11][465/594], loss: 4.7025\n",
      "Epoch [11][470/594], loss: 4.7874\n",
      "Epoch [11][475/594], loss: 4.7956\n",
      "Epoch [11][480/594], loss: 4.8483\n",
      "Epoch [11][485/594], loss: 4.8347\n",
      "Epoch [11][490/594], loss: 4.827\n",
      "Epoch [11][495/594], loss: 4.883\n",
      "Epoch [11][500/594], loss: 4.8722\n",
      "Epoch [11][505/594], loss: 4.7919\n",
      "Epoch [11][510/594], loss: 4.8175\n",
      "Epoch [11][515/594], loss: 4.802\n",
      "Epoch [11][520/594], loss: 4.7921\n",
      "Epoch [11][525/594], loss: 4.8508\n",
      "Epoch [11][530/594], loss: 4.8019\n",
      "Epoch [11][535/594], loss: 4.9297\n",
      "Epoch [11][540/594], loss: 4.8379\n",
      "Epoch [11][545/594], loss: 4.6952\n",
      "Epoch [11][550/594], loss: 4.7816\n",
      "Epoch [11][555/594], loss: 4.7792\n",
      "Epoch [11][560/594], loss: 4.8459\n",
      "Epoch [11][565/594], loss: 4.8565\n",
      "Epoch [11][570/594], loss: 4.7982\n",
      "Epoch [11][575/594], loss: 4.7939\n",
      "Epoch [11][580/594], loss: 4.806\n",
      "Epoch [11][585/594], loss: 4.8165\n",
      "Epoch [11][590/594], loss: 4.863\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.863, val_loss: 4.8036\n",
      "Saving checkpoint at 11 epochs...\n",
      "Epoch [12][5/594], loss: 4.7523\n",
      "Epoch [12][10/594], loss: 4.7707\n",
      "Epoch [12][15/594], loss: 4.8495\n",
      "Epoch [12][20/594], loss: 4.8374\n",
      "Epoch [12][25/594], loss: 4.7497\n",
      "Epoch [12][30/594], loss: 4.8323\n",
      "Epoch [12][35/594], loss: 4.7738\n",
      "Epoch [12][40/594], loss: 4.7196\n",
      "Epoch [12][45/594], loss: 4.8041\n",
      "Epoch [12][50/594], loss: 4.8359\n",
      "Epoch [12][55/594], loss: 4.6933\n",
      "Epoch [12][60/594], loss: 4.7249\n",
      "Epoch [12][65/594], loss: 4.7577\n",
      "Epoch [12][70/594], loss: 4.8432\n",
      "Epoch [12][75/594], loss: 4.8279\n",
      "Epoch [12][80/594], loss: 4.8521\n",
      "Epoch [12][85/594], loss: 4.7951\n",
      "Epoch [12][90/594], loss: 4.8315\n",
      "Epoch [12][95/594], loss: 4.7113\n",
      "Epoch [12][100/594], loss: 4.7968\n",
      "Epoch [12][105/594], loss: 4.8235\n",
      "Epoch [12][110/594], loss: 4.7497\n",
      "Epoch [12][115/594], loss: 4.86\n",
      "Epoch [12][120/594], loss: 4.8027\n",
      "Epoch [12][125/594], loss: 4.8343\n",
      "Epoch [12][130/594], loss: 4.7175\n",
      "Epoch [12][135/594], loss: 4.8075\n",
      "Epoch [12][140/594], loss: 4.8816\n",
      "Epoch [12][145/594], loss: 4.6973\n",
      "Epoch [12][150/594], loss: 4.824\n",
      "Epoch [12][155/594], loss: 4.73\n",
      "Epoch [12][160/594], loss: 4.723\n",
      "Epoch [12][165/594], loss: 4.8932\n",
      "Epoch [12][170/594], loss: 4.8656\n",
      "Epoch [12][175/594], loss: 4.7336\n",
      "Epoch [12][180/594], loss: 4.7934\n",
      "Epoch [12][185/594], loss: 4.76\n",
      "Epoch [12][190/594], loss: 4.8077\n",
      "Epoch [12][195/594], loss: 4.7888\n",
      "Epoch [12][200/594], loss: 4.7887\n",
      "Epoch [12][205/594], loss: 4.69\n",
      "Epoch [12][210/594], loss: 4.8417\n",
      "Epoch [12][215/594], loss: 4.8947\n",
      "Epoch [12][220/594], loss: 4.7658\n",
      "Epoch [12][225/594], loss: 4.7847\n",
      "Epoch [12][230/594], loss: 4.8181\n",
      "Epoch [12][235/594], loss: 4.8074\n",
      "Epoch [12][240/594], loss: 4.8385\n",
      "Epoch [12][245/594], loss: 4.8268\n",
      "Epoch [12][250/594], loss: 4.7914\n",
      "Epoch [12][255/594], loss: 4.7884\n",
      "Epoch [12][260/594], loss: 4.7509\n",
      "Epoch [12][265/594], loss: 4.7276\n",
      "Epoch [12][270/594], loss: 4.8845\n",
      "Epoch [12][275/594], loss: 4.8121\n",
      "Epoch [12][280/594], loss: 4.6734\n",
      "Epoch [12][285/594], loss: 4.8286\n",
      "Epoch [12][290/594], loss: 4.7785\n",
      "Epoch [12][295/594], loss: 4.7356\n",
      "Epoch [12][300/594], loss: 4.7225\n",
      "Epoch [12][305/594], loss: 4.8187\n",
      "Epoch [12][310/594], loss: 4.7837\n",
      "Epoch [12][315/594], loss: 4.7924\n",
      "Epoch [12][320/594], loss: 4.8288\n",
      "Epoch [12][325/594], loss: 4.7609\n",
      "Epoch [12][330/594], loss: 4.8041\n",
      "Epoch [12][335/594], loss: 4.7547\n",
      "Epoch [12][340/594], loss: 4.7729\n",
      "Epoch [12][345/594], loss: 4.6843\n",
      "Epoch [12][350/594], loss: 4.793\n",
      "Epoch [12][355/594], loss: 4.7381\n",
      "Epoch [12][360/594], loss: 4.7259\n",
      "Epoch [12][365/594], loss: 4.9083\n",
      "Epoch [12][370/594], loss: 4.8477\n",
      "Epoch [12][375/594], loss: 4.7713\n",
      "Epoch [12][380/594], loss: 4.8628\n",
      "Epoch [12][385/594], loss: 4.8118\n",
      "Epoch [12][390/594], loss: 4.7808\n",
      "Epoch [12][395/594], loss: 4.8059\n",
      "Epoch [12][400/594], loss: 4.7639\n",
      "Epoch [12][405/594], loss: 4.8246\n",
      "Epoch [12][410/594], loss: 4.8431\n",
      "Epoch [12][415/594], loss: 4.6778\n",
      "Epoch [12][420/594], loss: 4.7173\n",
      "Epoch [12][425/594], loss: 4.8395\n",
      "Epoch [12][430/594], loss: 4.7544\n",
      "Epoch [12][435/594], loss: 4.7949\n",
      "Epoch [12][440/594], loss: 4.7938\n",
      "Epoch [12][445/594], loss: 4.8793\n",
      "Epoch [12][450/594], loss: 4.8035\n",
      "Epoch [12][455/594], loss: 4.8616\n",
      "Epoch [12][460/594], loss: 4.715\n",
      "Epoch [12][465/594], loss: 4.8863\n",
      "Epoch [12][470/594], loss: 4.8853\n",
      "Epoch [12][475/594], loss: 4.8988\n",
      "Epoch [12][480/594], loss: 4.8134\n",
      "Epoch [12][485/594], loss: 4.7779\n",
      "Epoch [12][490/594], loss: 4.8445\n",
      "Epoch [12][495/594], loss: 4.7604\n",
      "Epoch [12][500/594], loss: 4.8234\n",
      "Epoch [12][505/594], loss: 4.8197\n",
      "Epoch [12][510/594], loss: 4.7723\n",
      "Epoch [12][515/594], loss: 4.8239\n",
      "Epoch [12][520/594], loss: 4.6981\n",
      "Epoch [12][525/594], loss: 4.8333\n",
      "Epoch [12][530/594], loss: 4.8585\n",
      "Epoch [12][535/594], loss: 4.5952\n",
      "Epoch [12][540/594], loss: 4.8225\n",
      "Epoch [12][545/594], loss: 4.7709\n",
      "Epoch [12][550/594], loss: 4.8122\n",
      "Epoch [12][555/594], loss: 4.8292\n",
      "Epoch [12][560/594], loss: 4.7991\n",
      "Epoch [12][565/594], loss: 4.7414\n",
      "Epoch [12][570/594], loss: 4.8346\n",
      "Epoch [12][575/594], loss: 4.8521\n",
      "Epoch [12][580/594], loss: 4.705\n",
      "Epoch [12][585/594], loss: 4.8079\n",
      "Epoch [12][590/594], loss: 4.8007\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.8007, val_loss: 4.7844\n",
      "Saving checkpoint at 12 epochs...\n",
      "Epoch [13][5/594], loss: 4.8392\n",
      "Epoch [13][10/594], loss: 4.7625\n",
      "Epoch [13][15/594], loss: 4.8253\n",
      "Epoch [13][20/594], loss: 4.7221\n",
      "Epoch [13][25/594], loss: 4.7388\n",
      "Epoch [13][30/594], loss: 4.7981\n",
      "Epoch [13][35/594], loss: 4.8263\n",
      "Epoch [13][40/594], loss: 4.7923\n",
      "Epoch [13][45/594], loss: 4.6792\n",
      "Epoch [13][50/594], loss: 4.6627\n",
      "Epoch [13][55/594], loss: 4.8279\n",
      "Epoch [13][60/594], loss: 4.7813\n",
      "Epoch [13][65/594], loss: 4.6059\n",
      "Epoch [13][70/594], loss: 4.7928\n",
      "Epoch [13][75/594], loss: 4.8024\n",
      "Epoch [13][80/594], loss: 4.7933\n",
      "Epoch [13][85/594], loss: 4.7506\n",
      "Epoch [13][90/594], loss: 4.6869\n",
      "Epoch [13][95/594], loss: 4.8043\n",
      "Epoch [13][100/594], loss: 4.8656\n",
      "Epoch [13][105/594], loss: 4.7436\n",
      "Epoch [13][110/594], loss: 4.7738\n",
      "Epoch [13][115/594], loss: 4.7541\n",
      "Epoch [13][120/594], loss: 4.7545\n",
      "Epoch [13][125/594], loss: 4.7639\n",
      "Epoch [13][130/594], loss: 4.8328\n",
      "Epoch [13][135/594], loss: 4.8267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13][140/594], loss: 4.6946\n",
      "Epoch [13][145/594], loss: 4.7848\n",
      "Epoch [13][150/594], loss: 4.8176\n",
      "Epoch [13][155/594], loss: 4.7698\n",
      "Epoch [13][160/594], loss: 4.7373\n",
      "Epoch [13][165/594], loss: 4.7836\n",
      "Epoch [13][170/594], loss: 4.7237\n",
      "Epoch [13][175/594], loss: 4.7294\n",
      "Epoch [13][180/594], loss: 4.7573\n",
      "Epoch [13][185/594], loss: 4.7394\n",
      "Epoch [13][190/594], loss: 4.7863\n",
      "Epoch [13][195/594], loss: 4.7318\n",
      "Epoch [13][200/594], loss: 4.857\n",
      "Epoch [13][205/594], loss: 4.7299\n",
      "Epoch [13][210/594], loss: 4.7678\n",
      "Epoch [13][215/594], loss: 4.8065\n",
      "Epoch [13][220/594], loss: 4.718\n",
      "Epoch [13][225/594], loss: 4.833\n",
      "Epoch [13][230/594], loss: 4.8199\n",
      "Epoch [13][235/594], loss: 4.7623\n",
      "Epoch [13][240/594], loss: 4.7974\n",
      "Epoch [13][245/594], loss: 4.8119\n",
      "Epoch [13][250/594], loss: 4.8427\n",
      "Epoch [13][255/594], loss: 4.8319\n",
      "Epoch [13][260/594], loss: 4.7777\n",
      "Epoch [13][265/594], loss: 4.8308\n",
      "Epoch [13][270/594], loss: 4.6983\n",
      "Epoch [13][275/594], loss: 4.8193\n",
      "Epoch [13][280/594], loss: 4.8493\n",
      "Epoch [13][285/594], loss: 4.7705\n",
      "Epoch [13][290/594], loss: 4.7775\n",
      "Epoch [13][295/594], loss: 4.751\n",
      "Epoch [13][300/594], loss: 4.6517\n",
      "Epoch [13][305/594], loss: 4.7777\n",
      "Epoch [13][310/594], loss: 4.6952\n",
      "Epoch [13][315/594], loss: 4.7992\n",
      "Epoch [13][320/594], loss: 4.8142\n",
      "Epoch [13][325/594], loss: 4.7642\n",
      "Epoch [13][330/594], loss: 4.7702\n",
      "Epoch [13][335/594], loss: 4.764\n",
      "Epoch [13][340/594], loss: 4.7883\n",
      "Epoch [13][345/594], loss: 4.6184\n",
      "Epoch [13][350/594], loss: 4.7727\n",
      "Epoch [13][355/594], loss: 4.7993\n",
      "Epoch [13][360/594], loss: 4.8284\n",
      "Epoch [13][365/594], loss: 4.7698\n",
      "Epoch [13][370/594], loss: 4.792\n",
      "Epoch [13][375/594], loss: 4.7972\n",
      "Epoch [13][380/594], loss: 4.828\n",
      "Epoch [13][385/594], loss: 4.7571\n",
      "Epoch [13][390/594], loss: 4.8141\n",
      "Epoch [13][395/594], loss: 4.704\n",
      "Epoch [13][400/594], loss: 4.8393\n",
      "Epoch [13][405/594], loss: 4.7841\n",
      "Epoch [13][410/594], loss: 4.7652\n",
      "Epoch [13][415/594], loss: 4.804\n",
      "Epoch [13][420/594], loss: 4.7713\n",
      "Epoch [13][425/594], loss: 4.7615\n",
      "Epoch [13][430/594], loss: 4.7889\n",
      "Epoch [13][435/594], loss: 4.7806\n",
      "Epoch [13][440/594], loss: 4.7754\n",
      "Epoch [13][445/594], loss: 4.7857\n",
      "Epoch [13][450/594], loss: 4.8075\n",
      "Epoch [13][455/594], loss: 4.8265\n",
      "Epoch [13][460/594], loss: 4.7658\n",
      "Epoch [13][465/594], loss: 4.7748\n",
      "Epoch [13][470/594], loss: 4.8274\n",
      "Epoch [13][475/594], loss: 4.8371\n",
      "Epoch [13][480/594], loss: 4.6891\n",
      "Epoch [13][485/594], loss: 4.7024\n",
      "Epoch [13][490/594], loss: 4.7636\n",
      "Epoch [13][495/594], loss: 4.8216\n",
      "Epoch [13][500/594], loss: 4.6388\n",
      "Epoch [13][505/594], loss: 4.711\n",
      "Epoch [13][510/594], loss: 4.7443\n",
      "Epoch [13][515/594], loss: 4.793\n",
      "Epoch [13][520/594], loss: 4.7606\n",
      "Epoch [13][525/594], loss: 4.7768\n",
      "Epoch [13][530/594], loss: 4.7336\n",
      "Epoch [13][535/594], loss: 4.8052\n",
      "Epoch [13][540/594], loss: 4.7624\n",
      "Epoch [13][545/594], loss: 4.8319\n",
      "Epoch [13][550/594], loss: 4.8293\n",
      "Epoch [13][555/594], loss: 4.7063\n",
      "Epoch [13][560/594], loss: 4.8304\n",
      "Epoch [13][565/594], loss: 4.7971\n",
      "Epoch [13][570/594], loss: 4.7504\n",
      "Epoch [13][575/594], loss: 4.8545\n",
      "Epoch [13][580/594], loss: 4.7695\n",
      "Epoch [13][585/594], loss: 4.813\n",
      "Epoch [13][590/594], loss: 4.6878\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.6878, val_loss: 4.7638\n",
      "Saving checkpoint at 13 epochs...\n",
      "Epoch [14][5/594], loss: 4.6791\n",
      "Epoch [14][10/594], loss: 4.7365\n",
      "Epoch [14][15/594], loss: 4.7506\n",
      "Epoch [14][20/594], loss: 4.8052\n",
      "Epoch [14][25/594], loss: 4.7882\n",
      "Epoch [14][30/594], loss: 4.709\n",
      "Epoch [14][35/594], loss: 4.7898\n",
      "Epoch [14][40/594], loss: 4.7951\n",
      "Epoch [14][45/594], loss: 4.7721\n",
      "Epoch [14][50/594], loss: 4.8222\n",
      "Epoch [14][55/594], loss: 4.7325\n",
      "Epoch [14][60/594], loss: 4.7993\n",
      "Epoch [14][65/594], loss: 4.7584\n",
      "Epoch [14][70/594], loss: 4.7537\n",
      "Epoch [14][75/594], loss: 4.8175\n",
      "Epoch [14][80/594], loss: 4.752\n",
      "Epoch [14][85/594], loss: 4.7754\n",
      "Epoch [14][90/594], loss: 4.7967\n",
      "Epoch [14][95/594], loss: 4.7316\n",
      "Epoch [14][100/594], loss: 4.855\n",
      "Epoch [14][105/594], loss: 4.7081\n",
      "Epoch [14][110/594], loss: 4.7883\n",
      "Epoch [14][115/594], loss: 4.7818\n",
      "Epoch [14][120/594], loss: 4.8431\n",
      "Epoch [14][125/594], loss: 4.7337\n",
      "Epoch [14][130/594], loss: 4.7435\n",
      "Epoch [14][135/594], loss: 4.8042\n",
      "Epoch [14][140/594], loss: 4.6596\n",
      "Epoch [14][145/594], loss: 4.7257\n",
      "Epoch [14][150/594], loss: 4.797\n",
      "Epoch [14][155/594], loss: 4.7078\n",
      "Epoch [14][160/594], loss: 4.7391\n",
      "Epoch [14][165/594], loss: 4.6669\n",
      "Epoch [14][170/594], loss: 4.8079\n",
      "Epoch [14][175/594], loss: 4.6721\n",
      "Epoch [14][180/594], loss: 4.7506\n",
      "Epoch [14][185/594], loss: 4.7304\n",
      "Epoch [14][190/594], loss: 4.7488\n",
      "Epoch [14][195/594], loss: 4.7861\n",
      "Epoch [14][200/594], loss: 4.7806\n",
      "Epoch [14][205/594], loss: 4.7508\n",
      "Epoch [14][210/594], loss: 4.6978\n",
      "Epoch [14][215/594], loss: 4.7599\n",
      "Epoch [14][220/594], loss: 4.7173\n",
      "Epoch [14][225/594], loss: 4.6462\n",
      "Epoch [14][230/594], loss: 4.7131\n",
      "Epoch [14][235/594], loss: 4.7494\n",
      "Epoch [14][240/594], loss: 4.8003\n",
      "Epoch [14][245/594], loss: 4.8024\n",
      "Epoch [14][250/594], loss: 4.6918\n",
      "Epoch [14][255/594], loss: 4.7771\n",
      "Epoch [14][260/594], loss: 4.5379\n",
      "Epoch [14][265/594], loss: 4.7585\n",
      "Epoch [14][270/594], loss: 4.7908\n",
      "Epoch [14][275/594], loss: 4.6624\n",
      "Epoch [14][280/594], loss: 4.7677\n",
      "Epoch [14][285/594], loss: 4.7115\n",
      "Epoch [14][290/594], loss: 4.8138\n",
      "Epoch [14][295/594], loss: 4.8039\n",
      "Epoch [14][300/594], loss: 4.7464\n",
      "Epoch [14][305/594], loss: 4.7395\n",
      "Epoch [14][310/594], loss: 4.7647\n",
      "Epoch [14][315/594], loss: 4.7639\n",
      "Epoch [14][320/594], loss: 4.8149\n",
      "Epoch [14][325/594], loss: 4.8314\n",
      "Epoch [14][330/594], loss: 4.7241\n",
      "Epoch [14][335/594], loss: 4.7759\n",
      "Epoch [14][340/594], loss: 4.6906\n",
      "Epoch [14][345/594], loss: 4.742\n",
      "Epoch [14][350/594], loss: 4.8164\n",
      "Epoch [14][355/594], loss: 4.813\n",
      "Epoch [14][360/594], loss: 4.7661\n",
      "Epoch [14][365/594], loss: 4.8044\n",
      "Epoch [14][370/594], loss: 4.8738\n",
      "Epoch [14][375/594], loss: 4.8299\n",
      "Epoch [14][380/594], loss: 4.7353\n",
      "Epoch [14][385/594], loss: 4.7201\n",
      "Epoch [14][390/594], loss: 4.776\n",
      "Epoch [14][395/594], loss: 4.7035\n",
      "Epoch [14][400/594], loss: 4.7582\n",
      "Epoch [14][405/594], loss: 4.7517\n",
      "Epoch [14][410/594], loss: 4.8292\n",
      "Epoch [14][415/594], loss: 4.7865\n",
      "Epoch [14][420/594], loss: 4.817\n",
      "Epoch [14][425/594], loss: 4.6945\n",
      "Epoch [14][430/594], loss: 4.7485\n",
      "Epoch [14][435/594], loss: 4.7986\n",
      "Epoch [14][440/594], loss: 4.7431\n",
      "Epoch [14][445/594], loss: 4.7175\n",
      "Epoch [14][450/594], loss: 4.8148\n",
      "Epoch [14][455/594], loss: 4.7823\n",
      "Epoch [14][460/594], loss: 4.7832\n",
      "Epoch [14][465/594], loss: 4.8006\n",
      "Epoch [14][470/594], loss: 4.7338\n",
      "Epoch [14][475/594], loss: 4.7308\n",
      "Epoch [14][480/594], loss: 4.6373\n",
      "Epoch [14][485/594], loss: 4.6677\n",
      "Epoch [14][490/594], loss: 4.6438\n",
      "Epoch [14][495/594], loss: 4.8093\n",
      "Epoch [14][500/594], loss: 4.7549\n",
      "Epoch [14][505/594], loss: 4.8068\n",
      "Epoch [14][510/594], loss: 4.7411\n",
      "Epoch [14][515/594], loss: 4.8179\n",
      "Epoch [14][520/594], loss: 4.7386\n",
      "Epoch [14][525/594], loss: 4.7522\n",
      "Epoch [14][530/594], loss: 4.8265\n",
      "Epoch [14][535/594], loss: 4.7734\n",
      "Epoch [14][540/594], loss: 4.7243\n",
      "Epoch [14][545/594], loss: 4.7875\n",
      "Epoch [14][550/594], loss: 4.766\n",
      "Epoch [14][555/594], loss: 4.7915\n",
      "Epoch [14][560/594], loss: 4.6688\n",
      "Epoch [14][565/594], loss: 4.7546\n",
      "Epoch [14][570/594], loss: 4.7774\n",
      "Epoch [14][575/594], loss: 4.7848\n",
      "Epoch [14][580/594], loss: 4.7108\n",
      "Epoch [14][585/594], loss: 4.8585\n",
      "Epoch [14][590/594], loss: 4.8069\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07752, train_loss: 4.8069, val_loss: 4.748\n",
      "Saving checkpoint at 14 epochs...\n",
      "Epoch [15][5/594], loss: 4.7538\n",
      "Epoch [15][10/594], loss: 4.7682\n",
      "Epoch [15][15/594], loss: 4.5763\n",
      "Epoch [15][20/594], loss: 4.7418\n",
      "Epoch [15][25/594], loss: 4.6722\n",
      "Epoch [15][30/594], loss: 4.7145\n",
      "Epoch [15][35/594], loss: 4.7535\n",
      "Epoch [15][40/594], loss: 4.7624\n",
      "Epoch [15][45/594], loss: 4.8057\n",
      "Epoch [15][50/594], loss: 4.6817\n",
      "Epoch [15][55/594], loss: 4.8515\n",
      "Epoch [15][60/594], loss: 4.7178\n",
      "Epoch [15][65/594], loss: 4.6381\n",
      "Epoch [15][70/594], loss: 4.6447\n",
      "Epoch [15][75/594], loss: 4.769\n",
      "Epoch [15][80/594], loss: 4.7217\n",
      "Epoch [15][85/594], loss: 4.7776\n",
      "Epoch [15][90/594], loss: 4.7463\n",
      "Epoch [15][95/594], loss: 4.77\n",
      "Epoch [15][100/594], loss: 4.6939\n",
      "Epoch [15][105/594], loss: 4.7892\n",
      "Epoch [15][110/594], loss: 4.7253\n",
      "Epoch [15][115/594], loss: 4.6494\n",
      "Epoch [15][120/594], loss: 4.6179\n",
      "Epoch [15][125/594], loss: 4.7439\n",
      "Epoch [15][130/594], loss: 4.7291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15][135/594], loss: 4.7397\n",
      "Epoch [15][140/594], loss: 4.8135\n",
      "Epoch [15][145/594], loss: 4.7986\n",
      "Epoch [15][150/594], loss: 4.7491\n",
      "Epoch [15][155/594], loss: 4.7562\n",
      "Epoch [15][160/594], loss: 4.7937\n",
      "Epoch [15][165/594], loss: 4.8228\n",
      "Epoch [15][170/594], loss: 4.7964\n",
      "Epoch [15][175/594], loss: 4.6801\n",
      "Epoch [15][180/594], loss: 4.6829\n",
      "Epoch [15][185/594], loss: 4.5789\n",
      "Epoch [15][190/594], loss: 4.8039\n",
      "Epoch [15][195/594], loss: 4.6605\n",
      "Epoch [15][200/594], loss: 4.743\n",
      "Epoch [15][205/594], loss: 4.7246\n",
      "Epoch [15][210/594], loss: 4.8288\n",
      "Epoch [15][215/594], loss: 4.8308\n",
      "Epoch [15][220/594], loss: 4.7753\n",
      "Epoch [15][225/594], loss: 4.7835\n",
      "Epoch [15][230/594], loss: 4.7499\n",
      "Epoch [15][235/594], loss: 4.8067\n",
      "Epoch [15][240/594], loss: 4.7182\n",
      "Epoch [15][245/594], loss: 4.6273\n",
      "Epoch [15][250/594], loss: 4.7731\n",
      "Epoch [15][255/594], loss: 4.82\n",
      "Epoch [15][260/594], loss: 4.7078\n",
      "Epoch [15][265/594], loss: 4.6307\n",
      "Epoch [15][270/594], loss: 4.7553\n",
      "Epoch [15][275/594], loss: 4.8468\n",
      "Epoch [15][280/594], loss: 4.749\n",
      "Epoch [15][285/594], loss: 4.5997\n",
      "Epoch [15][290/594], loss: 4.6091\n",
      "Epoch [15][295/594], loss: 4.8174\n",
      "Epoch [15][300/594], loss: 4.6554\n",
      "Epoch [15][305/594], loss: 4.8553\n",
      "Epoch [15][310/594], loss: 4.7548\n",
      "Epoch [15][315/594], loss: 4.7375\n",
      "Epoch [15][320/594], loss: 4.7422\n",
      "Epoch [15][325/594], loss: 4.8102\n",
      "Epoch [15][330/594], loss: 4.7885\n",
      "Epoch [15][335/594], loss: 4.7283\n",
      "Epoch [15][340/594], loss: 4.6341\n",
      "Epoch [15][345/594], loss: 4.7684\n",
      "Epoch [15][350/594], loss: 4.718\n",
      "Epoch [15][355/594], loss: 4.7229\n",
      "Epoch [15][360/594], loss: 4.6933\n",
      "Epoch [15][365/594], loss: 4.8126\n",
      "Epoch [15][370/594], loss: 4.7449\n",
      "Epoch [15][375/594], loss: 4.8014\n",
      "Epoch [15][380/594], loss: 4.7731\n",
      "Epoch [15][385/594], loss: 4.7112\n",
      "Epoch [15][390/594], loss: 4.7528\n",
      "Epoch [15][395/594], loss: 4.7752\n",
      "Epoch [15][400/594], loss: 4.754\n",
      "Epoch [15][405/594], loss: 4.8053\n",
      "Epoch [15][410/594], loss: 4.7453\n",
      "Epoch [15][415/594], loss: 4.7359\n",
      "Epoch [15][420/594], loss: 4.7592\n",
      "Epoch [15][425/594], loss: 4.716\n",
      "Epoch [15][430/594], loss: 4.7198\n",
      "Epoch [15][435/594], loss: 4.7652\n",
      "Epoch [15][440/594], loss: 4.7967\n",
      "Epoch [15][445/594], loss: 4.8225\n",
      "Epoch [15][450/594], loss: 4.8447\n",
      "Epoch [15][455/594], loss: 4.6674\n",
      "Epoch [15][460/594], loss: 4.7645\n",
      "Epoch [15][465/594], loss: 4.8063\n",
      "Epoch [15][470/594], loss: 4.6633\n",
      "Epoch [15][475/594], loss: 4.6866\n",
      "Epoch [15][480/594], loss: 4.7948\n",
      "Epoch [15][485/594], loss: 4.7132\n",
      "Epoch [15][490/594], loss: 4.7878\n",
      "Epoch [15][495/594], loss: 4.7688\n",
      "Epoch [15][500/594], loss: 4.7366\n",
      "Epoch [15][505/594], loss: 4.8128\n",
      "Epoch [15][510/594], loss: 4.7859\n",
      "Epoch [15][515/594], loss: 4.6899\n",
      "Epoch [15][520/594], loss: 4.8231\n",
      "Epoch [15][525/594], loss: 4.7748\n",
      "Epoch [15][530/594], loss: 4.7752\n",
      "Epoch [15][535/594], loss: 4.6846\n",
      "Epoch [15][540/594], loss: 4.7799\n",
      "Epoch [15][545/594], loss: 4.761\n",
      "Epoch [15][550/594], loss: 4.7879\n",
      "Epoch [15][555/594], loss: 4.7584\n",
      "Epoch [15][560/594], loss: 4.7131\n",
      "Epoch [15][565/594], loss: 4.7213\n",
      "Epoch [15][570/594], loss: 4.743\n",
      "Epoch [15][575/594], loss: 4.7783\n",
      "Epoch [15][580/594], loss: 4.7594\n",
      "Epoch [15][585/594], loss: 4.7768\n",
      "Epoch [15][590/594], loss: 4.7469\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.7469, val_loss: 4.7325\n",
      "Saving checkpoint at 15 epochs...\n",
      "Epoch [16][5/594], loss: 4.7826\n",
      "Epoch [16][10/594], loss: 4.7171\n",
      "Epoch [16][15/594], loss: 4.8031\n",
      "Epoch [16][20/594], loss: 4.7328\n",
      "Epoch [16][25/594], loss: 4.6991\n",
      "Epoch [16][30/594], loss: 4.7297\n",
      "Epoch [16][35/594], loss: 4.7725\n",
      "Epoch [16][40/594], loss: 4.748\n",
      "Epoch [16][45/594], loss: 4.7426\n",
      "Epoch [16][50/594], loss: 4.7255\n",
      "Epoch [16][55/594], loss: 4.6477\n",
      "Epoch [16][60/594], loss: 4.6826\n",
      "Epoch [16][65/594], loss: 4.6784\n",
      "Epoch [16][70/594], loss: 4.7222\n",
      "Epoch [16][75/594], loss: 4.7606\n",
      "Epoch [16][80/594], loss: 4.8078\n",
      "Epoch [16][85/594], loss: 4.6773\n",
      "Epoch [16][90/594], loss: 4.7625\n",
      "Epoch [16][95/594], loss: 4.7832\n",
      "Epoch [16][100/594], loss: 4.7155\n",
      "Epoch [16][105/594], loss: 4.6909\n",
      "Epoch [16][110/594], loss: 4.6819\n",
      "Epoch [16][115/594], loss: 4.7779\n",
      "Epoch [16][120/594], loss: 4.804\n",
      "Epoch [16][125/594], loss: 4.7603\n",
      "Epoch [16][130/594], loss: 4.731\n",
      "Epoch [16][135/594], loss: 4.6945\n",
      "Epoch [16][140/594], loss: 4.7431\n",
      "Epoch [16][145/594], loss: 4.6839\n",
      "Epoch [16][150/594], loss: 4.755\n",
      "Epoch [16][155/594], loss: 4.7555\n",
      "Epoch [16][160/594], loss: 4.7118\n",
      "Epoch [16][165/594], loss: 4.7706\n",
      "Epoch [16][170/594], loss: 4.7354\n",
      "Epoch [16][175/594], loss: 4.7\n",
      "Epoch [16][180/594], loss: 4.7793\n",
      "Epoch [16][185/594], loss: 4.7443\n",
      "Epoch [16][190/594], loss: 4.7338\n",
      "Epoch [16][195/594], loss: 4.7628\n",
      "Epoch [16][200/594], loss: 4.7627\n",
      "Epoch [16][205/594], loss: 4.6901\n",
      "Epoch [16][210/594], loss: 4.6998\n",
      "Epoch [16][215/594], loss: 4.7992\n",
      "Epoch [16][220/594], loss: 4.697\n",
      "Epoch [16][225/594], loss: 4.7981\n",
      "Epoch [16][230/594], loss: 4.6924\n",
      "Epoch [16][235/594], loss: 4.8028\n",
      "Epoch [16][240/594], loss: 4.768\n",
      "Epoch [16][245/594], loss: 4.7626\n",
      "Epoch [16][250/594], loss: 4.7796\n",
      "Epoch [16][255/594], loss: 4.7593\n",
      "Epoch [16][260/594], loss: 4.7066\n",
      "Epoch [16][265/594], loss: 4.7456\n",
      "Epoch [16][270/594], loss: 4.7643\n",
      "Epoch [16][275/594], loss: 4.7346\n",
      "Epoch [16][280/594], loss: 4.666\n",
      "Epoch [16][285/594], loss: 4.7318\n",
      "Epoch [16][290/594], loss: 4.7641\n",
      "Epoch [16][295/594], loss: 4.7571\n",
      "Epoch [16][300/594], loss: 4.7157\n",
      "Epoch [16][305/594], loss: 4.7006\n",
      "Epoch [16][310/594], loss: 4.7375\n",
      "Epoch [16][315/594], loss: 4.7032\n",
      "Epoch [16][320/594], loss: 4.6682\n",
      "Epoch [16][325/594], loss: 4.6467\n",
      "Epoch [16][330/594], loss: 4.7746\n",
      "Epoch [16][335/594], loss: 4.7048\n",
      "Epoch [16][340/594], loss: 4.7366\n",
      "Epoch [16][345/594], loss: 4.6944\n",
      "Epoch [16][350/594], loss: 4.6917\n",
      "Epoch [16][355/594], loss: 4.7654\n",
      "Epoch [16][360/594], loss: 4.7479\n",
      "Epoch [16][365/594], loss: 4.7485\n",
      "Epoch [16][370/594], loss: 4.7638\n",
      "Epoch [16][375/594], loss: 4.6735\n",
      "Epoch [16][380/594], loss: 4.6644\n",
      "Epoch [16][385/594], loss: 4.7847\n",
      "Epoch [16][390/594], loss: 4.7632\n",
      "Epoch [16][395/594], loss: 4.7929\n",
      "Epoch [16][400/594], loss: 4.7566\n",
      "Epoch [16][405/594], loss: 4.6528\n",
      "Epoch [16][410/594], loss: 4.6337\n",
      "Epoch [16][415/594], loss: 4.6507\n",
      "Epoch [16][420/594], loss: 4.7851\n",
      "Epoch [16][425/594], loss: 4.7889\n",
      "Epoch [16][430/594], loss: 4.8158\n",
      "Epoch [16][435/594], loss: 4.805\n",
      "Epoch [16][440/594], loss: 4.7683\n",
      "Epoch [16][445/594], loss: 4.772\n",
      "Epoch [16][450/594], loss: 4.7928\n",
      "Epoch [16][455/594], loss: 4.6953\n",
      "Epoch [16][460/594], loss: 4.6834\n",
      "Epoch [16][465/594], loss: 4.7357\n",
      "Epoch [16][470/594], loss: 4.7826\n",
      "Epoch [16][475/594], loss: 4.7336\n",
      "Epoch [16][480/594], loss: 4.7285\n",
      "Epoch [16][485/594], loss: 4.7046\n",
      "Epoch [16][490/594], loss: 4.6978\n",
      "Epoch [16][495/594], loss: 4.756\n",
      "Epoch [16][500/594], loss: 4.6909\n",
      "Epoch [16][505/594], loss: 4.7221\n",
      "Epoch [16][510/594], loss: 4.7865\n",
      "Epoch [16][515/594], loss: 4.6808\n",
      "Epoch [16][520/594], loss: 4.7428\n",
      "Epoch [16][525/594], loss: 4.7384\n",
      "Epoch [16][530/594], loss: 4.7944\n",
      "Epoch [16][535/594], loss: 4.7475\n",
      "Epoch [16][540/594], loss: 4.7687\n",
      "Epoch [16][545/594], loss: 4.7573\n",
      "Epoch [16][550/594], loss: 4.6394\n",
      "Epoch [16][555/594], loss: 4.6434\n",
      "Epoch [16][560/594], loss: 4.7785\n",
      "Epoch [16][565/594], loss: 4.7489\n",
      "Epoch [16][570/594], loss: 4.7836\n",
      "Epoch [16][575/594], loss: 4.7735\n",
      "Epoch [16][580/594], loss: 4.7009\n",
      "Epoch [16][585/594], loss: 4.6244\n",
      "Epoch [16][590/594], loss: 4.6983\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.6983, val_loss: 4.724\n",
      "Saving checkpoint at 16 epochs...\n",
      "Epoch [17][5/594], loss: 4.678\n",
      "Epoch [17][10/594], loss: 4.7334\n",
      "Epoch [17][15/594], loss: 4.7843\n",
      "Epoch [17][20/594], loss: 4.6918\n",
      "Epoch [17][25/594], loss: 4.7297\n",
      "Epoch [17][30/594], loss: 4.705\n",
      "Epoch [17][35/594], loss: 4.7018\n",
      "Epoch [17][40/594], loss: 4.7642\n",
      "Epoch [17][45/594], loss: 4.7655\n",
      "Epoch [17][50/594], loss: 4.7352\n",
      "Epoch [17][55/594], loss: 4.7119\n",
      "Epoch [17][60/594], loss: 4.6925\n",
      "Epoch [17][65/594], loss: 4.8048\n",
      "Epoch [17][70/594], loss: 4.7384\n",
      "Epoch [17][75/594], loss: 4.7253\n",
      "Epoch [17][80/594], loss: 4.7663\n",
      "Epoch [17][85/594], loss: 4.7037\n",
      "Epoch [17][90/594], loss: 4.6689\n",
      "Epoch [17][95/594], loss: 4.7339\n",
      "Epoch [17][100/594], loss: 4.7078\n",
      "Epoch [17][105/594], loss: 4.7655\n",
      "Epoch [17][110/594], loss: 4.7199\n",
      "Epoch [17][115/594], loss: 4.613\n",
      "Epoch [17][120/594], loss: 4.695\n",
      "Epoch [17][125/594], loss: 4.6989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][130/594], loss: 4.7342\n",
      "Epoch [17][135/594], loss: 4.7762\n",
      "Epoch [17][140/594], loss: 4.7592\n",
      "Epoch [17][145/594], loss: 4.7568\n",
      "Epoch [17][150/594], loss: 4.7375\n",
      "Epoch [17][155/594], loss: 4.731\n",
      "Epoch [17][160/594], loss: 4.7726\n",
      "Epoch [17][165/594], loss: 4.7732\n",
      "Epoch [17][170/594], loss: 4.6982\n",
      "Epoch [17][175/594], loss: 4.7309\n",
      "Epoch [17][180/594], loss: 4.7382\n",
      "Epoch [17][185/594], loss: 4.7929\n",
      "Epoch [17][190/594], loss: 4.7451\n",
      "Epoch [17][195/594], loss: 4.7219\n",
      "Epoch [17][200/594], loss: 4.6686\n",
      "Epoch [17][205/594], loss: 4.7264\n",
      "Epoch [17][210/594], loss: 4.736\n",
      "Epoch [17][215/594], loss: 4.7024\n",
      "Epoch [17][220/594], loss: 4.7724\n",
      "Epoch [17][225/594], loss: 4.7482\n",
      "Epoch [17][230/594], loss: 4.7247\n",
      "Epoch [17][235/594], loss: 4.7256\n",
      "Epoch [17][240/594], loss: 4.7099\n",
      "Epoch [17][245/594], loss: 4.7047\n",
      "Epoch [17][250/594], loss: 4.7087\n",
      "Epoch [17][255/594], loss: 4.777\n",
      "Epoch [17][260/594], loss: 4.6676\n",
      "Epoch [17][265/594], loss: 4.7025\n",
      "Epoch [17][270/594], loss: 4.7373\n",
      "Epoch [17][275/594], loss: 4.774\n",
      "Epoch [17][280/594], loss: 4.7444\n",
      "Epoch [17][285/594], loss: 4.7048\n",
      "Epoch [17][290/594], loss: 4.7545\n",
      "Epoch [17][295/594], loss: 4.7353\n",
      "Epoch [17][300/594], loss: 4.6848\n",
      "Epoch [17][305/594], loss: 4.6611\n",
      "Epoch [17][310/594], loss: 4.7183\n",
      "Epoch [17][315/594], loss: 4.6819\n",
      "Epoch [17][320/594], loss: 4.6488\n",
      "Epoch [17][325/594], loss: 4.7392\n",
      "Epoch [17][330/594], loss: 4.7515\n",
      "Epoch [17][335/594], loss: 4.754\n",
      "Epoch [17][340/594], loss: 4.8129\n",
      "Epoch [17][345/594], loss: 4.6471\n",
      "Epoch [17][350/594], loss: 4.6536\n",
      "Epoch [17][355/594], loss: 4.7377\n",
      "Epoch [17][360/594], loss: 4.7581\n",
      "Epoch [17][365/594], loss: 4.6875\n",
      "Epoch [17][370/594], loss: 4.6578\n",
      "Epoch [17][375/594], loss: 4.771\n",
      "Epoch [17][380/594], loss: 4.7724\n",
      "Epoch [17][385/594], loss: 4.7234\n",
      "Epoch [17][390/594], loss: 4.6331\n",
      "Epoch [17][395/594], loss: 4.6356\n",
      "Epoch [17][400/594], loss: 4.6602\n",
      "Epoch [17][405/594], loss: 4.5812\n",
      "Epoch [17][410/594], loss: 4.6338\n",
      "Epoch [17][415/594], loss: 4.7748\n",
      "Epoch [17][420/594], loss: 4.6984\n",
      "Epoch [17][425/594], loss: 4.6967\n",
      "Epoch [17][430/594], loss: 4.5961\n",
      "Epoch [17][435/594], loss: 4.6653\n",
      "Epoch [17][440/594], loss: 4.8339\n",
      "Epoch [17][445/594], loss: 4.7402\n",
      "Epoch [17][450/594], loss: 4.7564\n",
      "Epoch [17][455/594], loss: 4.7042\n",
      "Epoch [17][460/594], loss: 4.7321\n",
      "Epoch [17][465/594], loss: 4.6355\n",
      "Epoch [17][470/594], loss: 4.6641\n",
      "Epoch [17][475/594], loss: 4.7716\n",
      "Epoch [17][480/594], loss: 4.6618\n",
      "Epoch [17][485/594], loss: 4.7576\n",
      "Epoch [17][490/594], loss: 4.7668\n",
      "Epoch [17][495/594], loss: 4.7514\n",
      "Epoch [17][500/594], loss: 4.7462\n",
      "Epoch [17][505/594], loss: 4.8073\n",
      "Epoch [17][510/594], loss: 4.7712\n",
      "Epoch [17][515/594], loss: 4.7169\n",
      "Epoch [17][520/594], loss: 4.7485\n",
      "Epoch [17][525/594], loss: 4.8318\n",
      "Epoch [17][530/594], loss: 4.6976\n",
      "Epoch [17][535/594], loss: 4.655\n",
      "Epoch [17][540/594], loss: 4.7651\n",
      "Epoch [17][545/594], loss: 4.7561\n",
      "Epoch [17][550/594], loss: 4.7379\n",
      "Epoch [17][555/594], loss: 4.6981\n",
      "Epoch [17][560/594], loss: 4.72\n",
      "Epoch [17][565/594], loss: 4.7753\n",
      "Epoch [17][570/594], loss: 4.7806\n",
      "Epoch [17][575/594], loss: 4.7677\n",
      "Epoch [17][580/594], loss: 4.8068\n",
      "Epoch [17][585/594], loss: 4.5933\n",
      "Epoch [17][590/594], loss: 4.7689\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.7689, val_loss: 4.7168\n",
      "Saving checkpoint at 17 epochs...\n",
      "Epoch [18][5/594], loss: 4.7088\n",
      "Epoch [18][10/594], loss: 4.7225\n",
      "Epoch [18][15/594], loss: 4.6049\n",
      "Epoch [18][20/594], loss: 4.6878\n",
      "Epoch [18][25/594], loss: 4.7308\n",
      "Epoch [18][30/594], loss: 4.7168\n",
      "Epoch [18][35/594], loss: 4.6595\n",
      "Epoch [18][40/594], loss: 4.709\n",
      "Epoch [18][45/594], loss: 4.7163\n",
      "Epoch [18][50/594], loss: 4.7607\n",
      "Epoch [18][55/594], loss: 4.6887\n",
      "Epoch [18][60/594], loss: 4.6862\n",
      "Epoch [18][65/594], loss: 4.5003\n",
      "Epoch [18][70/594], loss: 4.7037\n",
      "Epoch [18][75/594], loss: 4.7109\n",
      "Epoch [18][80/594], loss: 4.7068\n",
      "Epoch [18][85/594], loss: 4.6153\n",
      "Epoch [18][90/594], loss: 4.7723\n",
      "Epoch [18][95/594], loss: 4.62\n",
      "Epoch [18][100/594], loss: 4.6823\n",
      "Epoch [18][105/594], loss: 4.694\n",
      "Epoch [18][110/594], loss: 4.7228\n",
      "Epoch [18][115/594], loss: 4.7081\n",
      "Epoch [18][120/594], loss: 4.7312\n",
      "Epoch [18][125/594], loss: 4.7024\n",
      "Epoch [18][130/594], loss: 4.7372\n",
      "Epoch [18][135/594], loss: 4.6835\n",
      "Epoch [18][140/594], loss: 4.8026\n",
      "Epoch [18][145/594], loss: 4.7648\n",
      "Epoch [18][150/594], loss: 4.6762\n",
      "Epoch [18][155/594], loss: 4.6742\n",
      "Epoch [18][160/594], loss: 4.6981\n",
      "Epoch [18][165/594], loss: 4.7814\n",
      "Epoch [18][170/594], loss: 4.6515\n",
      "Epoch [18][175/594], loss: 4.7157\n",
      "Epoch [18][180/594], loss: 4.7535\n",
      "Epoch [18][185/594], loss: 4.6886\n",
      "Epoch [18][190/594], loss: 4.7395\n",
      "Epoch [18][195/594], loss: 4.6961\n",
      "Epoch [18][200/594], loss: 4.7613\n",
      "Epoch [18][205/594], loss: 4.7822\n",
      "Epoch [18][210/594], loss: 4.7517\n",
      "Epoch [18][215/594], loss: 4.6977\n",
      "Epoch [18][220/594], loss: 4.682\n",
      "Epoch [18][225/594], loss: 4.7125\n",
      "Epoch [18][230/594], loss: 4.7363\n",
      "Epoch [18][235/594], loss: 4.8078\n",
      "Epoch [18][240/594], loss: 4.6244\n",
      "Epoch [18][245/594], loss: 4.7921\n",
      "Epoch [18][250/594], loss: 4.6676\n",
      "Epoch [18][255/594], loss: 4.8093\n",
      "Epoch [18][260/594], loss: 4.7129\n",
      "Epoch [18][265/594], loss: 4.6569\n",
      "Epoch [18][270/594], loss: 4.6847\n",
      "Epoch [18][275/594], loss: 4.7415\n",
      "Epoch [18][280/594], loss: 4.6422\n",
      "Epoch [18][285/594], loss: 4.6294\n",
      "Epoch [18][290/594], loss: 4.8018\n",
      "Epoch [18][295/594], loss: 4.7907\n",
      "Epoch [18][300/594], loss: 4.6737\n",
      "Epoch [18][305/594], loss: 4.7522\n",
      "Epoch [18][310/594], loss: 4.6903\n",
      "Epoch [18][315/594], loss: 4.7108\n",
      "Epoch [18][320/594], loss: 4.6752\n",
      "Epoch [18][325/594], loss: 4.7558\n",
      "Epoch [18][330/594], loss: 4.7564\n",
      "Epoch [18][335/594], loss: 4.6319\n",
      "Epoch [18][340/594], loss: 4.7748\n",
      "Epoch [18][345/594], loss: 4.6601\n",
      "Epoch [18][350/594], loss: 4.7887\n",
      "Epoch [18][355/594], loss: 4.7415\n",
      "Epoch [18][360/594], loss: 4.7437\n",
      "Epoch [18][365/594], loss: 4.7644\n",
      "Epoch [18][370/594], loss: 4.741\n",
      "Epoch [18][375/594], loss: 4.5106\n",
      "Epoch [18][380/594], loss: 4.6378\n",
      "Epoch [18][385/594], loss: 4.8078\n",
      "Epoch [18][390/594], loss: 4.7324\n",
      "Epoch [18][395/594], loss: 4.7496\n",
      "Epoch [18][400/594], loss: 4.6379\n",
      "Epoch [18][405/594], loss: 4.7817\n",
      "Epoch [18][410/594], loss: 4.7148\n",
      "Epoch [18][415/594], loss: 4.7006\n",
      "Epoch [18][420/594], loss: 4.6894\n",
      "Epoch [18][425/594], loss: 4.7133\n",
      "Epoch [18][430/594], loss: 4.7723\n",
      "Epoch [18][435/594], loss: 4.7597\n",
      "Epoch [18][440/594], loss: 4.7046\n",
      "Epoch [18][445/594], loss: 4.7679\n",
      "Epoch [18][450/594], loss: 4.7724\n",
      "Epoch [18][455/594], loss: 4.6226\n",
      "Epoch [18][460/594], loss: 4.7251\n",
      "Epoch [18][465/594], loss: 4.7354\n",
      "Epoch [18][470/594], loss: 4.7167\n",
      "Epoch [18][475/594], loss: 4.7304\n",
      "Epoch [18][480/594], loss: 4.6665\n",
      "Epoch [18][485/594], loss: 4.7604\n",
      "Epoch [18][490/594], loss: 4.7806\n",
      "Epoch [18][495/594], loss: 4.6901\n",
      "Epoch [18][500/594], loss: 4.7316\n",
      "Epoch [18][505/594], loss: 4.7788\n",
      "Epoch [18][510/594], loss: 4.7442\n",
      "Epoch [18][515/594], loss: 4.7466\n",
      "Epoch [18][520/594], loss: 4.6812\n",
      "Epoch [18][525/594], loss: 4.7101\n",
      "Epoch [18][530/594], loss: 4.8021\n",
      "Epoch [18][535/594], loss: 4.7185\n",
      "Epoch [18][540/594], loss: 4.778\n",
      "Epoch [18][545/594], loss: 4.6844\n",
      "Epoch [18][550/594], loss: 4.7548\n",
      "Epoch [18][555/594], loss: 4.6873\n",
      "Epoch [18][560/594], loss: 4.7529\n",
      "Epoch [18][565/594], loss: 4.7673\n",
      "Epoch [18][570/594], loss: 4.7802\n",
      "Epoch [18][575/594], loss: 4.6475\n",
      "Epoch [18][580/594], loss: 4.7529\n",
      "Epoch [18][585/594], loss: 4.6534\n",
      "Epoch [18][590/594], loss: 4.7608\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.7608, val_loss: 4.7067\n",
      "Saving checkpoint at 18 epochs...\n",
      "Epoch [19][5/594], loss: 4.6819\n",
      "Epoch [19][10/594], loss: 4.6742\n",
      "Epoch [19][15/594], loss: 4.6605\n",
      "Epoch [19][20/594], loss: 4.7493\n",
      "Epoch [19][25/594], loss: 4.657\n",
      "Epoch [19][30/594], loss: 4.6809\n",
      "Epoch [19][35/594], loss: 4.615\n",
      "Epoch [19][40/594], loss: 4.7267\n",
      "Epoch [19][45/594], loss: 4.6913\n",
      "Epoch [19][50/594], loss: 4.7699\n",
      "Epoch [19][55/594], loss: 4.5702\n",
      "Epoch [19][60/594], loss: 4.7374\n",
      "Epoch [19][65/594], loss: 4.7554\n",
      "Epoch [19][70/594], loss: 4.6984\n",
      "Epoch [19][75/594], loss: 4.7518\n",
      "Epoch [19][80/594], loss: 4.7224\n",
      "Epoch [19][85/594], loss: 4.6876\n",
      "Epoch [19][90/594], loss: 4.7321\n",
      "Epoch [19][95/594], loss: 4.6501\n",
      "Epoch [19][100/594], loss: 4.768\n",
      "Epoch [19][105/594], loss: 4.7561\n",
      "Epoch [19][110/594], loss: 4.7242\n",
      "Epoch [19][115/594], loss: 4.7463\n",
      "Epoch [19][120/594], loss: 4.6806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19][125/594], loss: 4.6996\n",
      "Epoch [19][130/594], loss: 4.7669\n",
      "Epoch [19][135/594], loss: 4.6514\n",
      "Epoch [19][140/594], loss: 4.6167\n",
      "Epoch [19][145/594], loss: 4.6199\n",
      "Epoch [19][150/594], loss: 4.7408\n",
      "Epoch [19][155/594], loss: 4.677\n",
      "Epoch [19][160/594], loss: 4.6746\n",
      "Epoch [19][165/594], loss: 4.7389\n",
      "Epoch [19][170/594], loss: 4.7403\n",
      "Epoch [19][175/594], loss: 4.6818\n",
      "Epoch [19][180/594], loss: 4.6324\n",
      "Epoch [19][185/594], loss: 4.641\n",
      "Epoch [19][190/594], loss: 4.6395\n",
      "Epoch [19][195/594], loss: 4.5623\n",
      "Epoch [19][200/594], loss: 4.7919\n",
      "Epoch [19][205/594], loss: 4.7786\n",
      "Epoch [19][210/594], loss: 4.781\n",
      "Epoch [19][215/594], loss: 4.7554\n",
      "Epoch [19][220/594], loss: 4.7491\n",
      "Epoch [19][225/594], loss: 4.6226\n",
      "Epoch [19][230/594], loss: 4.7073\n",
      "Epoch [19][235/594], loss: 4.6284\n",
      "Epoch [19][240/594], loss: 4.7025\n",
      "Epoch [19][245/594], loss: 4.6319\n",
      "Epoch [19][250/594], loss: 4.72\n",
      "Epoch [19][255/594], loss: 4.7423\n",
      "Epoch [19][260/594], loss: 4.7243\n",
      "Epoch [19][265/594], loss: 4.7116\n",
      "Epoch [19][270/594], loss: 4.7667\n",
      "Epoch [19][275/594], loss: 4.7207\n",
      "Epoch [19][280/594], loss: 4.7122\n",
      "Epoch [19][285/594], loss: 4.6755\n",
      "Epoch [19][290/594], loss: 4.6459\n",
      "Epoch [19][295/594], loss: 4.7343\n",
      "Epoch [19][300/594], loss: 4.7577\n",
      "Epoch [19][305/594], loss: 4.6396\n",
      "Epoch [19][310/594], loss: 4.6234\n",
      "Epoch [19][315/594], loss: 4.742\n",
      "Epoch [19][320/594], loss: 4.6432\n",
      "Epoch [19][325/594], loss: 4.6169\n",
      "Epoch [19][330/594], loss: 4.7264\n",
      "Epoch [19][335/594], loss: 4.7605\n",
      "Epoch [19][340/594], loss: 4.7428\n",
      "Epoch [19][345/594], loss: 4.7368\n",
      "Epoch [19][350/594], loss: 4.7382\n",
      "Epoch [19][355/594], loss: 4.6498\n",
      "Epoch [19][360/594], loss: 4.7235\n",
      "Epoch [19][365/594], loss: 4.7428\n",
      "Epoch [19][370/594], loss: 4.7252\n",
      "Epoch [19][375/594], loss: 4.7901\n",
      "Epoch [19][380/594], loss: 4.6917\n",
      "Epoch [19][385/594], loss: 4.7821\n",
      "Epoch [19][390/594], loss: 4.759\n",
      "Epoch [19][395/594], loss: 4.7188\n",
      "Epoch [19][400/594], loss: 4.7189\n",
      "Epoch [19][405/594], loss: 4.5982\n",
      "Epoch [19][410/594], loss: 4.7207\n",
      "Epoch [19][415/594], loss: 4.6925\n",
      "Epoch [19][420/594], loss: 4.7306\n",
      "Epoch [19][425/594], loss: 4.6672\n",
      "Epoch [19][430/594], loss: 4.6818\n",
      "Epoch [19][435/594], loss: 4.7769\n",
      "Epoch [19][440/594], loss: 4.6915\n",
      "Epoch [19][445/594], loss: 4.7524\n",
      "Epoch [19][450/594], loss: 4.6869\n",
      "Epoch [19][455/594], loss: 4.7184\n",
      "Epoch [19][460/594], loss: 4.7856\n",
      "Epoch [19][465/594], loss: 4.7268\n",
      "Epoch [19][470/594], loss: 4.709\n",
      "Epoch [19][475/594], loss: 4.7313\n",
      "Epoch [19][480/594], loss: 4.7335\n",
      "Epoch [19][485/594], loss: 4.7566\n",
      "Epoch [19][490/594], loss: 4.7285\n",
      "Epoch [19][495/594], loss: 4.6582\n",
      "Epoch [19][500/594], loss: 4.7192\n",
      "Epoch [19][505/594], loss: 4.7098\n",
      "Epoch [19][510/594], loss: 4.7201\n",
      "Epoch [19][515/594], loss: 4.7167\n",
      "Epoch [19][520/594], loss: 4.7879\n",
      "Epoch [19][525/594], loss: 4.6821\n",
      "Epoch [19][530/594], loss: 4.7088\n",
      "Epoch [19][535/594], loss: 4.6899\n",
      "Epoch [19][540/594], loss: 4.7569\n",
      "Epoch [19][545/594], loss: 4.755\n",
      "Epoch [19][550/594], loss: 4.7469\n",
      "Epoch [19][555/594], loss: 4.6964\n",
      "Epoch [19][560/594], loss: 4.7082\n",
      "Epoch [19][565/594], loss: 4.6637\n",
      "Epoch [19][570/594], loss: 4.701\n",
      "Epoch [19][575/594], loss: 4.7958\n",
      "Epoch [19][580/594], loss: 4.6875\n",
      "Epoch [19][585/594], loss: 4.7307\n",
      "Epoch [19][590/594], loss: 4.6698\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.6698, val_loss: 4.6957\n",
      "Saving checkpoint at 19 epochs...\n",
      "Epoch [20][5/594], loss: 4.6911\n",
      "Epoch [20][10/594], loss: 4.7405\n",
      "Epoch [20][15/594], loss: 4.6794\n",
      "Epoch [20][20/594], loss: 4.6908\n",
      "Epoch [20][25/594], loss: 4.7539\n",
      "Epoch [20][30/594], loss: 4.6411\n",
      "Epoch [20][35/594], loss: 4.7577\n",
      "Epoch [20][40/594], loss: 4.6768\n",
      "Epoch [20][45/594], loss: 4.7208\n",
      "Epoch [20][50/594], loss: 4.6184\n",
      "Epoch [20][55/594], loss: 4.7293\n",
      "Epoch [20][60/594], loss: 4.6601\n",
      "Epoch [20][65/594], loss: 4.6381\n",
      "Epoch [20][70/594], loss: 4.7001\n",
      "Epoch [20][75/594], loss: 4.6523\n",
      "Epoch [20][80/594], loss: 4.748\n",
      "Epoch [20][85/594], loss: 4.6551\n",
      "Epoch [20][90/594], loss: 4.6733\n",
      "Epoch [20][95/594], loss: 4.6375\n",
      "Epoch [20][100/594], loss: 4.7027\n",
      "Epoch [20][105/594], loss: 4.6856\n",
      "Epoch [20][110/594], loss: 4.7624\n",
      "Epoch [20][115/594], loss: 4.7202\n",
      "Epoch [20][120/594], loss: 4.6073\n",
      "Epoch [20][125/594], loss: 4.6223\n",
      "Epoch [20][130/594], loss: 4.7466\n",
      "Epoch [20][135/594], loss: 4.7463\n",
      "Epoch [20][140/594], loss: 4.7758\n",
      "Epoch [20][145/594], loss: 4.722\n",
      "Epoch [20][150/594], loss: 4.6377\n",
      "Epoch [20][155/594], loss: 4.7102\n",
      "Epoch [20][160/594], loss: 4.614\n",
      "Epoch [20][165/594], loss: 4.7313\n",
      "Epoch [20][170/594], loss: 4.6499\n",
      "Epoch [20][175/594], loss: 4.7202\n",
      "Epoch [20][180/594], loss: 4.6734\n",
      "Epoch [20][185/594], loss: 4.6982\n",
      "Epoch [20][190/594], loss: 4.7169\n",
      "Epoch [20][195/594], loss: 4.7073\n",
      "Epoch [20][200/594], loss: 4.684\n",
      "Epoch [20][205/594], loss: 4.6625\n",
      "Epoch [20][210/594], loss: 4.6893\n",
      "Epoch [20][215/594], loss: 4.7375\n",
      "Epoch [20][220/594], loss: 4.7382\n",
      "Epoch [20][225/594], loss: 4.7171\n",
      "Epoch [20][230/594], loss: 4.6162\n",
      "Epoch [20][235/594], loss: 4.6416\n",
      "Epoch [20][240/594], loss: 4.6821\n",
      "Epoch [20][245/594], loss: 4.6957\n",
      "Epoch [20][250/594], loss: 4.6495\n",
      "Epoch [20][255/594], loss: 4.7172\n",
      "Epoch [20][260/594], loss: 4.7213\n",
      "Epoch [20][265/594], loss: 4.7035\n",
      "Epoch [20][270/594], loss: 4.7251\n",
      "Epoch [20][275/594], loss: 4.6401\n",
      "Epoch [20][280/594], loss: 4.6922\n",
      "Epoch [20][285/594], loss: 4.7024\n",
      "Epoch [20][290/594], loss: 4.7593\n",
      "Epoch [20][295/594], loss: 4.612\n",
      "Epoch [20][300/594], loss: 4.7198\n",
      "Epoch [20][305/594], loss: 4.7637\n",
      "Epoch [20][310/594], loss: 4.7634\n",
      "Epoch [20][315/594], loss: 4.6723\n",
      "Epoch [20][320/594], loss: 4.7361\n",
      "Epoch [20][325/594], loss: 4.7364\n",
      "Epoch [20][330/594], loss: 4.7134\n",
      "Epoch [20][335/594], loss: 4.7276\n",
      "Epoch [20][340/594], loss: 4.6568\n",
      "Epoch [20][345/594], loss: 4.6409\n",
      "Epoch [20][350/594], loss: 4.7719\n",
      "Epoch [20][355/594], loss: 4.7197\n",
      "Epoch [20][360/594], loss: 4.7475\n",
      "Epoch [20][365/594], loss: 4.6573\n",
      "Epoch [20][370/594], loss: 4.7396\n",
      "Epoch [20][375/594], loss: 4.7115\n",
      "Epoch [20][380/594], loss: 4.7482\n",
      "Epoch [20][385/594], loss: 4.7419\n",
      "Epoch [20][390/594], loss: 4.7075\n",
      "Epoch [20][395/594], loss: 4.7071\n",
      "Epoch [20][400/594], loss: 4.7492\n",
      "Epoch [20][405/594], loss: 4.6585\n",
      "Epoch [20][410/594], loss: 4.5885\n",
      "Epoch [20][415/594], loss: 4.7088\n",
      "Epoch [20][420/594], loss: 4.7698\n",
      "Epoch [20][425/594], loss: 4.6662\n",
      "Epoch [20][430/594], loss: 4.7303\n",
      "Epoch [20][435/594], loss: 4.6519\n",
      "Epoch [20][440/594], loss: 4.713\n",
      "Epoch [20][445/594], loss: 4.7113\n",
      "Epoch [20][450/594], loss: 4.7359\n",
      "Epoch [20][455/594], loss: 4.778\n",
      "Epoch [20][460/594], loss: 4.7134\n",
      "Epoch [20][465/594], loss: 4.6072\n",
      "Epoch [20][470/594], loss: 4.742\n",
      "Epoch [20][475/594], loss: 4.7198\n",
      "Epoch [20][480/594], loss: 4.5596\n",
      "Epoch [20][485/594], loss: 4.7536\n",
      "Epoch [20][490/594], loss: 4.7628\n",
      "Epoch [20][495/594], loss: 4.6849\n",
      "Epoch [20][500/594], loss: 4.6728\n",
      "Epoch [20][505/594], loss: 4.7298\n",
      "Epoch [20][510/594], loss: 4.7029\n",
      "Epoch [20][515/594], loss: 4.6605\n",
      "Epoch [20][520/594], loss: 4.6004\n",
      "Epoch [20][525/594], loss: 4.7551\n",
      "Epoch [20][530/594], loss: 4.7711\n",
      "Epoch [20][535/594], loss: 4.7085\n",
      "Epoch [20][540/594], loss: 4.6312\n",
      "Epoch [20][545/594], loss: 4.7527\n",
      "Epoch [20][550/594], loss: 4.7109\n",
      "Epoch [20][555/594], loss: 4.6814\n",
      "Epoch [20][560/594], loss: 4.7809\n",
      "Epoch [20][565/594], loss: 4.7282\n",
      "Epoch [20][570/594], loss: 4.71\n",
      "Epoch [20][575/594], loss: 4.7608\n",
      "Epoch [20][580/594], loss: 4.7125\n",
      "Epoch [20][585/594], loss: 4.6593\n",
      "Epoch [20][590/594], loss: 4.7034\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0155, top5_acc: 0.07364, train_loss: 4.7034, val_loss: 4.6913\n",
      "Saving checkpoint at 20 epochs...\n",
      "Epoch [21][5/594], loss: 4.7839\n",
      "Epoch [21][10/594], loss: 4.7182\n",
      "Epoch [21][15/594], loss: 4.6787\n",
      "Epoch [21][20/594], loss: 4.6536\n",
      "Epoch [21][25/594], loss: 4.7092\n",
      "Epoch [21][30/594], loss: 4.6801\n",
      "Epoch [21][35/594], loss: 4.5437\n",
      "Epoch [21][40/594], loss: 4.7029\n",
      "Epoch [21][45/594], loss: 4.6607\n",
      "Epoch [21][50/594], loss: 4.6951\n",
      "Epoch [21][55/594], loss: 4.6514\n",
      "Epoch [21][60/594], loss: 4.597\n",
      "Epoch [21][65/594], loss: 4.7038\n",
      "Epoch [21][70/594], loss: 4.6972\n",
      "Epoch [21][75/594], loss: 4.6234\n",
      "Epoch [21][80/594], loss: 4.6767\n",
      "Epoch [21][85/594], loss: 4.7116\n",
      "Epoch [21][90/594], loss: 4.7402\n",
      "Epoch [21][95/594], loss: 4.732\n",
      "Epoch [21][100/594], loss: 4.6941\n",
      "Epoch [21][105/594], loss: 4.6173\n",
      "Epoch [21][110/594], loss: 4.6851\n",
      "Epoch [21][115/594], loss: 4.6195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21][120/594], loss: 4.7543\n",
      "Epoch [21][125/594], loss: 4.6715\n",
      "Epoch [21][130/594], loss: 4.6905\n",
      "Epoch [21][135/594], loss: 4.7296\n",
      "Epoch [21][140/594], loss: 4.655\n",
      "Epoch [21][145/594], loss: 4.7259\n",
      "Epoch [21][150/594], loss: 4.7102\n",
      "Epoch [21][155/594], loss: 4.6301\n",
      "Epoch [21][160/594], loss: 4.7479\n",
      "Epoch [21][165/594], loss: 4.7426\n",
      "Epoch [21][170/594], loss: 4.7342\n",
      "Epoch [21][175/594], loss: 4.7517\n",
      "Epoch [21][180/594], loss: 4.7612\n",
      "Epoch [21][185/594], loss: 4.6765\n",
      "Epoch [21][190/594], loss: 4.6507\n",
      "Epoch [21][195/594], loss: 4.6698\n",
      "Epoch [21][200/594], loss: 4.6903\n",
      "Epoch [21][205/594], loss: 4.7466\n",
      "Epoch [21][210/594], loss: 4.6392\n",
      "Epoch [21][215/594], loss: 4.5973\n",
      "Epoch [21][220/594], loss: 4.6981\n",
      "Epoch [21][225/594], loss: 4.6551\n",
      "Epoch [21][230/594], loss: 4.5804\n",
      "Epoch [21][235/594], loss: 4.6355\n",
      "Epoch [21][240/594], loss: 4.6463\n",
      "Epoch [21][245/594], loss: 4.7498\n",
      "Epoch [21][250/594], loss: 4.7595\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Turn on gradient tracking and do a forward pass\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Turn off  gradients for reporting\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, interval)\u001b[0m\n\u001b[1;32m     26\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#         scheduler.step()\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#         lr: {scheduler.get_last_lr()[0]:.5e}\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# Gather data and report\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m         running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m interval \u001b[38;5;241m==\u001b[39m interval\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     34\u001b[0m             last_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m interval \u001b[38;5;66;03m# loss per batch     \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "#21 \n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "# Transfer model to device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Turn on gradient tracking and do a forward pass\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch+1)\n",
    "    \n",
    "    # Turn off  gradients for reporting\n",
    "    model.train(False)\n",
    "    \n",
    "    avg_vloss, top1_acc, top5_acc = validate()\n",
    "    \n",
    "    print(f'top1_acc: {top1_acc:.4}, top5_acc: {top5_acc:.4}, train_loss: {avg_loss:.5}, val_loss: {avg_vloss:.5}')\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = work_dir + f'epoch_{epoch+1}.pth'\n",
    "        print(f'Saving checkpoint at {epoch+1} epochs...')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "    # Track wandb\n",
    "    wandb.log({'loss': avg_loss,\n",
    "             'val/loss': avg_vloss,\n",
    "             'val/top1_accuracy': top1_acc,\n",
    "             'val/top5_accuracy': top5_acc})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataloader",
   "language": "python",
   "name": "dataloader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
