{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff55122",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e609b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sadat/miniconda3/envs/dataloader/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "from mmcv_csn import ResNet3dCSN\n",
    "from csn import csn50\n",
    "from i3d_head import I3DHead\n",
    "from cls_autoencoder import EncoderDecoder\n",
    "from reconstruction_head import RecontructionHead\n",
    "from scheduler import GradualWarmupScheduler\n",
    "from mmaction.datasets import build_dataset\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1f6232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msttaseen\u001b[0m (\u001b[33mcares\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sadat/Desktop/csn-sign-autoencoder/wandb/run-20230204_221437-29oaxpar</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cares/autoencoder/runs/29oaxpar\" target=\"_blank\">reconstruction</a></strong> to <a href=\"https://wandb.ai/cares/autoencoder\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/cares/autoencoder/runs/29oaxpar?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbc5a1da340>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(entity=\"cares\", project=\"autoencoder\",\n",
    "           group=\"wlasl-100\", name=\"reconstruction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da63075",
   "metadata": {},
   "source": [
    "## Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcbb22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device agnostic code\n",
    "try:\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "except:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b938a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8426ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg=dict(\n",
    "    type='RawframeDataset',\n",
    "    ann_file='data/wlasl/train_annotations.txt',\n",
    "    data_prefix='data/wlasl/rawframes',\n",
    "    pipeline=[\n",
    "        dict(\n",
    "            type='SampleFrames',\n",
    "            clip_len=32,\n",
    "            frame_interval=2,\n",
    "            num_clips=1),\n",
    "        dict(type='RawFrameDecode'),\n",
    "        dict(type='Resize', scale=(-1, 256)),\n",
    "        dict(type='RandomResizedCrop'),\n",
    "        dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
    "        dict(type='Flip', flip_ratio=0.5),\n",
    "        dict(\n",
    "            type='Normalize',\n",
    "            mean=[123.675, 116.28, 103.53],\n",
    "            std=[58.395, 57.12, 57.375],\n",
    "            to_bgr=False),\n",
    "        dict(type='FormatShape', input_format='NCTHW'),\n",
    "        dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "        dict(type='ToTensor', keys=['imgs', 'label'])\n",
    "    ])\n",
    "\n",
    "\n",
    "test_cfg=dict(\n",
    "        type='RawframeDataset',\n",
    "        ann_file='data/wlasl/test_annotations.txt',\n",
    "        data_prefix='data/wlasl/rawframes',\n",
    "        pipeline=[\n",
    "            dict(\n",
    "                type='SampleFrames',\n",
    "                clip_len=32,\n",
    "                frame_interval=2,\n",
    "                num_clips=1,\n",
    "                test_mode=True),\n",
    "            dict(type='RawFrameDecode'),\n",
    "            dict(type='Resize', scale=(-1, 256)),\n",
    "            dict(type='CenterCrop', crop_size=224),\n",
    "            dict(\n",
    "                type='Normalize',\n",
    "                mean=[123.675, 116.28, 103.53],\n",
    "                std=[58.395, 57.12, 57.375],\n",
    "                to_bgr=False),\n",
    "            dict(type='FormatShape', input_format='NCTHW'),\n",
    "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
    "            dict(type='ToTensor', keys=['imgs'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "946a5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = 'work_dirs/wlasl-dataset/'\n",
    "\n",
    "os.makedirs(work_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d33be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the datasets\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_dataset = build_dataset(train_cfg)\n",
    "test_dataset = build_dataset(test_cfg)\n",
    "\n",
    "# Setting up dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=4,\n",
    "                                    pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=4,\n",
    "                                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99287401",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82eff841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSN model\n",
    "encoder = ResNet3dCSN(\n",
    "    pretrained2d=False,\n",
    "    # pretrained=None,\n",
    "    pretrained='https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth',\n",
    "    depth=50,\n",
    "    with_pool2=False,\n",
    "    bottleneck_mode='ir',\n",
    "    norm_eval=True,\n",
    "    zero_init_residual=False,\n",
    "    bn_frozen=True\n",
    ")\n",
    "\n",
    "encoder.init_weights()\n",
    "\n",
    "reconstruct_head = RecontructionHead()\n",
    "\n",
    "decoder = I3DHead(num_classes=400,\n",
    "                 in_channels=2048,\n",
    "                 spatial_type='avg',\n",
    "                 dropout_ratio=0.5,\n",
    "                 init_std=0.01)\n",
    "\n",
    "decoder.init_weights()\n",
    "\n",
    "model = EncoderDecoder(encoder, decoder, reconstruct_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7c5e9",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43494dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): ResNet3dCSN(\n",
       "    (conv1): ConvModule(\n",
       "      (conv): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
       "      (bn): BatchNorm3d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
       "    (pool2): MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64, bias=False)\n",
       "            (bn): BatchNorm3d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (downsample): ConvModule(\n",
       "          (conv): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64, bias=False)\n",
       "            (bn): BatchNorm3d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64, bias=False)\n",
       "            (bn): BatchNorm3d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm3d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (downsample): ConvModule(\n",
       "          (conv): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm3d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm3d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm3d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (downsample): ConvModule(\n",
       "          (conv): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (bn): BatchNorm3d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm3d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(1024, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=512, bias=False)\n",
       "            (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (downsample): ConvModule(\n",
       "          (conv): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "          (bn): BatchNorm3d(2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512, bias=False)\n",
       "            (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): CSNBottleneck3d(\n",
       "        (conv1): ConvModule(\n",
       "          (conv): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): ConvModule(\n",
       "            (conv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=512, bias=False)\n",
       "            (bn): BatchNorm3d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activate): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv3): ConvModule(\n",
       "          (conv): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "          (bn): BatchNorm3d(2048, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls_head): I3DHead(\n",
       "    (loss_cls): CrossEntropyLoss()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc_cls): Linear(in_features=2048, out_features=400, bias=True)\n",
       "    (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  )\n",
       "  (reconstruct_head): RecontructionHead(\n",
       "    (decoder1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Conv3d(2048, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (1): ConvTranspose3d(2048, 1024, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (decoder2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Conv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Conv3d(1024, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (2): ConvTranspose3d(1024, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (decoder3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (2): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (decoder4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      )\n",
       "      (1): ConvTranspose3d(256, 64, kernel_size=(3, 2, 2), stride=(1, 2, 2), padding=(1, 0, 0))\n",
       "    )\n",
       "    (outc): ConvTranspose3d(128, 3, kernel_size=(3, 2, 2), stride=(1, 2, 2), padding=(1, 0, 0))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.000125, momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "# Specify Loss\n",
    "loss_cls = nn.CrossEntropyLoss()\n",
    "loss_reconstruct = nn.MSELoss()\n",
    "\n",
    "# Specify total epochs\n",
    "epochs = 100\n",
    "\n",
    "# Specify learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=120, gamma=0.1)\n",
    "\n",
    "scheduler_steplr = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[34, 84], gamma=0.1)\n",
    "scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=16, after_scheduler=scheduler_steplr)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a246d",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "847f4470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1][5/721], loss_cls: 5.9559, reconstruct_loss: 3.8625 lr: 0.00000e+00, loss: 5.411\n",
      "Epoch [1][10/721], loss_cls: 6.077, reconstruct_loss: 2.3185 lr: 0.00000e+00, loss: 5.3872\n",
      "Epoch [1][15/721], loss_cls: 5.8407, reconstruct_loss: 3.0153 lr: 0.00000e+00, loss: 5.1726\n",
      "Epoch [1][20/721], loss_cls: 5.9913, reconstruct_loss: 2.202 lr: 0.00000e+00, loss: 5.3453\n",
      "Epoch [1][25/721], loss_cls: 5.9656, reconstruct_loss: 2.1652 lr: 0.00000e+00, loss: 5.3256\n",
      "Epoch [1][30/721], loss_cls: 5.7886, reconstruct_loss: 2.0581 lr: 0.00000e+00, loss: 5.4699\n",
      "Epoch [1][35/721], loss_cls: 5.9942, reconstruct_loss: 2.1716 lr: 0.00000e+00, loss: 5.3626\n",
      "Epoch [1][40/721], loss_cls: 5.832, reconstruct_loss: 2.1708 lr: 0.00000e+00, loss: 5.2411\n",
      "Epoch [1][45/721], loss_cls: 6.0216, reconstruct_loss: 2.9662 lr: 0.00000e+00, loss: 5.2971\n",
      "Epoch [1][50/721], loss_cls: 6.1063, reconstruct_loss: 2.2705 lr: 0.00000e+00, loss: 5.2872\n",
      "Epoch [1][55/721], loss_cls: 5.9997, reconstruct_loss: 1.8191 lr: 0.00000e+00, loss: 5.3738\n",
      "Epoch [1][60/721], loss_cls: 5.9586, reconstruct_loss: 2.2992 lr: 0.00000e+00, loss: 5.2356\n",
      "Epoch [1][65/721], loss_cls: 5.9936, reconstruct_loss: 1.8277 lr: 0.00000e+00, loss: 5.2682\n",
      "Epoch [1][70/721], loss_cls: 5.9711, reconstruct_loss: 1.6474 lr: 0.00000e+00, loss: 5.3195\n",
      "Epoch [1][75/721], loss_cls: 6.033, reconstruct_loss: 1.8083 lr: 0.00000e+00, loss: 5.2733\n",
      "Epoch [1][80/721], loss_cls: 5.9846, reconstruct_loss: 3.0943 lr: 0.00000e+00, loss: 5.3746\n",
      "Epoch [1][85/721], loss_cls: 6.0198, reconstruct_loss: 2.4316 lr: 0.00000e+00, loss: 5.4693\n",
      "Epoch [1][90/721], loss_cls: 6.1228, reconstruct_loss: 2.5527 lr: 0.00000e+00, loss: 5.3863\n",
      "Epoch [1][95/721], loss_cls: 5.9658, reconstruct_loss: 2.8402 lr: 0.00000e+00, loss: 5.2833\n",
      "Epoch [1][100/721], loss_cls: 5.8272, reconstruct_loss: 3.0783 lr: 0.00000e+00, loss: 5.4377\n",
      "Epoch [1][105/721], loss_cls: 6.1517, reconstruct_loss: 3.1435 lr: 0.00000e+00, loss: 5.2264\n",
      "Epoch [1][110/721], loss_cls: 6.0804, reconstruct_loss: 3.2755 lr: 0.00000e+00, loss: 5.3952\n",
      "Epoch [1][115/721], loss_cls: 5.9229, reconstruct_loss: 2.6838 lr: 0.00000e+00, loss: 5.3683\n",
      "Epoch [1][120/721], loss_cls: 5.9244, reconstruct_loss: 4.9633 lr: 0.00000e+00, loss: 5.2458\n",
      "Epoch [1][125/721], loss_cls: 6.1408, reconstruct_loss: 2.209 lr: 0.00000e+00, loss: 5.3565\n",
      "Epoch [1][130/721], loss_cls: 6.1535, reconstruct_loss: 2.8265 lr: 0.00000e+00, loss: 5.3819\n",
      "Epoch [1][135/721], loss_cls: 6.064, reconstruct_loss: 3.3796 lr: 0.00000e+00, loss: 5.3112\n",
      "Epoch [1][140/721], loss_cls: 5.9173, reconstruct_loss: 3.3087 lr: 0.00000e+00, loss: 5.3017\n",
      "Epoch [1][145/721], loss_cls: 6.2334, reconstruct_loss: 3.1944 lr: 0.00000e+00, loss: 5.3954\n",
      "Epoch [1][150/721], loss_cls: 5.8488, reconstruct_loss: 3.0549 lr: 0.00000e+00, loss: 5.3225\n",
      "Epoch [1][155/721], loss_cls: 5.9373, reconstruct_loss: 1.9709 lr: 0.00000e+00, loss: 5.2872\n",
      "Epoch [1][160/721], loss_cls: 5.86, reconstruct_loss: 1.5283 lr: 0.00000e+00, loss: 5.1025\n",
      "Epoch [1][165/721], loss_cls: 5.8268, reconstruct_loss: 2.7207 lr: 0.00000e+00, loss: 5.4664\n",
      "Epoch [1][170/721], loss_cls: 5.8803, reconstruct_loss: 2.5093 lr: 0.00000e+00, loss: 5.2807\n",
      "Epoch [1][175/721], loss_cls: 6.0165, reconstruct_loss: 3.0408 lr: 0.00000e+00, loss: 5.3926\n",
      "Epoch [1][180/721], loss_cls: 5.9279, reconstruct_loss: 3.23 lr: 0.00000e+00, loss: 5.3566\n",
      "Epoch [1][185/721], loss_cls: 5.8821, reconstruct_loss: 3.8102 lr: 0.00000e+00, loss: 5.3678\n",
      "Epoch [1][190/721], loss_cls: 5.9122, reconstruct_loss: 2.6827 lr: 0.00000e+00, loss: 5.2769\n",
      "Epoch [1][195/721], loss_cls: 6.093, reconstruct_loss: 3.0192 lr: 0.00000e+00, loss: 5.3772\n",
      "Epoch [1][200/721], loss_cls: 6.4106, reconstruct_loss: 3.6369 lr: 0.00000e+00, loss: 5.418\n",
      "Epoch [1][205/721], loss_cls: 6.1116, reconstruct_loss: 2.1936 lr: 0.00000e+00, loss: 5.2439\n",
      "Epoch [1][210/721], loss_cls: 5.9444, reconstruct_loss: 3.0125 lr: 0.00000e+00, loss: 5.3667\n",
      "Epoch [1][215/721], loss_cls: 5.9651, reconstruct_loss: 2.9782 lr: 0.00000e+00, loss: 5.3659\n",
      "Epoch [1][220/721], loss_cls: 6.085, reconstruct_loss: 4.4369 lr: 0.00000e+00, loss: 5.4267\n",
      "Epoch [1][225/721], loss_cls: 6.1851, reconstruct_loss: 1.9326 lr: 0.00000e+00, loss: 5.2904\n",
      "Epoch [1][230/721], loss_cls: 6.0999, reconstruct_loss: 2.2323 lr: 0.00000e+00, loss: 5.2496\n",
      "Epoch [1][235/721], loss_cls: 5.9197, reconstruct_loss: 2.1453 lr: 0.00000e+00, loss: 5.2813\n",
      "Epoch [1][240/721], loss_cls: 5.8965, reconstruct_loss: 2.4889 lr: 0.00000e+00, loss: 5.3691\n",
      "Epoch [1][245/721], loss_cls: 5.9259, reconstruct_loss: 2.2454 lr: 0.00000e+00, loss: 5.302\n",
      "Epoch [1][250/721], loss_cls: 6.0193, reconstruct_loss: 2.6237 lr: 0.00000e+00, loss: 5.4128\n",
      "Epoch [1][255/721], loss_cls: 5.9779, reconstruct_loss: 3.8773 lr: 0.00000e+00, loss: 5.5605\n",
      "Epoch [1][260/721], loss_cls: 6.0386, reconstruct_loss: 3.1878 lr: 0.00000e+00, loss: 5.4353\n",
      "Epoch [1][265/721], loss_cls: 5.9497, reconstruct_loss: 2.6599 lr: 0.00000e+00, loss: 5.1987\n",
      "Epoch [1][270/721], loss_cls: 6.1167, reconstruct_loss: 1.5726 lr: 0.00000e+00, loss: 5.4496\n",
      "Epoch [1][275/721], loss_cls: 6.1464, reconstruct_loss: 2.0959 lr: 0.00000e+00, loss: 5.3623\n",
      "Epoch [1][280/721], loss_cls: 6.1281, reconstruct_loss: 2.6405 lr: 0.00000e+00, loss: 5.3644\n",
      "Epoch [1][285/721], loss_cls: 6.1689, reconstruct_loss: 1.7478 lr: 0.00000e+00, loss: 5.2513\n",
      "Epoch [1][290/721], loss_cls: 5.9466, reconstruct_loss: 3.2925 lr: 0.00000e+00, loss: 5.4073\n",
      "Epoch [1][295/721], loss_cls: 5.9792, reconstruct_loss: 4.5288 lr: 0.00000e+00, loss: 5.3302\n",
      "Epoch [1][300/721], loss_cls: 6.1155, reconstruct_loss: 2.3441 lr: 0.00000e+00, loss: 5.2543\n",
      "Epoch [1][305/721], loss_cls: 5.8645, reconstruct_loss: 2.8434 lr: 0.00000e+00, loss: 5.3314\n",
      "Epoch [1][310/721], loss_cls: 5.9586, reconstruct_loss: 3.1871 lr: 0.00000e+00, loss: 5.324\n",
      "Epoch [1][315/721], loss_cls: 5.804, reconstruct_loss: 3.0441 lr: 0.00000e+00, loss: 5.3057\n",
      "Epoch [1][320/721], loss_cls: 6.009, reconstruct_loss: 4.0841 lr: 0.00000e+00, loss: 5.5415\n",
      "Epoch [1][325/721], loss_cls: 6.1168, reconstruct_loss: 1.9233 lr: 0.00000e+00, loss: 5.3904\n",
      "Epoch [1][330/721], loss_cls: 6.0416, reconstruct_loss: 1.7626 lr: 0.00000e+00, loss: 5.2613\n",
      "Epoch [1][335/721], loss_cls: 6.0169, reconstruct_loss: 2.3378 lr: 0.00000e+00, loss: 5.2747\n",
      "Epoch [1][340/721], loss_cls: 5.8091, reconstruct_loss: 2.7803 lr: 0.00000e+00, loss: 5.2191\n",
      "Epoch [1][345/721], loss_cls: 5.7882, reconstruct_loss: 2.113 lr: 0.00000e+00, loss: 5.2734\n",
      "Epoch [1][350/721], loss_cls: 6.2124, reconstruct_loss: 2.959 lr: 0.00000e+00, loss: 5.4296\n",
      "Epoch [1][355/721], loss_cls: 6.176, reconstruct_loss: 3.0811 lr: 0.00000e+00, loss: 5.3777\n",
      "Epoch [1][360/721], loss_cls: 6.0677, reconstruct_loss: 2.6073 lr: 0.00000e+00, loss: 5.2865\n",
      "Epoch [1][365/721], loss_cls: 6.139, reconstruct_loss: 1.7855 lr: 0.00000e+00, loss: 5.289\n",
      "Epoch [1][370/721], loss_cls: 5.9732, reconstruct_loss: 2.4533 lr: 0.00000e+00, loss: 5.2777\n",
      "Epoch [1][375/721], loss_cls: 6.0213, reconstruct_loss: 1.5463 lr: 0.00000e+00, loss: 5.1856\n",
      "Epoch [1][380/721], loss_cls: 6.0417, reconstruct_loss: 3.0683 lr: 0.00000e+00, loss: 5.3473\n",
      "Epoch [1][385/721], loss_cls: 6.2261, reconstruct_loss: 2.5004 lr: 0.00000e+00, loss: 5.3868\n",
      "Epoch [1][390/721], loss_cls: 5.9846, reconstruct_loss: 2.5455 lr: 0.00000e+00, loss: 5.3249\n",
      "Epoch [1][395/721], loss_cls: 6.0264, reconstruct_loss: 3.4627 lr: 0.00000e+00, loss: 5.4634\n",
      "Epoch [1][400/721], loss_cls: 6.0153, reconstruct_loss: 3.1553 lr: 0.00000e+00, loss: 5.4284\n",
      "Epoch [1][405/721], loss_cls: 6.2211, reconstruct_loss: 2.5777 lr: 0.00000e+00, loss: 5.3177\n",
      "Epoch [1][410/721], loss_cls: 6.0411, reconstruct_loss: 2.7434 lr: 0.00000e+00, loss: 5.3934\n",
      "Epoch [1][415/721], loss_cls: 5.8278, reconstruct_loss: 1.5662 lr: 0.00000e+00, loss: 5.2178\n",
      "Epoch [1][420/721], loss_cls: 6.0607, reconstruct_loss: 2.3958 lr: 0.00000e+00, loss: 5.283\n",
      "Epoch [1][425/721], loss_cls: 6.009, reconstruct_loss: 1.7287 lr: 0.00000e+00, loss: 5.4476\n",
      "Epoch [1][430/721], loss_cls: 5.972, reconstruct_loss: 1.7668 lr: 0.00000e+00, loss: 5.2504\n",
      "Epoch [1][435/721], loss_cls: 6.123, reconstruct_loss: 1.7819 lr: 0.00000e+00, loss: 5.2835\n",
      "Epoch [1][440/721], loss_cls: 6.0327, reconstruct_loss: 2.112 lr: 0.00000e+00, loss: 5.3908\n",
      "Epoch [1][445/721], loss_cls: 6.1805, reconstruct_loss: 3.0469 lr: 0.00000e+00, loss: 5.5934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1][450/721], loss_cls: 5.9606, reconstruct_loss: 3.3753 lr: 0.00000e+00, loss: 5.3101\n",
      "Epoch [1][455/721], loss_cls: 6.1868, reconstruct_loss: 2.3195 lr: 0.00000e+00, loss: 5.2679\n",
      "Epoch [1][460/721], loss_cls: 6.0628, reconstruct_loss: 1.9187 lr: 0.00000e+00, loss: 5.3227\n",
      "Epoch [1][465/721], loss_cls: 6.1129, reconstruct_loss: 2.0251 lr: 0.00000e+00, loss: 5.3119\n",
      "Epoch [1][470/721], loss_cls: 6.0044, reconstruct_loss: 1.2593 lr: 0.00000e+00, loss: 5.308\n",
      "Epoch [1][475/721], loss_cls: 5.9143, reconstruct_loss: 1.8403 lr: 0.00000e+00, loss: 5.3218\n",
      "Epoch [1][480/721], loss_cls: 6.0154, reconstruct_loss: 2.7038 lr: 0.00000e+00, loss: 5.2919\n",
      "Epoch [1][485/721], loss_cls: 6.0508, reconstruct_loss: 1.6417 lr: 0.00000e+00, loss: 5.3086\n",
      "Epoch [1][490/721], loss_cls: 6.2807, reconstruct_loss: 3.1535 lr: 0.00000e+00, loss: 5.3136\n",
      "Epoch [1][495/721], loss_cls: 5.9491, reconstruct_loss: 2.9956 lr: 0.00000e+00, loss: 5.2549\n",
      "Epoch [1][500/721], loss_cls: 5.9422, reconstruct_loss: 2.0317 lr: 0.00000e+00, loss: 5.3064\n",
      "Epoch [1][505/721], loss_cls: 5.8999, reconstruct_loss: 2.53 lr: 0.00000e+00, loss: 5.2721\n",
      "Epoch [1][510/721], loss_cls: 5.8013, reconstruct_loss: 3.553 lr: 0.00000e+00, loss: 5.3505\n",
      "Epoch [1][515/721], loss_cls: 6.1633, reconstruct_loss: 4.5053 lr: 0.00000e+00, loss: 5.3989\n",
      "Epoch [1][520/721], loss_cls: 6.1126, reconstruct_loss: 3.5996 lr: 0.00000e+00, loss: 5.3109\n",
      "Epoch [1][525/721], loss_cls: 5.8292, reconstruct_loss: 3.7235 lr: 0.00000e+00, loss: 5.3463\n",
      "Epoch [1][530/721], loss_cls: 6.0523, reconstruct_loss: 1.6959 lr: 0.00000e+00, loss: 5.4647\n",
      "Epoch [1][535/721], loss_cls: 5.9789, reconstruct_loss: 5.3134 lr: 0.00000e+00, loss: 5.3572\n",
      "Epoch [1][540/721], loss_cls: 6.3582, reconstruct_loss: 2.2876 lr: 0.00000e+00, loss: 5.3418\n",
      "Epoch [1][545/721], loss_cls: 6.1798, reconstruct_loss: 2.1112 lr: 0.00000e+00, loss: 5.4269\n",
      "Epoch [1][550/721], loss_cls: 6.0064, reconstruct_loss: 3.424 lr: 0.00000e+00, loss: 5.4855\n",
      "Epoch [1][555/721], loss_cls: 6.0499, reconstruct_loss: 2.2776 lr: 0.00000e+00, loss: 5.3466\n",
      "Epoch [1][560/721], loss_cls: 6.0967, reconstruct_loss: 2.6241 lr: 0.00000e+00, loss: 5.3496\n",
      "Epoch [1][565/721], loss_cls: 6.0883, reconstruct_loss: 2.1065 lr: 0.00000e+00, loss: 5.262\n",
      "Epoch [1][570/721], loss_cls: 6.0731, reconstruct_loss: 3.3105 lr: 0.00000e+00, loss: 5.3718\n",
      "Epoch [1][575/721], loss_cls: 6.2946, reconstruct_loss: 2.9157 lr: 0.00000e+00, loss: 5.4865\n",
      "Epoch [1][580/721], loss_cls: 6.0257, reconstruct_loss: 2.6733 lr: 0.00000e+00, loss: 5.3764\n",
      "Epoch [1][585/721], loss_cls: 6.0621, reconstruct_loss: 2.7136 lr: 0.00000e+00, loss: 5.3976\n",
      "Epoch [1][590/721], loss_cls: 5.8758, reconstruct_loss: 2.8961 lr: 0.00000e+00, loss: 5.3062\n",
      "Epoch [1][595/721], loss_cls: 5.8548, reconstruct_loss: 1.7322 lr: 0.00000e+00, loss: 5.2875\n",
      "Epoch [1][600/721], loss_cls: 6.0017, reconstruct_loss: 2.1475 lr: 0.00000e+00, loss: 5.4132\n",
      "Epoch [1][605/721], loss_cls: 5.9121, reconstruct_loss: 2.6998 lr: 0.00000e+00, loss: 5.3672\n",
      "Epoch [1][610/721], loss_cls: 6.3216, reconstruct_loss: 2.2894 lr: 0.00000e+00, loss: 5.4906\n",
      "Epoch [1][615/721], loss_cls: 6.3455, reconstruct_loss: 4.8878 lr: 0.00000e+00, loss: 5.4023\n",
      "Epoch [1][620/721], loss_cls: 6.0662, reconstruct_loss: 3.251 lr: 0.00000e+00, loss: 5.3886\n",
      "Epoch [1][625/721], loss_cls: 6.0204, reconstruct_loss: 2.0639 lr: 0.00000e+00, loss: 5.2935\n",
      "Epoch [1][630/721], loss_cls: 5.914, reconstruct_loss: 1.669 lr: 0.00000e+00, loss: 5.2431\n",
      "Epoch [1][635/721], loss_cls: 6.1856, reconstruct_loss: 4.3808 lr: 0.00000e+00, loss: 5.4749\n",
      "Epoch [1][640/721], loss_cls: 5.7934, reconstruct_loss: 2.5382 lr: 0.00000e+00, loss: 5.331\n",
      "Epoch [1][645/721], loss_cls: 6.1463, reconstruct_loss: 3.2474 lr: 0.00000e+00, loss: 5.3898\n",
      "Epoch [1][650/721], loss_cls: 6.0228, reconstruct_loss: 2.0438 lr: 0.00000e+00, loss: 5.4002\n",
      "Epoch [1][655/721], loss_cls: 5.9519, reconstruct_loss: 1.5599 lr: 0.00000e+00, loss: 5.2511\n",
      "Epoch [1][660/721], loss_cls: 5.914, reconstruct_loss: 3.2371 lr: 0.00000e+00, loss: 5.4528\n",
      "Epoch [1][665/721], loss_cls: 6.1841, reconstruct_loss: 2.7803 lr: 0.00000e+00, loss: 5.3916\n",
      "Epoch [1][670/721], loss_cls: 6.3146, reconstruct_loss: 2.6501 lr: 0.00000e+00, loss: 5.4754\n",
      "Epoch [1][675/721], loss_cls: 6.1431, reconstruct_loss: 2.6839 lr: 0.00000e+00, loss: 5.4698\n",
      "Epoch [1][680/721], loss_cls: 6.0919, reconstruct_loss: 1.7878 lr: 0.00000e+00, loss: 5.2049\n",
      "Epoch [1][685/721], loss_cls: 6.1368, reconstruct_loss: 1.5198 lr: 0.00000e+00, loss: 5.2733\n",
      "Epoch [1][690/721], loss_cls: 6.001, reconstruct_loss: 2.0415 lr: 0.00000e+00, loss: 5.3306\n",
      "Epoch [1][695/721], loss_cls: 6.0102, reconstruct_loss: 1.5544 lr: 0.00000e+00, loss: 5.4013\n",
      "Epoch [1][700/721], loss_cls: 6.0435, reconstruct_loss: 3.5062 lr: 0.00000e+00, loss: 5.4319\n",
      "Epoch [1][705/721], loss_cls: 5.7311, reconstruct_loss: 1.9545 lr: 0.00000e+00, loss: 5.2852\n",
      "Epoch [1][710/721], loss_cls: 6.031, reconstruct_loss: 4.2942 lr: 0.00000e+00, loss: 5.3442\n",
      "Epoch [1][715/721], loss_cls: 6.0624, reconstruct_loss: 3.1855 lr: 0.00000e+00, loss: 5.4016\n",
      "Epoch [1][720/721], loss_cls: 5.9635, reconstruct_loss: 3.8205 lr: 0.00000e+00, loss: 5.4765\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.0, top5_acc: 0.01938, train_loss: 5.4765, val_loss: 5.4188\n",
      "Saving checkpoint at 1 epochs...\n",
      "Epoch [2][5/721], loss_cls: 6.0551, reconstruct_loss: 2.2488 lr: 7.81250e-06, loss: 5.2732\n",
      "Epoch [2][10/721], loss_cls: 5.8185, reconstruct_loss: 1.7016 lr: 7.81250e-06, loss: 5.3346\n",
      "Epoch [2][15/721], loss_cls: 6.1317, reconstruct_loss: 2.4003 lr: 7.81250e-06, loss: 5.3369\n",
      "Epoch [2][20/721], loss_cls: 6.1605, reconstruct_loss: 3.4658 lr: 7.81250e-06, loss: 5.1411\n",
      "Epoch [2][25/721], loss_cls: 6.1847, reconstruct_loss: 1.8483 lr: 7.81250e-06, loss: 5.4299\n",
      "Epoch [2][30/721], loss_cls: 6.3727, reconstruct_loss: 4.0123 lr: 7.81250e-06, loss: 5.6043\n",
      "Epoch [2][35/721], loss_cls: 5.9506, reconstruct_loss: 2.539 lr: 7.81250e-06, loss: 5.3154\n",
      "Epoch [2][40/721], loss_cls: 6.0153, reconstruct_loss: 3.6141 lr: 7.81250e-06, loss: 5.3256\n",
      "Epoch [2][45/721], loss_cls: 6.1434, reconstruct_loss: 1.9123 lr: 7.81250e-06, loss: 5.3207\n",
      "Epoch [2][50/721], loss_cls: 5.9148, reconstruct_loss: 2.966 lr: 7.81250e-06, loss: 5.3287\n",
      "Epoch [2][55/721], loss_cls: 5.9415, reconstruct_loss: 2.8143 lr: 7.81250e-06, loss: 5.3595\n",
      "Epoch [2][60/721], loss_cls: 5.6148, reconstruct_loss: 1.897 lr: 7.81250e-06, loss: 5.1474\n",
      "Epoch [2][65/721], loss_cls: 5.9835, reconstruct_loss: 2.9469 lr: 7.81250e-06, loss: 5.3598\n",
      "Epoch [2][70/721], loss_cls: 5.7374, reconstruct_loss: 1.8504 lr: 7.81250e-06, loss: 5.2012\n",
      "Epoch [2][75/721], loss_cls: 5.9052, reconstruct_loss: 3.0384 lr: 7.81250e-06, loss: 5.3617\n",
      "Epoch [2][80/721], loss_cls: 6.0382, reconstruct_loss: 2.194 lr: 7.81250e-06, loss: 5.3298\n",
      "Epoch [2][85/721], loss_cls: 6.0132, reconstruct_loss: 2.2552 lr: 7.81250e-06, loss: 5.3094\n",
      "Epoch [2][90/721], loss_cls: 6.0477, reconstruct_loss: 2.7991 lr: 7.81250e-06, loss: 5.3327\n",
      "Epoch [2][95/721], loss_cls: 6.038, reconstruct_loss: 3.4791 lr: 7.81250e-06, loss: 5.2433\n",
      "Epoch [2][100/721], loss_cls: 5.9572, reconstruct_loss: 2.7668 lr: 7.81250e-06, loss: 5.4066\n",
      "Epoch [2][105/721], loss_cls: 5.9039, reconstruct_loss: 2.0884 lr: 7.81250e-06, loss: 5.3082\n",
      "Epoch [2][110/721], loss_cls: 6.0693, reconstruct_loss: 2.8094 lr: 7.81250e-06, loss: 5.3363\n",
      "Epoch [2][115/721], loss_cls: 6.0924, reconstruct_loss: 3.069 lr: 7.81250e-06, loss: 5.4164\n",
      "Epoch [2][120/721], loss_cls: 5.8939, reconstruct_loss: 2.2529 lr: 7.81250e-06, loss: 5.1576\n",
      "Epoch [2][125/721], loss_cls: 6.0607, reconstruct_loss: 2.7258 lr: 7.81250e-06, loss: 5.3612\n",
      "Epoch [2][130/721], loss_cls: 5.9958, reconstruct_loss: 1.744 lr: 7.81250e-06, loss: 5.3564\n",
      "Epoch [2][135/721], loss_cls: 5.8857, reconstruct_loss: 2.0751 lr: 7.81250e-06, loss: 5.2928\n",
      "Epoch [2][140/721], loss_cls: 6.0682, reconstruct_loss: 2.4711 lr: 7.81250e-06, loss: 5.2832\n",
      "Epoch [2][145/721], loss_cls: 5.6825, reconstruct_loss: 3.5286 lr: 7.81250e-06, loss: 5.4406\n",
      "Epoch [2][150/721], loss_cls: 5.9851, reconstruct_loss: 1.5096 lr: 7.81250e-06, loss: 5.2602\n",
      "Epoch [2][155/721], loss_cls: 6.0544, reconstruct_loss: 2.9936 lr: 7.81250e-06, loss: 5.5823\n",
      "Epoch [2][160/721], loss_cls: 6.0518, reconstruct_loss: 3.7372 lr: 7.81250e-06, loss: 5.3438\n",
      "Epoch [2][165/721], loss_cls: 6.0227, reconstruct_loss: 3.0092 lr: 7.81250e-06, loss: 5.2672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2][170/721], loss_cls: 5.8941, reconstruct_loss: 3.057 lr: 7.81250e-06, loss: 5.2558\n",
      "Epoch [2][175/721], loss_cls: 5.8287, reconstruct_loss: 2.8421 lr: 7.81250e-06, loss: 5.2006\n",
      "Epoch [2][180/721], loss_cls: 6.1002, reconstruct_loss: 2.7902 lr: 7.81250e-06, loss: 5.5164\n",
      "Epoch [2][185/721], loss_cls: 5.859, reconstruct_loss: 1.5538 lr: 7.81250e-06, loss: 5.2662\n",
      "Epoch [2][190/721], loss_cls: 6.0371, reconstruct_loss: 1.6094 lr: 7.81250e-06, loss: 5.2619\n",
      "Epoch [2][195/721], loss_cls: 6.0879, reconstruct_loss: 2.9528 lr: 7.81250e-06, loss: 5.3787\n",
      "Epoch [2][200/721], loss_cls: 5.9397, reconstruct_loss: 2.0017 lr: 7.81250e-06, loss: 5.2498\n",
      "Epoch [2][205/721], loss_cls: 6.1234, reconstruct_loss: 1.9043 lr: 7.81250e-06, loss: 5.2813\n",
      "Epoch [2][210/721], loss_cls: 5.8246, reconstruct_loss: 2.6347 lr: 7.81250e-06, loss: 5.2983\n",
      "Epoch [2][215/721], loss_cls: 5.8106, reconstruct_loss: 1.4554 lr: 7.81250e-06, loss: 5.2101\n",
      "Epoch [2][220/721], loss_cls: 6.3837, reconstruct_loss: 2.0751 lr: 7.81250e-06, loss: 5.2942\n",
      "Epoch [2][225/721], loss_cls: 5.9697, reconstruct_loss: 3.3163 lr: 7.81250e-06, loss: 5.3171\n",
      "Epoch [2][230/721], loss_cls: 5.6818, reconstruct_loss: 2.1317 lr: 7.81250e-06, loss: 5.2067\n",
      "Epoch [2][235/721], loss_cls: 5.8729, reconstruct_loss: 3.3242 lr: 7.81250e-06, loss: 5.3354\n",
      "Epoch [2][240/721], loss_cls: 5.8867, reconstruct_loss: 2.2317 lr: 7.81250e-06, loss: 5.2076\n",
      "Epoch [2][245/721], loss_cls: 6.1248, reconstruct_loss: 1.4092 lr: 7.81250e-06, loss: 5.3767\n",
      "Epoch [2][250/721], loss_cls: 5.8111, reconstruct_loss: 3.6367 lr: 7.81250e-06, loss: 5.2536\n",
      "Epoch [2][255/721], loss_cls: 5.765, reconstruct_loss: 2.6894 lr: 7.81250e-06, loss: 5.2662\n",
      "Epoch [2][260/721], loss_cls: 5.8666, reconstruct_loss: 1.5257 lr: 7.81250e-06, loss: 5.2699\n",
      "Epoch [2][265/721], loss_cls: 6.0916, reconstruct_loss: 2.5926 lr: 7.81250e-06, loss: 5.3088\n",
      "Epoch [2][270/721], loss_cls: 6.0239, reconstruct_loss: 1.9398 lr: 7.81250e-06, loss: 5.3174\n",
      "Epoch [2][275/721], loss_cls: 6.0318, reconstruct_loss: 3.1185 lr: 7.81250e-06, loss: 5.3294\n",
      "Epoch [2][280/721], loss_cls: 5.9519, reconstruct_loss: 1.4637 lr: 7.81250e-06, loss: 5.27\n",
      "Epoch [2][285/721], loss_cls: 6.0944, reconstruct_loss: 2.0442 lr: 7.81250e-06, loss: 5.2753\n",
      "Epoch [2][290/721], loss_cls: 6.0709, reconstruct_loss: 1.8406 lr: 7.81250e-06, loss: 5.2548\n",
      "Epoch [2][295/721], loss_cls: 5.9738, reconstruct_loss: 2.5557 lr: 7.81250e-06, loss: 5.3627\n",
      "Epoch [2][300/721], loss_cls: 5.7986, reconstruct_loss: 2.7574 lr: 7.81250e-06, loss: 5.2302\n",
      "Epoch [2][305/721], loss_cls: 6.111, reconstruct_loss: 2.9334 lr: 7.81250e-06, loss: 5.2696\n",
      "Epoch [2][310/721], loss_cls: 5.8929, reconstruct_loss: 2.5578 lr: 7.81250e-06, loss: 5.2491\n",
      "Epoch [2][315/721], loss_cls: 5.9755, reconstruct_loss: 2.656 lr: 7.81250e-06, loss: 5.2868\n",
      "Epoch [2][320/721], loss_cls: 6.0387, reconstruct_loss: 2.8384 lr: 7.81250e-06, loss: 5.3285\n",
      "Epoch [2][325/721], loss_cls: 5.7677, reconstruct_loss: 2.93 lr: 7.81250e-06, loss: 5.3745\n",
      "Epoch [2][330/721], loss_cls: 5.9414, reconstruct_loss: 3.2638 lr: 7.81250e-06, loss: 5.2828\n",
      "Epoch [2][335/721], loss_cls: 5.9603, reconstruct_loss: 2.0426 lr: 7.81250e-06, loss: 5.257\n",
      "Epoch [2][340/721], loss_cls: 6.043, reconstruct_loss: 2.1378 lr: 7.81250e-06, loss: 5.3889\n",
      "Epoch [2][345/721], loss_cls: 5.9234, reconstruct_loss: 1.8465 lr: 7.81250e-06, loss: 5.1702\n",
      "Epoch [2][350/721], loss_cls: 6.1367, reconstruct_loss: 2.945 lr: 7.81250e-06, loss: 5.2675\n",
      "Epoch [2][355/721], loss_cls: 5.9343, reconstruct_loss: 2.3438 lr: 7.81250e-06, loss: 5.2745\n",
      "Epoch [2][360/721], loss_cls: 6.0375, reconstruct_loss: 1.6828 lr: 7.81250e-06, loss: 5.231\n",
      "Epoch [2][365/721], loss_cls: 6.0032, reconstruct_loss: 3.172 lr: 7.81250e-06, loss: 5.3123\n",
      "Epoch [2][370/721], loss_cls: 5.8518, reconstruct_loss: 3.4021 lr: 7.81250e-06, loss: 5.1992\n",
      "Epoch [2][375/721], loss_cls: 5.9169, reconstruct_loss: 2.6996 lr: 7.81250e-06, loss: 5.2623\n",
      "Epoch [2][380/721], loss_cls: 5.6789, reconstruct_loss: 1.631 lr: 7.81250e-06, loss: 5.1414\n",
      "Epoch [2][385/721], loss_cls: 5.9602, reconstruct_loss: 3.5545 lr: 7.81250e-06, loss: 5.2423\n",
      "Epoch [2][390/721], loss_cls: 5.8223, reconstruct_loss: 2.0529 lr: 7.81250e-06, loss: 5.2581\n",
      "Epoch [2][395/721], loss_cls: 5.9512, reconstruct_loss: 2.5113 lr: 7.81250e-06, loss: 5.3707\n",
      "Epoch [2][400/721], loss_cls: 6.0642, reconstruct_loss: 2.2351 lr: 7.81250e-06, loss: 5.2645\n",
      "Epoch [2][405/721], loss_cls: 5.9085, reconstruct_loss: 2.394 lr: 7.81250e-06, loss: 5.1361\n",
      "Epoch [2][410/721], loss_cls: 5.8988, reconstruct_loss: 3.095 lr: 7.81250e-06, loss: 5.2981\n",
      "Epoch [2][415/721], loss_cls: 5.9395, reconstruct_loss: 1.7918 lr: 7.81250e-06, loss: 5.2379\n",
      "Epoch [2][420/721], loss_cls: 6.1816, reconstruct_loss: 4.0526 lr: 7.81250e-06, loss: 5.3485\n",
      "Epoch [2][425/721], loss_cls: 6.0565, reconstruct_loss: 2.812 lr: 7.81250e-06, loss: 5.1587\n",
      "Epoch [2][430/721], loss_cls: 5.9352, reconstruct_loss: 3.4373 lr: 7.81250e-06, loss: 5.1777\n",
      "Epoch [2][435/721], loss_cls: 5.8553, reconstruct_loss: 2.8784 lr: 7.81250e-06, loss: 5.238\n",
      "Epoch [2][440/721], loss_cls: 6.2274, reconstruct_loss: 2.0205 lr: 7.81250e-06, loss: 5.3122\n",
      "Epoch [2][445/721], loss_cls: 5.9343, reconstruct_loss: 3.9414 lr: 7.81250e-06, loss: 5.2893\n",
      "Epoch [2][450/721], loss_cls: 5.9244, reconstruct_loss: 2.6351 lr: 7.81250e-06, loss: 5.1656\n",
      "Epoch [2][455/721], loss_cls: 5.7109, reconstruct_loss: 1.9802 lr: 7.81250e-06, loss: 5.1248\n",
      "Epoch [2][460/721], loss_cls: 5.8627, reconstruct_loss: 1.7726 lr: 7.81250e-06, loss: 5.241\n",
      "Epoch [2][465/721], loss_cls: 6.0085, reconstruct_loss: 2.4077 lr: 7.81250e-06, loss: 5.2504\n",
      "Epoch [2][470/721], loss_cls: 6.4746, reconstruct_loss: 3.1464 lr: 7.81250e-06, loss: 5.4154\n",
      "Epoch [2][475/721], loss_cls: 5.864, reconstruct_loss: 3.0835 lr: 7.81250e-06, loss: 5.2604\n",
      "Epoch [2][480/721], loss_cls: 5.922, reconstruct_loss: 1.4755 lr: 7.81250e-06, loss: 5.1441\n",
      "Epoch [2][485/721], loss_cls: 6.0832, reconstruct_loss: 2.085 lr: 7.81250e-06, loss: 5.1964\n",
      "Epoch [2][490/721], loss_cls: 6.1492, reconstruct_loss: 2.8296 lr: 7.81250e-06, loss: 5.1444\n",
      "Epoch [2][495/721], loss_cls: 6.0551, reconstruct_loss: 2.4588 lr: 7.81250e-06, loss: 5.3336\n",
      "Epoch [2][500/721], loss_cls: 5.9668, reconstruct_loss: 2.0386 lr: 7.81250e-06, loss: 5.2093\n",
      "Epoch [2][505/721], loss_cls: 5.9732, reconstruct_loss: 2.0617 lr: 7.81250e-06, loss: 5.2332\n",
      "Epoch [2][510/721], loss_cls: 5.4924, reconstruct_loss: 2.7118 lr: 7.81250e-06, loss: 5.1814\n",
      "Epoch [2][515/721], loss_cls: 6.0607, reconstruct_loss: 1.9102 lr: 7.81250e-06, loss: 5.3087\n",
      "Epoch [2][520/721], loss_cls: 5.9619, reconstruct_loss: 2.1715 lr: 7.81250e-06, loss: 5.1839\n",
      "Epoch [2][525/721], loss_cls: 5.9645, reconstruct_loss: 2.1913 lr: 7.81250e-06, loss: 5.1854\n",
      "Epoch [2][530/721], loss_cls: 5.9676, reconstruct_loss: 3.3788 lr: 7.81250e-06, loss: 5.2842\n",
      "Epoch [2][535/721], loss_cls: 5.4814, reconstruct_loss: 1.905 lr: 7.81250e-06, loss: 5.1067\n",
      "Epoch [2][540/721], loss_cls: 6.3198, reconstruct_loss: 1.8587 lr: 7.81250e-06, loss: 5.3598\n",
      "Epoch [2][545/721], loss_cls: 5.9728, reconstruct_loss: 1.292 lr: 7.81250e-06, loss: 5.1386\n",
      "Epoch [2][550/721], loss_cls: 6.1438, reconstruct_loss: 3.0845 lr: 7.81250e-06, loss: 5.2023\n",
      "Epoch [2][555/721], loss_cls: 6.1318, reconstruct_loss: 3.0206 lr: 7.81250e-06, loss: 5.3159\n",
      "Epoch [2][560/721], loss_cls: 6.3571, reconstruct_loss: 3.1633 lr: 7.81250e-06, loss: 5.28\n",
      "Epoch [2][565/721], loss_cls: 5.8819, reconstruct_loss: 2.4933 lr: 7.81250e-06, loss: 5.3221\n",
      "Epoch [2][570/721], loss_cls: 5.7977, reconstruct_loss: 2.5366 lr: 7.81250e-06, loss: 5.1365\n",
      "Epoch [2][575/721], loss_cls: 5.8975, reconstruct_loss: 3.3463 lr: 7.81250e-06, loss: 5.1123\n",
      "Epoch [2][580/721], loss_cls: 6.0918, reconstruct_loss: 2.2638 lr: 7.81250e-06, loss: 5.1867\n",
      "Epoch [2][585/721], loss_cls: 5.7447, reconstruct_loss: 1.829 lr: 7.81250e-06, loss: 5.1897\n",
      "Epoch [2][590/721], loss_cls: 5.8478, reconstruct_loss: 3.6743 lr: 7.81250e-06, loss: 5.172\n",
      "Epoch [2][595/721], loss_cls: 6.1023, reconstruct_loss: 2.896 lr: 7.81250e-06, loss: 5.3485\n",
      "Epoch [2][600/721], loss_cls: 5.8185, reconstruct_loss: 2.68 lr: 7.81250e-06, loss: 5.3473\n",
      "Epoch [2][605/721], loss_cls: 5.8526, reconstruct_loss: 1.9236 lr: 7.81250e-06, loss: 5.1936\n",
      "Epoch [2][610/721], loss_cls: 5.7511, reconstruct_loss: 2.6911 lr: 7.81250e-06, loss: 5.1275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2][615/721], loss_cls: 5.8299, reconstruct_loss: 2.1944 lr: 7.81250e-06, loss: 5.3437\n",
      "Epoch [2][620/721], loss_cls: 5.5651, reconstruct_loss: 2.0238 lr: 7.81250e-06, loss: 5.155\n",
      "Epoch [2][625/721], loss_cls: 5.8258, reconstruct_loss: 2.6239 lr: 7.81250e-06, loss: 5.2447\n",
      "Epoch [2][630/721], loss_cls: 5.8735, reconstruct_loss: 2.0369 lr: 7.81250e-06, loss: 5.2954\n",
      "Epoch [2][635/721], loss_cls: 6.1254, reconstruct_loss: 3.5061 lr: 7.81250e-06, loss: 5.1621\n",
      "Epoch [2][640/721], loss_cls: 6.0142, reconstruct_loss: 2.1146 lr: 7.81250e-06, loss: 5.3238\n",
      "Epoch [2][645/721], loss_cls: 5.8725, reconstruct_loss: 0.76192 lr: 7.81250e-06, loss: 5.1959\n",
      "Epoch [2][650/721], loss_cls: 5.9484, reconstruct_loss: 2.5367 lr: 7.81250e-06, loss: 5.2149\n",
      "Epoch [2][655/721], loss_cls: 5.9788, reconstruct_loss: 2.9929 lr: 7.81250e-06, loss: 5.0581\n",
      "Epoch [2][660/721], loss_cls: 5.7171, reconstruct_loss: 2.2 lr: 7.81250e-06, loss: 5.0674\n",
      "Epoch [2][665/721], loss_cls: 6.2615, reconstruct_loss: 2.7697 lr: 7.81250e-06, loss: 5.1914\n",
      "Epoch [2][670/721], loss_cls: 5.8409, reconstruct_loss: 1.5787 lr: 7.81250e-06, loss: 5.3177\n",
      "Epoch [2][675/721], loss_cls: 5.4128, reconstruct_loss: 2.1404 lr: 7.81250e-06, loss: 5.0401\n",
      "Epoch [2][680/721], loss_cls: 6.2257, reconstruct_loss: 3.3029 lr: 7.81250e-06, loss: 5.2518\n",
      "Epoch [2][685/721], loss_cls: 5.7994, reconstruct_loss: 2.8338 lr: 7.81250e-06, loss: 5.196\n",
      "Epoch [2][690/721], loss_cls: 6.125, reconstruct_loss: 1.2719 lr: 7.81250e-06, loss: 5.1723\n",
      "Epoch [2][695/721], loss_cls: 5.8317, reconstruct_loss: 2.0387 lr: 7.81250e-06, loss: 5.1217\n",
      "Epoch [2][700/721], loss_cls: 6.086, reconstruct_loss: 3.5668 lr: 7.81250e-06, loss: 5.4402\n",
      "Epoch [2][705/721], loss_cls: 5.7417, reconstruct_loss: 1.884 lr: 7.81250e-06, loss: 5.2591\n",
      "Epoch [2][710/721], loss_cls: 4.9317, reconstruct_loss: 1.6835 lr: 7.81250e-06, loss: 5.0156\n",
      "Epoch [2][715/721], loss_cls: 6.0172, reconstruct_loss: 1.9617 lr: 7.81250e-06, loss: 5.0829\n",
      "Epoch [2][720/721], loss_cls: 5.7425, reconstruct_loss: 2.8878 lr: 7.81250e-06, loss: 5.1379\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.003876, top5_acc: 0.03101, train_loss: 5.1379, val_loss: 5.1897\n",
      "Saving checkpoint at 2 epochs...\n",
      "Epoch [3][5/721], loss_cls: 5.1196, reconstruct_loss: 2.8617 lr: 1.56250e-05, loss: 4.858\n",
      "Epoch [3][10/721], loss_cls: 5.6211, reconstruct_loss: 2.226 lr: 1.56250e-05, loss: 5.0884\n",
      "Epoch [3][15/721], loss_cls: 6.8921, reconstruct_loss: 1.9185 lr: 1.56250e-05, loss: 5.2062\n",
      "Epoch [3][20/721], loss_cls: 5.681, reconstruct_loss: 2.6614 lr: 1.56250e-05, loss: 4.9557\n",
      "Epoch [3][25/721], loss_cls: 6.2678, reconstruct_loss: 3.531 lr: 1.56250e-05, loss: 5.4526\n",
      "Epoch [3][30/721], loss_cls: 6.2365, reconstruct_loss: 2.5929 lr: 1.56250e-05, loss: 5.1682\n",
      "Epoch [3][35/721], loss_cls: 6.1785, reconstruct_loss: 1.9293 lr: 1.56250e-05, loss: 5.2059\n",
      "Epoch [3][40/721], loss_cls: 5.6411, reconstruct_loss: 2.1483 lr: 1.56250e-05, loss: 5.0652\n",
      "Epoch [3][45/721], loss_cls: 6.0033, reconstruct_loss: 2.0537 lr: 1.56250e-05, loss: 5.1851\n",
      "Epoch [3][50/721], loss_cls: 5.9923, reconstruct_loss: 2.0695 lr: 1.56250e-05, loss: 4.9809\n",
      "Epoch [3][55/721], loss_cls: 5.7791, reconstruct_loss: 3.0096 lr: 1.56250e-05, loss: 5.0099\n",
      "Epoch [3][60/721], loss_cls: 5.6371, reconstruct_loss: 1.8139 lr: 1.56250e-05, loss: 5.1186\n",
      "Epoch [3][65/721], loss_cls: 6.0289, reconstruct_loss: 3.2803 lr: 1.56250e-05, loss: 5.1386\n",
      "Epoch [3][70/721], loss_cls: 5.7863, reconstruct_loss: 1.672 lr: 1.56250e-05, loss: 5.1664\n",
      "Epoch [3][75/721], loss_cls: 5.3719, reconstruct_loss: 1.825 lr: 1.56250e-05, loss: 4.9159\n",
      "Epoch [3][80/721], loss_cls: 5.3211, reconstruct_loss: 2.0978 lr: 1.56250e-05, loss: 4.9594\n",
      "Epoch [3][85/721], loss_cls: 6.827, reconstruct_loss: 3.0101 lr: 1.56250e-05, loss: 5.3601\n",
      "Epoch [3][90/721], loss_cls: 5.109, reconstruct_loss: 2.4637 lr: 1.56250e-05, loss: 4.8763\n",
      "Epoch [3][95/721], loss_cls: 5.7045, reconstruct_loss: 2.7927 lr: 1.56250e-05, loss: 5.1083\n",
      "Epoch [3][100/721], loss_cls: 5.2245, reconstruct_loss: 3.2784 lr: 1.56250e-05, loss: 5.1843\n",
      "Epoch [3][105/721], loss_cls: 6.1156, reconstruct_loss: 2.0085 lr: 1.56250e-05, loss: 5.3507\n",
      "Epoch [3][110/721], loss_cls: 6.2736, reconstruct_loss: 1.8235 lr: 1.56250e-05, loss: 5.022\n",
      "Epoch [3][115/721], loss_cls: 6.0395, reconstruct_loss: 1.7347 lr: 1.56250e-05, loss: 5.0685\n",
      "Epoch [3][120/721], loss_cls: 6.0131, reconstruct_loss: 2.8867 lr: 1.56250e-05, loss: 5.1234\n",
      "Epoch [3][125/721], loss_cls: 6.4535, reconstruct_loss: 1.8156 lr: 1.56250e-05, loss: 5.1272\n",
      "Epoch [3][130/721], loss_cls: 5.7201, reconstruct_loss: 1.7407 lr: 1.56250e-05, loss: 5.1442\n",
      "Epoch [3][135/721], loss_cls: 6.4319, reconstruct_loss: 1.6311 lr: 1.56250e-05, loss: 5.0212\n",
      "Epoch [3][140/721], loss_cls: 5.4494, reconstruct_loss: 2.6315 lr: 1.56250e-05, loss: 4.8686\n",
      "Epoch [3][145/721], loss_cls: 6.1335, reconstruct_loss: 2.3813 lr: 1.56250e-05, loss: 4.8825\n",
      "Epoch [3][150/721], loss_cls: 5.5977, reconstruct_loss: 2.6835 lr: 1.56250e-05, loss: 5.0736\n",
      "Epoch [3][155/721], loss_cls: 5.3742, reconstruct_loss: 3.7005 lr: 1.56250e-05, loss: 4.9691\n",
      "Epoch [3][160/721], loss_cls: 6.0736, reconstruct_loss: 3.3604 lr: 1.56250e-05, loss: 5.2523\n",
      "Epoch [3][165/721], loss_cls: 5.41, reconstruct_loss: 2.225 lr: 1.56250e-05, loss: 4.8101\n",
      "Epoch [3][170/721], loss_cls: 5.6566, reconstruct_loss: 3.5778 lr: 1.56250e-05, loss: 5.2675\n",
      "Epoch [3][175/721], loss_cls: 5.4597, reconstruct_loss: 1.7829 lr: 1.56250e-05, loss: 4.8128\n",
      "Epoch [3][180/721], loss_cls: 5.4248, reconstruct_loss: 2.6011 lr: 1.56250e-05, loss: 5.1328\n",
      "Epoch [3][185/721], loss_cls: 6.0235, reconstruct_loss: 3.3807 lr: 1.56250e-05, loss: 5.2954\n",
      "Epoch [3][190/721], loss_cls: 5.7144, reconstruct_loss: 3.6438 lr: 1.56250e-05, loss: 5.5164\n",
      "Epoch [3][195/721], loss_cls: 5.6156, reconstruct_loss: 1.9289 lr: 1.56250e-05, loss: 5.0418\n",
      "Epoch [3][200/721], loss_cls: 5.6204, reconstruct_loss: 2.5296 lr: 1.56250e-05, loss: 5.0944\n",
      "Epoch [3][205/721], loss_cls: 5.9847, reconstruct_loss: 2.1761 lr: 1.56250e-05, loss: 4.8532\n",
      "Epoch [3][210/721], loss_cls: 5.1914, reconstruct_loss: 1.4443 lr: 1.56250e-05, loss: 5.0157\n",
      "Epoch [3][215/721], loss_cls: 5.5214, reconstruct_loss: 1.4555 lr: 1.56250e-05, loss: 5.0229\n",
      "Epoch [3][220/721], loss_cls: 5.5403, reconstruct_loss: 2.2827 lr: 1.56250e-05, loss: 4.938\n",
      "Epoch [3][225/721], loss_cls: 5.0927, reconstruct_loss: 1.2868 lr: 1.56250e-05, loss: 4.8914\n",
      "Epoch [3][230/721], loss_cls: 6.443, reconstruct_loss: 1.4517 lr: 1.56250e-05, loss: 5.096\n",
      "Epoch [3][235/721], loss_cls: 5.5181, reconstruct_loss: 3.371 lr: 1.56250e-05, loss: 5.1075\n",
      "Epoch [3][240/721], loss_cls: 5.7071, reconstruct_loss: 2.7605 lr: 1.56250e-05, loss: 4.957\n",
      "Epoch [3][245/721], loss_cls: 5.7442, reconstruct_loss: 2.1884 lr: 1.56250e-05, loss: 5.015\n",
      "Epoch [3][250/721], loss_cls: 5.4927, reconstruct_loss: 1.8589 lr: 1.56250e-05, loss: 5.0945\n",
      "Epoch [3][255/721], loss_cls: 6.7191, reconstruct_loss: 2.0763 lr: 1.56250e-05, loss: 5.0947\n",
      "Epoch [3][260/721], loss_cls: 5.786, reconstruct_loss: 1.5658 lr: 1.56250e-05, loss: 4.8895\n",
      "Epoch [3][265/721], loss_cls: 5.8338, reconstruct_loss: 1.3766 lr: 1.56250e-05, loss: 5.0349\n",
      "Epoch [3][270/721], loss_cls: 6.2148, reconstruct_loss: 2.557 lr: 1.56250e-05, loss: 5.2524\n",
      "Epoch [3][275/721], loss_cls: 5.3934, reconstruct_loss: 3.0778 lr: 1.56250e-05, loss: 4.9076\n",
      "Epoch [3][280/721], loss_cls: 5.3551, reconstruct_loss: 1.8556 lr: 1.56250e-05, loss: 4.7841\n",
      "Epoch [3][285/721], loss_cls: 5.6524, reconstruct_loss: 1.8296 lr: 1.56250e-05, loss: 4.8754\n",
      "Epoch [3][290/721], loss_cls: 4.6171, reconstruct_loss: 2.6151 lr: 1.56250e-05, loss: 5.0264\n",
      "Epoch [3][295/721], loss_cls: 5.4559, reconstruct_loss: 2.8562 lr: 1.56250e-05, loss: 5.2831\n",
      "Epoch [3][300/721], loss_cls: 4.9453, reconstruct_loss: 3.466 lr: 1.56250e-05, loss: 4.9001\n",
      "Epoch [3][305/721], loss_cls: 5.0417, reconstruct_loss: 1.63 lr: 1.56250e-05, loss: 4.6794\n",
      "Epoch [3][310/721], loss_cls: 5.9016, reconstruct_loss: 1.5844 lr: 1.56250e-05, loss: 4.817\n",
      "Epoch [3][315/721], loss_cls: 5.1647, reconstruct_loss: 3.0696 lr: 1.56250e-05, loss: 5.1466\n",
      "Epoch [3][320/721], loss_cls: 6.4488, reconstruct_loss: 3.2937 lr: 1.56250e-05, loss: 5.1038\n",
      "Epoch [3][325/721], loss_cls: 6.1361, reconstruct_loss: 2.2868 lr: 1.56250e-05, loss: 4.7379\n",
      "Epoch [3][330/721], loss_cls: 5.4038, reconstruct_loss: 3.9726 lr: 1.56250e-05, loss: 4.9805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3][335/721], loss_cls: 5.9034, reconstruct_loss: 1.5243 lr: 1.56250e-05, loss: 4.749\n",
      "Epoch [3][340/721], loss_cls: 5.9739, reconstruct_loss: 2.0973 lr: 1.56250e-05, loss: 4.6631\n",
      "Epoch [3][345/721], loss_cls: 4.6865, reconstruct_loss: 1.3546 lr: 1.56250e-05, loss: 4.4989\n",
      "Epoch [3][350/721], loss_cls: 5.5368, reconstruct_loss: 1.625 lr: 1.56250e-05, loss: 5.0658\n",
      "Epoch [3][355/721], loss_cls: 4.901, reconstruct_loss: 3.0479 lr: 1.56250e-05, loss: 4.8572\n",
      "Epoch [3][360/721], loss_cls: 5.3632, reconstruct_loss: 2.9578 lr: 1.56250e-05, loss: 4.9926\n",
      "Epoch [3][365/721], loss_cls: 5.4993, reconstruct_loss: 2.0886 lr: 1.56250e-05, loss: 5.1123\n",
      "Epoch [3][370/721], loss_cls: 6.0068, reconstruct_loss: 2.9424 lr: 1.56250e-05, loss: 4.8817\n",
      "Epoch [3][375/721], loss_cls: 5.2896, reconstruct_loss: 2.9635 lr: 1.56250e-05, loss: 4.7918\n",
      "Epoch [3][380/721], loss_cls: 5.3786, reconstruct_loss: 1.9215 lr: 1.56250e-05, loss: 4.9412\n",
      "Epoch [3][385/721], loss_cls: 5.5944, reconstruct_loss: 3.0504 lr: 1.56250e-05, loss: 4.7203\n",
      "Epoch [3][390/721], loss_cls: 5.1861, reconstruct_loss: 2.0861 lr: 1.56250e-05, loss: 4.5854\n",
      "Epoch [3][395/721], loss_cls: 5.3911, reconstruct_loss: 2.8293 lr: 1.56250e-05, loss: 4.6596\n",
      "Epoch [3][400/721], loss_cls: 5.633, reconstruct_loss: 2.2977 lr: 1.56250e-05, loss: 5.0607\n",
      "Epoch [3][405/721], loss_cls: 5.4914, reconstruct_loss: 2.6794 lr: 1.56250e-05, loss: 4.5769\n",
      "Epoch [3][410/721], loss_cls: 5.7029, reconstruct_loss: 3.0614 lr: 1.56250e-05, loss: 4.9071\n",
      "Epoch [3][415/721], loss_cls: 5.5258, reconstruct_loss: 1.7158 lr: 1.56250e-05, loss: 4.6414\n",
      "Epoch [3][420/721], loss_cls: 4.3059, reconstruct_loss: 2.3622 lr: 1.56250e-05, loss: 5.067\n",
      "Epoch [3][425/721], loss_cls: 5.5971, reconstruct_loss: 1.5495 lr: 1.56250e-05, loss: 4.3186\n",
      "Epoch [3][430/721], loss_cls: 5.7728, reconstruct_loss: 1.4709 lr: 1.56250e-05, loss: 4.9563\n",
      "Epoch [3][435/721], loss_cls: 5.527, reconstruct_loss: 4.3438 lr: 1.56250e-05, loss: 4.914\n",
      "Epoch [3][440/721], loss_cls: 4.8467, reconstruct_loss: 2.8447 lr: 1.56250e-05, loss: 4.3654\n",
      "Epoch [3][445/721], loss_cls: 4.8903, reconstruct_loss: 1.2098 lr: 1.56250e-05, loss: 4.8095\n",
      "Epoch [3][450/721], loss_cls: 5.1855, reconstruct_loss: 1.9923 lr: 1.56250e-05, loss: 4.4807\n",
      "Epoch [3][455/721], loss_cls: 5.001, reconstruct_loss: 0.8881 lr: 1.56250e-05, loss: 4.357\n",
      "Epoch [3][460/721], loss_cls: 4.1649, reconstruct_loss: 2.4072 lr: 1.56250e-05, loss: 4.3901\n",
      "Epoch [3][465/721], loss_cls: 4.9206, reconstruct_loss: 3.5705 lr: 1.56250e-05, loss: 4.736\n",
      "Epoch [3][470/721], loss_cls: 5.4397, reconstruct_loss: 2.4782 lr: 1.56250e-05, loss: 4.8672\n",
      "Epoch [3][475/721], loss_cls: 5.2836, reconstruct_loss: 1.6603 lr: 1.56250e-05, loss: 4.7903\n",
      "Epoch [3][480/721], loss_cls: 5.4209, reconstruct_loss: 3.0906 lr: 1.56250e-05, loss: 4.8275\n",
      "Epoch [3][485/721], loss_cls: 5.0189, reconstruct_loss: 2.6803 lr: 1.56250e-05, loss: 4.7709\n",
      "Epoch [3][490/721], loss_cls: 5.6205, reconstruct_loss: 1.6612 lr: 1.56250e-05, loss: 4.4859\n",
      "Epoch [3][495/721], loss_cls: 5.0016, reconstruct_loss: 3.1117 lr: 1.56250e-05, loss: 4.8514\n",
      "Epoch [3][500/721], loss_cls: 5.559, reconstruct_loss: 3.383 lr: 1.56250e-05, loss: 4.8449\n",
      "Epoch [3][505/721], loss_cls: 6.0356, reconstruct_loss: 3.6493 lr: 1.56250e-05, loss: 4.942\n",
      "Epoch [3][510/721], loss_cls: 4.734, reconstruct_loss: 1.7663 lr: 1.56250e-05, loss: 4.7613\n",
      "Epoch [3][515/721], loss_cls: 5.3433, reconstruct_loss: 2.4781 lr: 1.56250e-05, loss: 4.8796\n",
      "Epoch [3][520/721], loss_cls: 5.5261, reconstruct_loss: 1.3522 lr: 1.56250e-05, loss: 4.7593\n",
      "Epoch [3][525/721], loss_cls: 6.0012, reconstruct_loss: 2.9476 lr: 1.56250e-05, loss: 5.0768\n",
      "Epoch [3][530/721], loss_cls: 5.36, reconstruct_loss: 3.1913 lr: 1.56250e-05, loss: 4.7996\n",
      "Epoch [3][535/721], loss_cls: 5.3481, reconstruct_loss: 1.449 lr: 1.56250e-05, loss: 4.6098\n",
      "Epoch [3][540/721], loss_cls: 5.4524, reconstruct_loss: 1.8905 lr: 1.56250e-05, loss: 4.953\n",
      "Epoch [3][545/721], loss_cls: 4.3874, reconstruct_loss: 2.0777 lr: 1.56250e-05, loss: 4.7554\n",
      "Epoch [3][550/721], loss_cls: 5.1911, reconstruct_loss: 1.0965 lr: 1.56250e-05, loss: 5.0107\n",
      "Epoch [3][555/721], loss_cls: 5.3464, reconstruct_loss: 2.4106 lr: 1.56250e-05, loss: 4.234\n",
      "Epoch [3][560/721], loss_cls: 6.0576, reconstruct_loss: 1.9643 lr: 1.56250e-05, loss: 4.7545\n",
      "Epoch [3][565/721], loss_cls: 5.3437, reconstruct_loss: 1.751 lr: 1.56250e-05, loss: 4.7361\n",
      "Epoch [3][570/721], loss_cls: 5.1701, reconstruct_loss: 1.1824 lr: 1.56250e-05, loss: 4.7399\n",
      "Epoch [3][575/721], loss_cls: 5.791, reconstruct_loss: 2.525 lr: 1.56250e-05, loss: 4.6569\n",
      "Epoch [3][580/721], loss_cls: 4.6346, reconstruct_loss: 2.1114 lr: 1.56250e-05, loss: 4.4992\n",
      "Epoch [3][585/721], loss_cls: 5.3207, reconstruct_loss: 2.5933 lr: 1.56250e-05, loss: 4.915\n",
      "Epoch [3][590/721], loss_cls: 4.8237, reconstruct_loss: 3.27 lr: 1.56250e-05, loss: 4.4167\n",
      "Epoch [3][595/721], loss_cls: 5.1063, reconstruct_loss: 2.0946 lr: 1.56250e-05, loss: 4.8583\n",
      "Epoch [3][600/721], loss_cls: 4.6586, reconstruct_loss: 3.2686 lr: 1.56250e-05, loss: 5.0849\n",
      "Epoch [3][605/721], loss_cls: 5.2587, reconstruct_loss: 1.8395 lr: 1.56250e-05, loss: 5.0451\n",
      "Epoch [3][610/721], loss_cls: 5.3617, reconstruct_loss: 1.8729 lr: 1.56250e-05, loss: 4.8943\n",
      "Epoch [3][615/721], loss_cls: 5.7866, reconstruct_loss: 3.5697 lr: 1.56250e-05, loss: 4.9949\n",
      "Epoch [3][620/721], loss_cls: 4.7628, reconstruct_loss: 2.2821 lr: 1.56250e-05, loss: 4.5888\n",
      "Epoch [3][625/721], loss_cls: 5.6327, reconstruct_loss: 1.7949 lr: 1.56250e-05, loss: 4.6521\n",
      "Epoch [3][630/721], loss_cls: 5.3994, reconstruct_loss: 3.0098 lr: 1.56250e-05, loss: 4.769\n",
      "Epoch [3][635/721], loss_cls: 4.7478, reconstruct_loss: 2.5053 lr: 1.56250e-05, loss: 4.5721\n",
      "Epoch [3][640/721], loss_cls: 5.3031, reconstruct_loss: 2.2807 lr: 1.56250e-05, loss: 4.5881\n",
      "Epoch [3][645/721], loss_cls: 5.2441, reconstruct_loss: 3.688 lr: 1.56250e-05, loss: 4.5615\n",
      "Epoch [3][650/721], loss_cls: 6.0953, reconstruct_loss: 1.4375 lr: 1.56250e-05, loss: 4.6753\n",
      "Epoch [3][655/721], loss_cls: 5.01, reconstruct_loss: 1.682 lr: 1.56250e-05, loss: 4.6728\n",
      "Epoch [3][660/721], loss_cls: 4.5606, reconstruct_loss: 2.1975 lr: 1.56250e-05, loss: 4.7254\n",
      "Epoch [3][665/721], loss_cls: 4.8122, reconstruct_loss: 2.2465 lr: 1.56250e-05, loss: 4.536\n",
      "Epoch [3][670/721], loss_cls: 5.3902, reconstruct_loss: 1.5964 lr: 1.56250e-05, loss: 4.7358\n",
      "Epoch [3][675/721], loss_cls: 4.8487, reconstruct_loss: 2.2501 lr: 1.56250e-05, loss: 4.6333\n",
      "Epoch [3][680/721], loss_cls: 4.6668, reconstruct_loss: 2.5967 lr: 1.56250e-05, loss: 4.6922\n",
      "Epoch [3][685/721], loss_cls: 6.3073, reconstruct_loss: 1.3023 lr: 1.56250e-05, loss: 4.7305\n",
      "Epoch [3][690/721], loss_cls: 5.1188, reconstruct_loss: 1.5201 lr: 1.56250e-05, loss: 4.6196\n",
      "Epoch [3][695/721], loss_cls: 5.165, reconstruct_loss: 2.3403 lr: 1.56250e-05, loss: 4.5424\n",
      "Epoch [3][700/721], loss_cls: 5.4467, reconstruct_loss: 1.4334 lr: 1.56250e-05, loss: 4.729\n",
      "Epoch [3][705/721], loss_cls: 5.1674, reconstruct_loss: 1.22 lr: 1.56250e-05, loss: 4.6183\n",
      "Epoch [3][710/721], loss_cls: 4.8545, reconstruct_loss: 1.8182 lr: 1.56250e-05, loss: 4.4417\n",
      "Epoch [3][715/721], loss_cls: 5.7159, reconstruct_loss: 2.8246 lr: 1.56250e-05, loss: 4.7676\n",
      "Epoch [3][720/721], loss_cls: 5.2089, reconstruct_loss: 1.6014 lr: 1.56250e-05, loss: 4.6907\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.01938, top5_acc: 0.05426, train_loss: 4.6907, val_loss: 4.4569\n",
      "Saving checkpoint at 3 epochs...\n",
      "Epoch [4][5/721], loss_cls: 4.506, reconstruct_loss: 1.1769 lr: 2.34375e-05, loss: 3.9528\n",
      "Epoch [4][10/721], loss_cls: 5.6589, reconstruct_loss: 1.7001 lr: 2.34375e-05, loss: 4.9585\n",
      "Epoch [4][15/721], loss_cls: 4.9523, reconstruct_loss: 2.0037 lr: 2.34375e-05, loss: 4.4864\n",
      "Epoch [4][20/721], loss_cls: 4.5011, reconstruct_loss: 2.2941 lr: 2.34375e-05, loss: 4.3125\n",
      "Epoch [4][25/721], loss_cls: 3.7854, reconstruct_loss: 2.3559 lr: 2.34375e-05, loss: 4.4813\n",
      "Epoch [4][30/721], loss_cls: 5.788, reconstruct_loss: 1.8651 lr: 2.34375e-05, loss: 4.6201\n",
      "Epoch [4][35/721], loss_cls: 4.2041, reconstruct_loss: 1.1523 lr: 2.34375e-05, loss: 4.5492\n",
      "Epoch [4][40/721], loss_cls: 6.5583, reconstruct_loss: 1.348 lr: 2.34375e-05, loss: 4.8778\n",
      "Epoch [4][45/721], loss_cls: 6.2006, reconstruct_loss: 1.3693 lr: 2.34375e-05, loss: 4.6675\n",
      "Epoch [4][50/721], loss_cls: 4.9902, reconstruct_loss: 2.6618 lr: 2.34375e-05, loss: 4.7293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4][55/721], loss_cls: 5.2941, reconstruct_loss: 2.3552 lr: 2.34375e-05, loss: 4.4579\n",
      "Epoch [4][60/721], loss_cls: 3.8712, reconstruct_loss: 2.8643 lr: 2.34375e-05, loss: 4.4667\n",
      "Epoch [4][65/721], loss_cls: 4.8961, reconstruct_loss: 2.1197 lr: 2.34375e-05, loss: 4.855\n",
      "Epoch [4][70/721], loss_cls: 5.285, reconstruct_loss: 1.4669 lr: 2.34375e-05, loss: 4.5517\n",
      "Epoch [4][75/721], loss_cls: 4.4621, reconstruct_loss: 1.298 lr: 2.34375e-05, loss: 4.5179\n",
      "Epoch [4][80/721], loss_cls: 4.31, reconstruct_loss: 2.4093 lr: 2.34375e-05, loss: 4.5033\n",
      "Epoch [4][85/721], loss_cls: 5.7649, reconstruct_loss: 2.4129 lr: 2.34375e-05, loss: 4.8026\n",
      "Epoch [4][90/721], loss_cls: 4.3198, reconstruct_loss: 1.9382 lr: 2.34375e-05, loss: 4.3824\n",
      "Epoch [4][95/721], loss_cls: 4.6917, reconstruct_loss: 2.0446 lr: 2.34375e-05, loss: 4.1453\n",
      "Epoch [4][100/721], loss_cls: 3.7388, reconstruct_loss: 2.3478 lr: 2.34375e-05, loss: 4.5288\n",
      "Epoch [4][105/721], loss_cls: 4.9031, reconstruct_loss: 3.6034 lr: 2.34375e-05, loss: 4.6592\n",
      "Epoch [4][110/721], loss_cls: 6.291, reconstruct_loss: 1.5991 lr: 2.34375e-05, loss: 4.7289\n",
      "Epoch [4][115/721], loss_cls: 4.7857, reconstruct_loss: 2.2572 lr: 2.34375e-05, loss: 4.1728\n",
      "Epoch [4][120/721], loss_cls: 4.883, reconstruct_loss: 1.5375 lr: 2.34375e-05, loss: 4.5261\n",
      "Epoch [4][125/721], loss_cls: 4.5407, reconstruct_loss: 1.8732 lr: 2.34375e-05, loss: 4.7156\n",
      "Epoch [4][130/721], loss_cls: 6.1085, reconstruct_loss: 2.5519 lr: 2.34375e-05, loss: 4.8804\n",
      "Epoch [4][135/721], loss_cls: 5.6984, reconstruct_loss: 2.7581 lr: 2.34375e-05, loss: 5.0968\n",
      "Epoch [4][140/721], loss_cls: 5.1954, reconstruct_loss: 2.0266 lr: 2.34375e-05, loss: 4.9014\n",
      "Epoch [4][145/721], loss_cls: 5.2376, reconstruct_loss: 1.1703 lr: 2.34375e-05, loss: 4.537\n",
      "Epoch [4][150/721], loss_cls: 5.1688, reconstruct_loss: 1.3646 lr: 2.34375e-05, loss: 4.7523\n",
      "Epoch [4][155/721], loss_cls: 4.3142, reconstruct_loss: 1.4091 lr: 2.34375e-05, loss: 4.2078\n",
      "Epoch [4][160/721], loss_cls: 5.5561, reconstruct_loss: 1.9889 lr: 2.34375e-05, loss: 4.6229\n",
      "Epoch [4][165/721], loss_cls: 4.352, reconstruct_loss: 2.2496 lr: 2.34375e-05, loss: 4.5011\n",
      "Epoch [4][170/721], loss_cls: 5.5933, reconstruct_loss: 2.827 lr: 2.34375e-05, loss: 4.7349\n",
      "Epoch [4][175/721], loss_cls: 4.9849, reconstruct_loss: 3.536 lr: 2.34375e-05, loss: 4.9433\n",
      "Epoch [4][180/721], loss_cls: 5.576, reconstruct_loss: 2.7926 lr: 2.34375e-05, loss: 4.6391\n",
      "Epoch [4][185/721], loss_cls: 5.7315, reconstruct_loss: 3.1261 lr: 2.34375e-05, loss: 4.5801\n",
      "Epoch [4][190/721], loss_cls: 5.7932, reconstruct_loss: 2.7106 lr: 2.34375e-05, loss: 4.8282\n",
      "Epoch [4][195/721], loss_cls: 5.7845, reconstruct_loss: 2.4811 lr: 2.34375e-05, loss: 4.6535\n",
      "Epoch [4][200/721], loss_cls: 5.3679, reconstruct_loss: 1.2804 lr: 2.34375e-05, loss: 4.8521\n",
      "Epoch [4][205/721], loss_cls: 5.9478, reconstruct_loss: 1.4451 lr: 2.34375e-05, loss: 4.8164\n",
      "Epoch [4][210/721], loss_cls: 5.0534, reconstruct_loss: 2.567 lr: 2.34375e-05, loss: 4.8748\n",
      "Epoch [4][215/721], loss_cls: 5.4152, reconstruct_loss: 1.766 lr: 2.34375e-05, loss: 4.7392\n",
      "Epoch [4][220/721], loss_cls: 5.3566, reconstruct_loss: 1.9688 lr: 2.34375e-05, loss: 4.7861\n",
      "Epoch [4][225/721], loss_cls: 4.7346, reconstruct_loss: 1.4272 lr: 2.34375e-05, loss: 4.5449\n",
      "Epoch [4][230/721], loss_cls: 5.1374, reconstruct_loss: 2.0799 lr: 2.34375e-05, loss: 4.6977\n",
      "Epoch [4][235/721], loss_cls: 4.8205, reconstruct_loss: 1.4697 lr: 2.34375e-05, loss: 4.5529\n",
      "Epoch [4][240/721], loss_cls: 4.675, reconstruct_loss: 2.9121 lr: 2.34375e-05, loss: 4.1136\n",
      "Epoch [4][245/721], loss_cls: 5.0938, reconstruct_loss: 2.4447 lr: 2.34375e-05, loss: 4.8798\n",
      "Epoch [4][250/721], loss_cls: 4.7193, reconstruct_loss: 1.0688 lr: 2.34375e-05, loss: 4.6984\n",
      "Epoch [4][255/721], loss_cls: 5.2303, reconstruct_loss: 1.5899 lr: 2.34375e-05, loss: 4.657\n",
      "Epoch [4][260/721], loss_cls: 6.008, reconstruct_loss: 1.4014 lr: 2.34375e-05, loss: 4.272\n",
      "Epoch [4][265/721], loss_cls: 5.1263, reconstruct_loss: 2.7917 lr: 2.34375e-05, loss: 4.7802\n",
      "Epoch [4][270/721], loss_cls: 3.4293, reconstruct_loss: 1.5173 lr: 2.34375e-05, loss: 4.3111\n",
      "Epoch [4][275/721], loss_cls: 4.6283, reconstruct_loss: 2.369 lr: 2.34375e-05, loss: 4.3726\n",
      "Epoch [4][280/721], loss_cls: 5.8356, reconstruct_loss: 1.9042 lr: 2.34375e-05, loss: 4.5207\n",
      "Epoch [4][285/721], loss_cls: 5.3161, reconstruct_loss: 2.6952 lr: 2.34375e-05, loss: 4.5546\n",
      "Epoch [4][290/721], loss_cls: 5.6337, reconstruct_loss: 2.3864 lr: 2.34375e-05, loss: 4.5178\n",
      "Epoch [4][295/721], loss_cls: 5.2831, reconstruct_loss: 1.6341 lr: 2.34375e-05, loss: 4.4835\n",
      "Epoch [4][300/721], loss_cls: 4.7133, reconstruct_loss: 2.6769 lr: 2.34375e-05, loss: 4.3804\n",
      "Epoch [4][305/721], loss_cls: 5.1708, reconstruct_loss: 2.4479 lr: 2.34375e-05, loss: 4.627\n",
      "Epoch [4][310/721], loss_cls: 5.0739, reconstruct_loss: 2.0573 lr: 2.34375e-05, loss: 4.6584\n",
      "Epoch [4][315/721], loss_cls: 4.6661, reconstruct_loss: 2.465 lr: 2.34375e-05, loss: 4.5629\n",
      "Epoch [4][320/721], loss_cls: 4.8656, reconstruct_loss: 1.0 lr: 2.34375e-05, loss: 4.533\n",
      "Epoch [4][325/721], loss_cls: 5.5364, reconstruct_loss: 1.7718 lr: 2.34375e-05, loss: 4.4003\n",
      "Epoch [4][330/721], loss_cls: 5.9773, reconstruct_loss: 2.1871 lr: 2.34375e-05, loss: 4.7034\n",
      "Epoch [4][335/721], loss_cls: 5.3492, reconstruct_loss: 1.312 lr: 2.34375e-05, loss: 4.3509\n",
      "Epoch [4][340/721], loss_cls: 5.2929, reconstruct_loss: 1.5379 lr: 2.34375e-05, loss: 4.5091\n",
      "Epoch [4][345/721], loss_cls: 4.7927, reconstruct_loss: 1.4579 lr: 2.34375e-05, loss: 4.3878\n",
      "Epoch [4][350/721], loss_cls: 5.7094, reconstruct_loss: 1.7999 lr: 2.34375e-05, loss: 4.6955\n",
      "Epoch [4][355/721], loss_cls: 4.9887, reconstruct_loss: 1.8189 lr: 2.34375e-05, loss: 3.9527\n",
      "Epoch [4][360/721], loss_cls: 6.0682, reconstruct_loss: 2.5296 lr: 2.34375e-05, loss: 4.7991\n",
      "Epoch [4][365/721], loss_cls: 5.3225, reconstruct_loss: 2.2385 lr: 2.34375e-05, loss: 4.6785\n",
      "Epoch [4][370/721], loss_cls: 3.8196, reconstruct_loss: 1.8375 lr: 2.34375e-05, loss: 4.4256\n",
      "Epoch [4][375/721], loss_cls: 4.8284, reconstruct_loss: 2.7313 lr: 2.34375e-05, loss: 4.6073\n",
      "Epoch [4][380/721], loss_cls: 4.9413, reconstruct_loss: 1.2763 lr: 2.34375e-05, loss: 4.2515\n",
      "Epoch [4][385/721], loss_cls: 4.8108, reconstruct_loss: 2.0312 lr: 2.34375e-05, loss: 4.4356\n",
      "Epoch [4][390/721], loss_cls: 5.626, reconstruct_loss: 0.79505 lr: 2.34375e-05, loss: 4.4531\n",
      "Epoch [4][395/721], loss_cls: 4.9657, reconstruct_loss: 1.3862 lr: 2.34375e-05, loss: 4.6648\n",
      "Epoch [4][400/721], loss_cls: 5.1867, reconstruct_loss: 2.2393 lr: 2.34375e-05, loss: 4.4238\n",
      "Epoch [4][405/721], loss_cls: 5.3097, reconstruct_loss: 1.7306 lr: 2.34375e-05, loss: 4.6139\n",
      "Epoch [4][410/721], loss_cls: 4.9336, reconstruct_loss: 1.3019 lr: 2.34375e-05, loss: 4.6141\n",
      "Epoch [4][415/721], loss_cls: 4.2426, reconstruct_loss: 1.5398 lr: 2.34375e-05, loss: 4.4032\n",
      "Epoch [4][420/721], loss_cls: 4.6823, reconstruct_loss: 3.6897 lr: 2.34375e-05, loss: 4.4997\n",
      "Epoch [4][425/721], loss_cls: 4.8619, reconstruct_loss: 2.2489 lr: 2.34375e-05, loss: 4.4161\n",
      "Epoch [4][430/721], loss_cls: 4.2953, reconstruct_loss: 2.0395 lr: 2.34375e-05, loss: 4.0241\n",
      "Epoch [4][435/721], loss_cls: 6.0956, reconstruct_loss: 1.7177 lr: 2.34375e-05, loss: 4.7133\n",
      "Epoch [4][440/721], loss_cls: 5.015, reconstruct_loss: 2.1617 lr: 2.34375e-05, loss: 4.6351\n",
      "Epoch [4][445/721], loss_cls: 5.2393, reconstruct_loss: 2.1699 lr: 2.34375e-05, loss: 4.4957\n",
      "Epoch [4][450/721], loss_cls: 5.104, reconstruct_loss: 2.3339 lr: 2.34375e-05, loss: 4.4381\n",
      "Epoch [4][455/721], loss_cls: 5.2767, reconstruct_loss: 2.9029 lr: 2.34375e-05, loss: 4.8034\n",
      "Epoch [4][460/721], loss_cls: 5.6604, reconstruct_loss: 1.4661 lr: 2.34375e-05, loss: 4.5927\n",
      "Epoch [4][465/721], loss_cls: 4.6368, reconstruct_loss: 1.64 lr: 2.34375e-05, loss: 4.3665\n",
      "Epoch [4][470/721], loss_cls: 5.4976, reconstruct_loss: 2.1964 lr: 2.34375e-05, loss: 4.3984\n",
      "Epoch [4][475/721], loss_cls: 6.0461, reconstruct_loss: 1.629 lr: 2.34375e-05, loss: 4.9347\n",
      "Epoch [4][480/721], loss_cls: 4.8851, reconstruct_loss: 1.7935 lr: 2.34375e-05, loss: 4.5461\n",
      "Epoch [4][485/721], loss_cls: 4.7594, reconstruct_loss: 1.7975 lr: 2.34375e-05, loss: 4.4755\n",
      "Epoch [4][490/721], loss_cls: 6.4282, reconstruct_loss: 1.5796 lr: 2.34375e-05, loss: 4.651\n",
      "Epoch [4][495/721], loss_cls: 5.2649, reconstruct_loss: 1.5892 lr: 2.34375e-05, loss: 4.6256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4][500/721], loss_cls: 5.0503, reconstruct_loss: 1.9544 lr: 2.34375e-05, loss: 4.3417\n",
      "Epoch [4][505/721], loss_cls: 5.2414, reconstruct_loss: 2.4866 lr: 2.34375e-05, loss: 4.334\n",
      "Epoch [4][510/721], loss_cls: 3.8299, reconstruct_loss: 1.9243 lr: 2.34375e-05, loss: 4.1526\n",
      "Epoch [4][515/721], loss_cls: 5.0416, reconstruct_loss: 1.1555 lr: 2.34375e-05, loss: 4.3132\n",
      "Epoch [4][520/721], loss_cls: 4.6436, reconstruct_loss: 3.5353 lr: 2.34375e-05, loss: 4.3868\n",
      "Epoch [4][525/721], loss_cls: 5.892, reconstruct_loss: 1.9366 lr: 2.34375e-05, loss: 4.7217\n",
      "Epoch [4][530/721], loss_cls: 5.4733, reconstruct_loss: 3.698 lr: 2.34375e-05, loss: 4.6756\n",
      "Epoch [4][535/721], loss_cls: 4.6125, reconstruct_loss: 1.5938 lr: 2.34375e-05, loss: 4.5655\n",
      "Epoch [4][540/721], loss_cls: 5.4042, reconstruct_loss: 1.6591 lr: 2.34375e-05, loss: 4.2625\n",
      "Epoch [4][545/721], loss_cls: 5.9773, reconstruct_loss: 1.5682 lr: 2.34375e-05, loss: 4.6146\n",
      "Epoch [4][550/721], loss_cls: 4.3122, reconstruct_loss: 3.8903 lr: 2.34375e-05, loss: 4.5708\n",
      "Epoch [4][555/721], loss_cls: 5.5451, reconstruct_loss: 3.2705 lr: 2.34375e-05, loss: 4.57\n",
      "Epoch [4][560/721], loss_cls: 3.6401, reconstruct_loss: 1.5679 lr: 2.34375e-05, loss: 4.1252\n",
      "Epoch [4][565/721], loss_cls: 4.685, reconstruct_loss: 2.7469 lr: 2.34375e-05, loss: 4.6025\n",
      "Epoch [4][570/721], loss_cls: 5.3881, reconstruct_loss: 2.4356 lr: 2.34375e-05, loss: 4.4418\n",
      "Epoch [4][575/721], loss_cls: 5.4096, reconstruct_loss: 2.7104 lr: 2.34375e-05, loss: 4.6036\n",
      "Epoch [4][580/721], loss_cls: 6.1901, reconstruct_loss: 1.3999 lr: 2.34375e-05, loss: 4.5285\n",
      "Epoch [4][585/721], loss_cls: 5.9383, reconstruct_loss: 1.2649 lr: 2.34375e-05, loss: 4.5365\n",
      "Epoch [4][590/721], loss_cls: 4.6516, reconstruct_loss: 2.2883 lr: 2.34375e-05, loss: 4.294\n",
      "Epoch [4][595/721], loss_cls: 5.7284, reconstruct_loss: 1.8107 lr: 2.34375e-05, loss: 4.7845\n",
      "Epoch [4][600/721], loss_cls: 5.1598, reconstruct_loss: 2.6459 lr: 2.34375e-05, loss: 4.4787\n",
      "Epoch [4][605/721], loss_cls: 4.2681, reconstruct_loss: 2.6726 lr: 2.34375e-05, loss: 4.4537\n",
      "Epoch [4][610/721], loss_cls: 5.9336, reconstruct_loss: 2.218 lr: 2.34375e-05, loss: 4.591\n",
      "Epoch [4][615/721], loss_cls: 5.1611, reconstruct_loss: 1.7326 lr: 2.34375e-05, loss: 4.3542\n",
      "Epoch [4][620/721], loss_cls: 4.6911, reconstruct_loss: 1.2877 lr: 2.34375e-05, loss: 4.2976\n",
      "Epoch [4][625/721], loss_cls: 4.4514, reconstruct_loss: 2.5603 lr: 2.34375e-05, loss: 4.5743\n",
      "Epoch [4][630/721], loss_cls: 5.7884, reconstruct_loss: 1.3799 lr: 2.34375e-05, loss: 4.7146\n",
      "Epoch [4][635/721], loss_cls: 4.7132, reconstruct_loss: 1.8327 lr: 2.34375e-05, loss: 4.5353\n",
      "Epoch [4][640/721], loss_cls: 4.3792, reconstruct_loss: 1.7622 lr: 2.34375e-05, loss: 4.5658\n",
      "Epoch [4][645/721], loss_cls: 5.3295, reconstruct_loss: 0.78899 lr: 2.34375e-05, loss: 4.6295\n",
      "Epoch [4][650/721], loss_cls: 5.2835, reconstruct_loss: 1.6077 lr: 2.34375e-05, loss: 4.8034\n",
      "Epoch [4][655/721], loss_cls: 5.0067, reconstruct_loss: 2.2952 lr: 2.34375e-05, loss: 4.659\n",
      "Epoch [4][660/721], loss_cls: 5.498, reconstruct_loss: 1.3176 lr: 2.34375e-05, loss: 4.6981\n",
      "Epoch [4][665/721], loss_cls: 5.3199, reconstruct_loss: 2.4425 lr: 2.34375e-05, loss: 4.5606\n",
      "Epoch [4][670/721], loss_cls: 5.1354, reconstruct_loss: 2.3132 lr: 2.34375e-05, loss: 4.4665\n",
      "Epoch [4][675/721], loss_cls: 5.4525, reconstruct_loss: 1.5989 lr: 2.34375e-05, loss: 4.3995\n",
      "Epoch [4][680/721], loss_cls: 5.388, reconstruct_loss: 2.4713 lr: 2.34375e-05, loss: 4.3741\n",
      "Epoch [4][685/721], loss_cls: 5.2605, reconstruct_loss: 1.4399 lr: 2.34375e-05, loss: 4.5131\n",
      "Epoch [4][690/721], loss_cls: 5.3524, reconstruct_loss: 1.4668 lr: 2.34375e-05, loss: 3.9864\n",
      "Epoch [4][695/721], loss_cls: 5.3165, reconstruct_loss: 2.1228 lr: 2.34375e-05, loss: 4.2494\n",
      "Epoch [4][700/721], loss_cls: 5.9363, reconstruct_loss: 1.6542 lr: 2.34375e-05, loss: 4.4441\n",
      "Epoch [4][705/721], loss_cls: 4.8137, reconstruct_loss: 1.7986 lr: 2.34375e-05, loss: 4.5894\n",
      "Epoch [4][710/721], loss_cls: 5.7595, reconstruct_loss: 2.6203 lr: 2.34375e-05, loss: 4.7146\n",
      "Epoch [4][715/721], loss_cls: 4.9166, reconstruct_loss: 2.1335 lr: 2.34375e-05, loss: 4.532\n",
      "Epoch [4][720/721], loss_cls: 5.3861, reconstruct_loss: 1.5072 lr: 2.34375e-05, loss: 4.6848\n",
      "Evaluating top_k_accuracy...\n",
      "top1_acc: 0.02713, top5_acc: 0.07364, train_loss: 4.6848, val_loss: 4.3279\n",
      "Saving checkpoint at 4 epochs...\n",
      "Epoch [5][5/721], loss_cls: 5.458, reconstruct_loss: 1.4517 lr: 3.12500e-05, loss: 4.4487\n",
      "Epoch [5][10/721], loss_cls: 5.4206, reconstruct_loss: 2.0873 lr: 3.12500e-05, loss: 4.5093\n",
      "Epoch [5][15/721], loss_cls: 5.6759, reconstruct_loss: 1.0603 lr: 3.12500e-05, loss: 4.3473\n",
      "Epoch [5][20/721], loss_cls: 4.8764, reconstruct_loss: 1.8982 lr: 3.12500e-05, loss: 4.2408\n",
      "Epoch [5][25/721], loss_cls: 3.8933, reconstruct_loss: 2.3987 lr: 3.12500e-05, loss: 4.3225\n",
      "Epoch [5][30/721], loss_cls: 5.2427, reconstruct_loss: 2.9334 lr: 3.12500e-05, loss: 4.2767\n",
      "Epoch [5][35/721], loss_cls: 4.7532, reconstruct_loss: 1.4088 lr: 3.12500e-05, loss: 4.4781\n",
      "Epoch [5][40/721], loss_cls: 5.3784, reconstruct_loss: 2.6787 lr: 3.12500e-05, loss: 4.423\n",
      "Epoch [5][45/721], loss_cls: 5.137, reconstruct_loss: 1.9455 lr: 3.12500e-05, loss: 4.1325\n",
      "Epoch [5][50/721], loss_cls: 4.9931, reconstruct_loss: 1.6219 lr: 3.12500e-05, loss: 4.8111\n",
      "Epoch [5][55/721], loss_cls: 4.6476, reconstruct_loss: 3.4784 lr: 3.12500e-05, loss: 4.3482\n",
      "Epoch [5][60/721], loss_cls: 4.7605, reconstruct_loss: 1.5568 lr: 3.12500e-05, loss: 4.2485\n",
      "Epoch [5][65/721], loss_cls: 4.9326, reconstruct_loss: 0.98333 lr: 3.12500e-05, loss: 4.4189\n",
      "Epoch [5][70/721], loss_cls: 5.0094, reconstruct_loss: 1.319 lr: 3.12500e-05, loss: 4.4491\n",
      "Epoch [5][75/721], loss_cls: 5.1176, reconstruct_loss: 1.4912 lr: 3.12500e-05, loss: 4.2473\n",
      "Epoch [5][80/721], loss_cls: 4.4861, reconstruct_loss: 2.7232 lr: 3.12500e-05, loss: 4.3275\n",
      "Epoch [5][85/721], loss_cls: 5.4014, reconstruct_loss: 2.8792 lr: 3.12500e-05, loss: 4.3831\n",
      "Epoch [5][90/721], loss_cls: 5.7881, reconstruct_loss: 1.5258 lr: 3.12500e-05, loss: 4.7341\n",
      "Epoch [5][95/721], loss_cls: 4.7613, reconstruct_loss: 2.0922 lr: 3.12500e-05, loss: 4.6072\n",
      "Epoch [5][100/721], loss_cls: 5.1911, reconstruct_loss: 2.4208 lr: 3.12500e-05, loss: 4.6023\n",
      "Epoch [5][105/721], loss_cls: 4.4868, reconstruct_loss: 1.4514 lr: 3.12500e-05, loss: 4.3602\n",
      "Epoch [5][110/721], loss_cls: 5.5834, reconstruct_loss: 2.8032 lr: 3.12500e-05, loss: 4.3191\n",
      "Epoch [5][115/721], loss_cls: 5.0738, reconstruct_loss: 2.5041 lr: 3.12500e-05, loss: 4.4201\n",
      "Epoch [5][120/721], loss_cls: 5.3659, reconstruct_loss: 1.1615 lr: 3.12500e-05, loss: 4.7563\n",
      "Epoch [5][125/721], loss_cls: 5.2913, reconstruct_loss: 1.7416 lr: 3.12500e-05, loss: 4.0694\n",
      "Epoch [5][130/721], loss_cls: 4.7656, reconstruct_loss: 1.6582 lr: 3.12500e-05, loss: 4.0672\n",
      "Epoch [5][135/721], loss_cls: 4.8194, reconstruct_loss: 2.7167 lr: 3.12500e-05, loss: 4.8944\n",
      "Epoch [5][140/721], loss_cls: 6.3473, reconstruct_loss: 2.465 lr: 3.12500e-05, loss: 4.566\n",
      "Epoch [5][145/721], loss_cls: 4.8661, reconstruct_loss: 1.76 lr: 3.12500e-05, loss: 4.3864\n",
      "Epoch [5][150/721], loss_cls: 5.1282, reconstruct_loss: 1.8946 lr: 3.12500e-05, loss: 4.397\n",
      "Epoch [5][155/721], loss_cls: 5.0537, reconstruct_loss: 1.4582 lr: 3.12500e-05, loss: 4.4835\n",
      "Epoch [5][160/721], loss_cls: 4.5699, reconstruct_loss: 2.8969 lr: 3.12500e-05, loss: 3.9727\n",
      "Epoch [5][165/721], loss_cls: 4.6778, reconstruct_loss: 1.2598 lr: 3.12500e-05, loss: 4.5726\n",
      "Epoch [5][170/721], loss_cls: 5.5137, reconstruct_loss: 2.2409 lr: 3.12500e-05, loss: 4.3911\n",
      "Epoch [5][175/721], loss_cls: 5.6997, reconstruct_loss: 1.9336 lr: 3.12500e-05, loss: 4.4072\n",
      "Epoch [5][180/721], loss_cls: 4.4151, reconstruct_loss: 2.281 lr: 3.12500e-05, loss: 4.2651\n",
      "Epoch [5][185/721], loss_cls: 4.979, reconstruct_loss: 1.4823 lr: 3.12500e-05, loss: 4.3197\n",
      "Epoch [5][190/721], loss_cls: 5.8594, reconstruct_loss: 1.4496 lr: 3.12500e-05, loss: 4.8872\n",
      "Epoch [5][195/721], loss_cls: 5.2902, reconstruct_loss: 1.2676 lr: 3.12500e-05, loss: 4.462\n",
      "Epoch [5][200/721], loss_cls: 5.183, reconstruct_loss: 1.2613 lr: 3.12500e-05, loss: 4.1691\n",
      "Epoch [5][205/721], loss_cls: 5.5157, reconstruct_loss: 2.2652 lr: 3.12500e-05, loss: 4.5993\n",
      "Epoch [5][210/721], loss_cls: 4.4386, reconstruct_loss: 2.4645 lr: 3.12500e-05, loss: 4.3018\n",
      "Epoch [5][215/721], loss_cls: 4.8272, reconstruct_loss: 2.0097 lr: 3.12500e-05, loss: 4.4678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5][220/721], loss_cls: 4.5725, reconstruct_loss: 2.4375 lr: 3.12500e-05, loss: 4.6007\n",
      "Epoch [5][225/721], loss_cls: 4.9231, reconstruct_loss: 0.84579 lr: 3.12500e-05, loss: 4.4483\n",
      "Epoch [5][230/721], loss_cls: 5.2238, reconstruct_loss: 1.5312 lr: 3.12500e-05, loss: 4.671\n",
      "Epoch [5][235/721], loss_cls: 5.3373, reconstruct_loss: 1.7664 lr: 3.12500e-05, loss: 4.4858\n",
      "Epoch [5][240/721], loss_cls: 4.7613, reconstruct_loss: 1.6432 lr: 3.12500e-05, loss: 4.6445\n",
      "Epoch [5][245/721], loss_cls: 5.2738, reconstruct_loss: 1.9339 lr: 3.12500e-05, loss: 4.1184\n",
      "Epoch [5][250/721], loss_cls: 4.4927, reconstruct_loss: 3.1957 lr: 3.12500e-05, loss: 4.0839\n",
      "Epoch [5][255/721], loss_cls: 4.4511, reconstruct_loss: 1.3127 lr: 3.12500e-05, loss: 4.2321\n",
      "Epoch [5][260/721], loss_cls: 5.0431, reconstruct_loss: 1.5481 lr: 3.12500e-05, loss: 4.2565\n",
      "Epoch [5][265/721], loss_cls: 4.5535, reconstruct_loss: 2.3397 lr: 3.12500e-05, loss: 4.1746\n",
      "Epoch [5][270/721], loss_cls: 4.8482, reconstruct_loss: 1.2574 lr: 3.12500e-05, loss: 4.3771\n",
      "Epoch [5][275/721], loss_cls: 5.3519, reconstruct_loss: 1.2934 lr: 3.12500e-05, loss: 4.0772\n",
      "Epoch [5][280/721], loss_cls: 4.0668, reconstruct_loss: 1.617 lr: 3.12500e-05, loss: 4.1149\n",
      "Epoch [5][285/721], loss_cls: 4.9986, reconstruct_loss: 1.5067 lr: 3.12500e-05, loss: 4.1073\n",
      "Epoch [5][290/721], loss_cls: 5.2841, reconstruct_loss: 1.8827 lr: 3.12500e-05, loss: 4.2916\n",
      "Epoch [5][295/721], loss_cls: 5.0699, reconstruct_loss: 2.4842 lr: 3.12500e-05, loss: 4.7644\n",
      "Epoch [5][300/721], loss_cls: 5.1301, reconstruct_loss: 1.5674 lr: 3.12500e-05, loss: 4.3316\n",
      "Epoch [5][305/721], loss_cls: 4.4478, reconstruct_loss: 1.1335 lr: 3.12500e-05, loss: 4.1844\n",
      "Epoch [5][310/721], loss_cls: 4.0421, reconstruct_loss: 2.6328 lr: 3.12500e-05, loss: 4.123\n",
      "Epoch [5][315/721], loss_cls: 4.0326, reconstruct_loss: 2.4947 lr: 3.12500e-05, loss: 4.0076\n",
      "Epoch [5][320/721], loss_cls: 4.1748, reconstruct_loss: 1.4987 lr: 3.12500e-05, loss: 4.1834\n",
      "Epoch [5][325/721], loss_cls: 6.1219, reconstruct_loss: 3.3121 lr: 3.12500e-05, loss: 4.7181\n",
      "Epoch [5][330/721], loss_cls: 5.0734, reconstruct_loss: 1.26 lr: 3.12500e-05, loss: 4.5061\n",
      "Epoch [5][335/721], loss_cls: 5.6457, reconstruct_loss: 2.2804 lr: 3.12500e-05, loss: 4.7762\n",
      "Epoch [5][340/721], loss_cls: 5.4891, reconstruct_loss: 1.7622 lr: 3.12500e-05, loss: 4.488\n",
      "Epoch [5][345/721], loss_cls: 5.515, reconstruct_loss: 1.0785 lr: 3.12500e-05, loss: 4.5507\n",
      "Epoch [5][350/721], loss_cls: 5.5512, reconstruct_loss: 1.4733 lr: 3.12500e-05, loss: 4.5596\n",
      "Epoch [5][355/721], loss_cls: 5.2446, reconstruct_loss: 2.851 lr: 3.12500e-05, loss: 4.6659\n",
      "Epoch [5][360/721], loss_cls: 5.6082, reconstruct_loss: 1.3782 lr: 3.12500e-05, loss: 4.508\n",
      "Epoch [5][365/721], loss_cls: 4.2432, reconstruct_loss: 2.3558 lr: 3.12500e-05, loss: 4.142\n",
      "Epoch [5][370/721], loss_cls: 5.3771, reconstruct_loss: 2.9623 lr: 3.12500e-05, loss: 4.6893\n",
      "Epoch [5][375/721], loss_cls: 4.3637, reconstruct_loss: 1.3449 lr: 3.12500e-05, loss: 4.0989\n",
      "Epoch [5][380/721], loss_cls: 4.3772, reconstruct_loss: 1.0493 lr: 3.12500e-05, loss: 3.9592\n",
      "Epoch [5][385/721], loss_cls: 4.3589, reconstruct_loss: 2.1986 lr: 3.12500e-05, loss: 4.0461\n",
      "Epoch [5][390/721], loss_cls: 5.2253, reconstruct_loss: 1.3551 lr: 3.12500e-05, loss: 4.2588\n",
      "Epoch [5][395/721], loss_cls: 6.2085, reconstruct_loss: 2.0349 lr: 3.12500e-05, loss: 4.7366\n",
      "Epoch [5][400/721], loss_cls: 5.1748, reconstruct_loss: 1.0685 lr: 3.12500e-05, loss: 4.3025\n",
      "Epoch [5][405/721], loss_cls: 5.6737, reconstruct_loss: 2.0338 lr: 3.12500e-05, loss: 4.6021\n",
      "Epoch [5][410/721], loss_cls: 5.1395, reconstruct_loss: 2.22 lr: 3.12500e-05, loss: 4.2683\n",
      "Epoch [5][415/721], loss_cls: 5.6954, reconstruct_loss: 1.7895 lr: 3.12500e-05, loss: 4.5635\n",
      "Epoch [5][420/721], loss_cls: 4.2834, reconstruct_loss: 2.4164 lr: 3.12500e-05, loss: 4.2295\n",
      "Epoch [5][425/721], loss_cls: 5.1687, reconstruct_loss: 2.2901 lr: 3.12500e-05, loss: 4.3647\n",
      "Epoch [5][430/721], loss_cls: 5.9411, reconstruct_loss: 1.2733 lr: 3.12500e-05, loss: 4.5068\n",
      "Epoch [5][435/721], loss_cls: 4.889, reconstruct_loss: 2.4924 lr: 3.12500e-05, loss: 4.2662\n",
      "Epoch [5][440/721], loss_cls: 6.4546, reconstruct_loss: 1.8982 lr: 3.12500e-05, loss: 4.386\n",
      "Epoch [5][445/721], loss_cls: 5.5691, reconstruct_loss: 2.0855 lr: 3.12500e-05, loss: 4.3914\n",
      "Epoch [5][450/721], loss_cls: 4.9337, reconstruct_loss: 2.096 lr: 3.12500e-05, loss: 4.3323\n",
      "Epoch [5][455/721], loss_cls: 4.9788, reconstruct_loss: 1.2728 lr: 3.12500e-05, loss: 4.4019\n",
      "Epoch [5][460/721], loss_cls: 4.6701, reconstruct_loss: 2.8361 lr: 3.12500e-05, loss: 4.135\n",
      "Epoch [5][465/721], loss_cls: 5.4001, reconstruct_loss: 1.413 lr: 3.12500e-05, loss: 4.2227\n",
      "Epoch [5][470/721], loss_cls: 5.0082, reconstruct_loss: 1.7843 lr: 3.12500e-05, loss: 4.0828\n",
      "Epoch [5][475/721], loss_cls: 5.2812, reconstruct_loss: 2.3784 lr: 3.12500e-05, loss: 4.4427\n",
      "Epoch [5][480/721], loss_cls: 5.1558, reconstruct_loss: 2.259 lr: 3.12500e-05, loss: 4.799\n",
      "Epoch [5][485/721], loss_cls: 5.0419, reconstruct_loss: 1.5027 lr: 3.12500e-05, loss: 4.6276\n",
      "Epoch [5][490/721], loss_cls: 4.9623, reconstruct_loss: 1.0739 lr: 3.12500e-05, loss: 4.2017\n",
      "Epoch [5][495/721], loss_cls: 5.1394, reconstruct_loss: 1.8513 lr: 3.12500e-05, loss: 4.4127\n",
      "Epoch [5][500/721], loss_cls: 4.8304, reconstruct_loss: 1.0889 lr: 3.12500e-05, loss: 4.1862\n",
      "Epoch [5][505/721], loss_cls: 5.6072, reconstruct_loss: 1.6187 lr: 3.12500e-05, loss: 4.2966\n",
      "Epoch [5][510/721], loss_cls: 5.2386, reconstruct_loss: 1.2518 lr: 3.12500e-05, loss: 4.3333\n",
      "Epoch [5][515/721], loss_cls: 4.8209, reconstruct_loss: 2.154 lr: 3.12500e-05, loss: 4.2389\n",
      "Epoch [5][520/721], loss_cls: 4.1668, reconstruct_loss: 1.0038 lr: 3.12500e-05, loss: 4.4242\n",
      "Epoch [5][525/721], loss_cls: 4.727, reconstruct_loss: 1.6708 lr: 3.12500e-05, loss: 4.0332\n",
      "Epoch [5][530/721], loss_cls: 5.5531, reconstruct_loss: 1.4284 lr: 3.12500e-05, loss: 4.4152\n",
      "Epoch [5][535/721], loss_cls: 5.4463, reconstruct_loss: 1.402 lr: 3.12500e-05, loss: 4.3537\n",
      "Epoch [5][540/721], loss_cls: 5.468, reconstruct_loss: 1.5317 lr: 3.12500e-05, loss: 4.1086\n",
      "Epoch [5][545/721], loss_cls: 4.3195, reconstruct_loss: 1.7354 lr: 3.12500e-05, loss: 4.3608\n",
      "Epoch [5][550/721], loss_cls: 4.6862, reconstruct_loss: 2.6839 lr: 3.12500e-05, loss: 4.2616\n",
      "Epoch [5][555/721], loss_cls: 5.2318, reconstruct_loss: 2.3732 lr: 3.12500e-05, loss: 4.3625\n",
      "Epoch [5][560/721], loss_cls: 6.0614, reconstruct_loss: 1.2704 lr: 3.12500e-05, loss: 4.2663\n",
      "Epoch [5][565/721], loss_cls: 5.6604, reconstruct_loss: 1.3783 lr: 3.12500e-05, loss: 4.6001\n",
      "Epoch [5][570/721], loss_cls: 5.2421, reconstruct_loss: 1.3602 lr: 3.12500e-05, loss: 4.4886\n",
      "Epoch [5][575/721], loss_cls: 6.0495, reconstruct_loss: 1.0392 lr: 3.12500e-05, loss: 4.5415\n",
      "Epoch [5][580/721], loss_cls: 5.2909, reconstruct_loss: 1.7761 lr: 3.12500e-05, loss: 4.2441\n",
      "Epoch [5][585/721], loss_cls: 5.0823, reconstruct_loss: 1.1955 lr: 3.12500e-05, loss: 4.0285\n",
      "Epoch [5][590/721], loss_cls: 5.2663, reconstruct_loss: 2.1316 lr: 3.12500e-05, loss: 4.4747\n",
      "Epoch [5][595/721], loss_cls: 4.3668, reconstruct_loss: 2.1052 lr: 3.12500e-05, loss: 4.0706\n",
      "Epoch [5][600/721], loss_cls: 4.5012, reconstruct_loss: 1.1661 lr: 3.12500e-05, loss: 4.4989\n",
      "Epoch [5][605/721], loss_cls: 4.5693, reconstruct_loss: 1.9397 lr: 3.12500e-05, loss: 4.4919\n",
      "Epoch [5][610/721], loss_cls: 4.7552, reconstruct_loss: 1.5054 lr: 3.12500e-05, loss: 4.1716\n",
      "Epoch [5][615/721], loss_cls: 4.8452, reconstruct_loss: 1.3827 lr: 3.12500e-05, loss: 3.8571\n",
      "Epoch [5][620/721], loss_cls: 5.1706, reconstruct_loss: 1.2304 lr: 3.12500e-05, loss: 4.2349\n",
      "Epoch [5][625/721], loss_cls: 5.2517, reconstruct_loss: 1.8988 lr: 3.12500e-05, loss: 4.0672\n",
      "Epoch [5][630/721], loss_cls: 4.289, reconstruct_loss: 2.1613 lr: 3.12500e-05, loss: 4.054\n",
      "Epoch [5][635/721], loss_cls: 6.0531, reconstruct_loss: 1.1271 lr: 3.12500e-05, loss: 4.183\n",
      "Epoch [5][640/721], loss_cls: 4.0984, reconstruct_loss: 2.257 lr: 3.12500e-05, loss: 4.276\n",
      "Epoch [5][645/721], loss_cls: 5.1153, reconstruct_loss: 1.266 lr: 3.12500e-05, loss: 4.1536\n",
      "Epoch [5][650/721], loss_cls: 4.7812, reconstruct_loss: 2.2867 lr: 3.12500e-05, loss: 4.2929\n",
      "Epoch [5][655/721], loss_cls: 5.4237, reconstruct_loss: 2.0243 lr: 3.12500e-05, loss: 4.7113\n",
      "Epoch [5][660/721], loss_cls: 4.691, reconstruct_loss: 2.4541 lr: 3.12500e-05, loss: 4.2855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5][665/721], loss_cls: 5.0136, reconstruct_loss: 1.1936 lr: 3.12500e-05, loss: 4.51\n",
      "Epoch [5][670/721], loss_cls: 5.0649, reconstruct_loss: 2.0647 lr: 3.12500e-05, loss: 4.1457\n",
      "Epoch [5][675/721], loss_cls: 4.8582, reconstruct_loss: 1.2851 lr: 3.12500e-05, loss: 4.0935\n",
      "Epoch [5][680/721], loss_cls: 5.0291, reconstruct_loss: 1.7455 lr: 3.12500e-05, loss: 4.4321\n",
      "Epoch [5][685/721], loss_cls: 4.5276, reconstruct_loss: 1.16 lr: 3.12500e-05, loss: 4.0952\n",
      "Epoch [5][690/721], loss_cls: 4.5706, reconstruct_loss: 3.0713 lr: 3.12500e-05, loss: 4.3967\n",
      "Epoch [5][695/721], loss_cls: 5.6889, reconstruct_loss: 2.2758 lr: 3.12500e-05, loss: 4.3168\n",
      "Epoch [5][700/721], loss_cls: 4.5588, reconstruct_loss: 1.9038 lr: 3.12500e-05, loss: 4.2532\n",
      "Epoch [5][705/721], loss_cls: 5.1533, reconstruct_loss: 1.1397 lr: 3.12500e-05, loss: 4.43\n",
      "Epoch [5][710/721], loss_cls: 4.6373, reconstruct_loss: 1.057 lr: 3.12500e-05, loss: 4.0773\n",
      "Epoch [5][715/721], loss_cls: 4.9935, reconstruct_loss: 1.8231 lr: 3.12500e-05, loss: 4.4793\n",
      "Epoch [5][720/721], loss_cls: 5.1197, reconstruct_loss: 1.4214 lr: 3.12500e-05, loss: 4.3271\n",
      "Evaluating top_k_accuracy...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Turn off  gradients for reporting\u001b[39;00m\n\u001b[1;32m    125\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 127\u001b[0m avg_vloss, top1_acc, top5_acc \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop1_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop1_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, top5_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop5_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_vloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Track best performance, and save the model's state\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 95\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m vimages \u001b[38;5;241m=\u001b[39m vimages\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m+\u001b[39m vimages\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m     93\u001b[0m vtargets \u001b[38;5;241m=\u001b[39m vtargets\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, )\n\u001b[0;32m---> 95\u001b[0m cls_score, reconstructed \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Get losses\u001b[39;00m\n\u001b[1;32m     98\u001b[0m loss_cls_score \u001b[38;5;241m=\u001b[39m loss_cls(cls_score, vtargets)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/csn-sign-autoencoder/cls_autoencoder.py:13\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 13\u001b[0m     reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     cls_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_head(x[\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cls_score, reconstructed\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/csn-sign-autoencoder/reconstruction_head.py:111\u001b[0m, in \u001b[0;36mRecontructionHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder2(out)\n\u001b[1;32m    110\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat((out,x2), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat((out,x1), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    113\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder4(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/csn-sign-autoencoder/reconstruction_head.py:65\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m     64\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[0;32m---> 65\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_planes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplanes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     68\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataloader/lib/python3.8/site-packages/torch/nn/functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2419\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup wandb\n",
    "wandb.watch(model, log_freq=10)\n",
    "\n",
    "def top_k_accuracy(scores, labels, topk=(1, )):\n",
    "    \"\"\"Calculate top k accuracy score.\n",
    "    Args:\n",
    "        scores (list[np.ndarray]): Prediction scores for each class.\n",
    "        labels (list[int]): Ground truth labels.\n",
    "        topk (tuple[int]): K value for top_k_accuracy. Default: (1, ).\n",
    "    Returns:\n",
    "        list[float]: Top k accuracy score for each k.\n",
    "    \"\"\"\n",
    "    res = np.zeros(len(topk))\n",
    "    labels = np.array(labels)[:, np.newaxis]\n",
    "    for i, k in enumerate(topk):\n",
    "        max_k_preds = np.argsort(scores, axis=1)[:, -k:][:, ::-1]\n",
    "        match_array = np.logical_or.reduce(max_k_preds == labels, axis=1)\n",
    "        topk_acc_score = match_array.sum() / match_array.shape[0]\n",
    "        res[i] = topk_acc_score\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, interval=5):\n",
    "    \"\"\"Run one epoch for training.\n",
    "    Args:\n",
    "        epoch_index (int): Current epoch.\n",
    "        interval (int): Frequency at which to print logs.\n",
    "    Returns:\n",
    "        last_loss (float): Loss value for the last batch.\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, x in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        images, targets = x['imgs'].to(device), x['label'].to(device)\n",
    "        images = images.reshape((-1, ) + images.shape[2:])\n",
    "        targets = targets.reshape(-1, )\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        cls_score, reconstructed = model(images)\n",
    "\n",
    "        # Get losses\n",
    "        loss_cls_score = loss_cls(cls_score, targets)\n",
    "        loss_reconstruct_score = loss_reconstruct(reconstructed, images)\n",
    "        loss = 0.8 * loss_cls_score + 0.2 * loss_reconstruct_score\n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), max_norm=40, norm_type=2.0)\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % interval == interval-1:\n",
    "            last_loss = running_loss / interval  # loss per batch\n",
    "            print(\n",
    "                f'Epoch [{epoch_index}][{i+1}/{len(train_loader)}], loss_cls: {loss_cls_score.item():.5}, reconstruct_loss: {loss_reconstruct_score.item():.5} lr: {scheduler.get_last_lr()[0]:.5e}, loss: {last_loss:.5}')\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss, scheduler.get_last_lr()[0]\n",
    "\n",
    "\n",
    "def validate():\n",
    "    \"\"\"Run one epoch for validation.\n",
    "    Returns:\n",
    "        avg_vloss (float): Validation loss value for the last batch.\n",
    "        top1_acc (float): Top-1 accuracy in decimal.\n",
    "        top5_acc (float): Top-5 accuracy in decimal.\n",
    "    \"\"\"\n",
    "    running_vloss = 0.0\n",
    "    running_vacc = np.zeros(2)\n",
    "\n",
    "    print('Evaluating top_k_accuracy...')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, x in enumerate(test_loader):\n",
    "            vimages, vtargets = x['imgs'].to(device), x['label'].to(device)\n",
    "            vimages = vimages.reshape((-1, ) + vimages.shape[2:])\n",
    "            vtargets = vtargets.reshape(-1, )\n",
    "            \n",
    "            cls_score, reconstructed = model(vimages)\n",
    "\n",
    "            # Get losses\n",
    "            loss_cls_score = loss_cls(cls_score, vtargets)\n",
    "            loss_reconstruct_score = loss_reconstruct(reconstructed, vimages)\n",
    "            vloss = 0.8 * loss_cls_score + 0.2 * loss_reconstruct_score\n",
    "            \n",
    "            running_vloss += vloss\n",
    "\n",
    "            running_vacc += top_k_accuracy(cls_score.detach().cpu().numpy(),\n",
    "                                           vtargets.detach().cpu().numpy(), topk=(1, 5))\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "\n",
    "    acc = running_vacc/len(test_loader)\n",
    "    top1_acc = acc[0].item()\n",
    "    top5_acc = acc[1].item()\n",
    "\n",
    "    return (avg_vloss, top1_acc, top5_acc)\n",
    "\n",
    "\n",
    "# Train Loop\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Turn on gradient tracking and do a forward pass\n",
    "    model.train(True)\n",
    "    avg_loss, learning_rate = train_one_epoch(epoch+1)\n",
    "\n",
    "    # Turn off  gradients for reporting\n",
    "    model.train(False)\n",
    "\n",
    "    avg_vloss, top1_acc, top5_acc = validate()\n",
    "\n",
    "    print(\n",
    "        f'top1_acc: {top1_acc:.4}, top5_acc: {top5_acc:.4}, train_loss: {avg_loss:.5}, val_loss: {avg_vloss:.5}')\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = work_dir + f'epoch_{epoch+1}.pth'\n",
    "        print(f'Saving checkpoint at {epoch+1} epochs...')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "     # Adjust learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Track wandb\n",
    "    wandb.log({'train/loss': avg_loss,\n",
    "               'train/learning_rate': learning_rate,\n",
    "               'val/loss': avg_vloss,\n",
    "               'val/top1_accuracy': top1_acc,\n",
    "               'val/top5_accuracy': top5_acc})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataloader",
   "language": "python",
   "name": "dataloader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
