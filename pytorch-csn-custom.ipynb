{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07d501a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1415e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorchvideo.models.head import create_res_basic_head\n",
    "from pytorchvideo.models.resnet import Net, create_bottleneck_block, create_res_stage\n",
    "from pytorchvideo.models.stem import create_res_basic_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d1585",
   "metadata": {},
   "source": [
    "## Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d696d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input clip configs.\n",
    "input_channel: int = 3\n",
    "# Model configs.\n",
    "model_depth: int = 50\n",
    "model_num_class: int = 400\n",
    "dropout_rate: float = 0.5\n",
    "# Normalization configs.\n",
    "norm: Callable = nn.BatchNorm3d\n",
    "# Activation configs.\n",
    "activation: Callable = nn.ReLU\n",
    "# Stem configs.\n",
    "stem_dim_out: int = 64\n",
    "stem_conv_kernel_size: Tuple[int] = (3, 7, 7)\n",
    "stem_conv_stride: Tuple[int] = (1, 2, 2)\n",
    "stem_pool: Callable = None\n",
    "stem_pool_kernel_size: Tuple[int] = (1, 3, 3)\n",
    "stem_pool_stride: Tuple[int] = (1, 2, 2)\n",
    "# Stage configs.\n",
    "stage_conv_a_kernel_size: Tuple[int] = (1, 1, 1)\n",
    "stage_conv_b_kernel_size: Tuple[int] = (3, 3, 3)\n",
    "stage_conv_b_width_per_group: int = 1\n",
    "stage_spatial_stride: Tuple[int] = (1, 2, 2, 2)\n",
    "stage_temporal_stride: Tuple[int] = (1, 2, 2, 2)\n",
    "bottleneck: Callable = create_bottleneck_block\n",
    "bottleneck_ratio: int = 4\n",
    "# Head configs.\n",
    "head_pool: Callable = nn.AvgPool3d\n",
    "head_pool_kernel_size: Tuple[int] = (1, 7, 7)\n",
    "head_output_size: Tuple[int] = (1, 1, 1)\n",
    "head_activation: Callable = None\n",
    "head_output_with_global_average: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cf2615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of blocks for different stages given the model depth.\n",
    "_MODEL_STAGE_DEPTH = {50: (3, 4, 6, 3), 101: (3, 4, 23, 3), 152: (3, 8, 36, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "648fe342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a model depth, get the number of blocks for each stage.\n",
    "assert (\n",
    "    model_depth in _MODEL_STAGE_DEPTH.keys()\n",
    "), f\"{model_depth} is not in {_MODEL_STAGE_DEPTH.keys()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7ef9bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 6, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_depths = _MODEL_STAGE_DEPTH[model_depth]\n",
    "stage_depths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1eced9",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56c26fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40eaf9",
   "metadata": {},
   "source": [
    "## Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f63906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetBasicStem(\n",
       "  (conv): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
       "  (norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = create_res_basic_stem(\n",
    "    in_channels=input_channel,\n",
    "    out_channels=stem_dim_out,\n",
    "    conv_kernel_size=stem_conv_kernel_size,\n",
    "    conv_stride=stem_conv_stride,\n",
    "    conv_padding=[size // 2 for size in stem_conv_kernel_size],\n",
    "    pool=stem_pool,\n",
    "    pool_kernel_size=stem_pool_kernel_size,\n",
    "    pool_stride=stem_pool_stride,\n",
    "    pool_padding=[size // 2 for size in stem_pool_kernel_size],\n",
    "    norm=norm,\n",
    "    activation=activation,\n",
    ")\n",
    "stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a1161",
   "metadata": {},
   "source": [
    "### Check stem size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5f2efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.weight: torch.Size([64, 3, 3, 7, 7])\n",
      "norm.weight: torch.Size([64])\n",
      "norm.bias: torch.Size([64])\n",
      "norm.running_mean: torch.Size([64])\n",
      "norm.running_var: torch.Size([64])\n",
      "norm.num_batches_tracked: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "module = stem\n",
    "\n",
    "torch.save(module.state_dict(), 'module.pth')\n",
    "module_checkpoint = torch.load('module.pth')\n",
    "\n",
    "checkpoint_iter = iter(module_checkpoint.keys())\n",
    "for i in checkpoint_iter:\n",
    "    print(f'{i}: {module_checkpoint[i].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f42b62",
   "metadata": {},
   "source": [
    "### Add stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2f1cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f41a836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_dim_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fe2ed131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_dim_in = stem_dim_out\n",
    "stage_dim_out = stage_dim_in * 4\n",
    "stage_dim_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "48a0f733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_dim_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad0d91",
   "metadata": {},
   "source": [
    "## Create each stage for CSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a074c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(stage_depths)):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72b48b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd5abe05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "90a03241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_dim_inner = stage_dim_out // bottleneck_ratio\n",
    "stage_dim_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "126cc21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 6, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6bfd6000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth = stage_depths[idx]\n",
    "depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2a5c6d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_conv_b_stride = (\n",
    "        stage_temporal_stride[idx],\n",
    "        stage_spatial_stride[idx],\n",
    "        stage_spatial_stride[idx],\n",
    "    )\n",
    "\n",
    "stage_conv_b_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "da40eeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResStage(\n",
       "  (res_blocks): ModuleList(\n",
       "    (0): ResBlock(\n",
       "      (branch1_conv): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      (branch1_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (branch2): BottleneckBlock(\n",
       "        (conv_a): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act_a): ReLU()\n",
       "        (conv_b): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64, bias=False)\n",
       "        (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act_b): ReLU()\n",
       "        (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (branch2): BottleneckBlock(\n",
       "        (conv_a): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act_a): ReLU()\n",
       "        (conv_b): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64, bias=False)\n",
       "        (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act_b): ReLU()\n",
       "        (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (branch2): BottleneckBlock(\n",
       "        (conv_a): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act_a): ReLU()\n",
       "        (conv_b): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=64, bias=False)\n",
       "        (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act_b): ReLU()\n",
       "        (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "        (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_dim_in = 64\n",
    "stage_dim_out = 256\n",
    "stage_dim_inner = 64\n",
    "stage = create_res_stage(\n",
    "            depth=depth,\n",
    "            dim_in=stage_dim_in,\n",
    "            dim_inner=stage_dim_inner,\n",
    "            dim_out=stage_dim_out,\n",
    "            bottleneck=bottleneck,\n",
    "            conv_a_kernel_size=stage_conv_a_kernel_size,\n",
    "            conv_a_stride=(1, 1, 1),\n",
    "            conv_a_padding=[size // 2 for size in stage_conv_a_kernel_size],\n",
    "            conv_b_kernel_size=stage_conv_b_kernel_size,\n",
    "            conv_b_stride=stage_conv_b_stride,\n",
    "            conv_b_padding=[size // 2 for size in stage_conv_b_kernel_size],\n",
    "            conv_b_num_groups=(stage_dim_inner // stage_conv_b_width_per_group),\n",
    "            conv_b_dilation=(1, 1, 1),\n",
    "            norm=norm,\n",
    "            activation=activation,\n",
    "        )\n",
    "stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca04fd",
   "metadata": {},
   "source": [
    "## Check stage 0 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ca3f70a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_blocks.0.branch1_conv.weight: torch.Size([256, 64, 1, 1, 1])\n",
      "res_blocks.0.branch1_norm.weight: torch.Size([256])\n",
      "res_blocks.0.branch1_norm.bias: torch.Size([256])\n",
      "res_blocks.0.branch1_norm.running_mean: torch.Size([256])\n",
      "res_blocks.0.branch1_norm.running_var: torch.Size([256])\n",
      "res_blocks.0.branch1_norm.num_batches_tracked: torch.Size([])\n",
      "res_blocks.0.branch2.conv_a.weight: torch.Size([64, 64, 1, 1, 1])\n",
      "res_blocks.0.branch2.norm_a.weight: torch.Size([64])\n",
      "res_blocks.0.branch2.norm_a.bias: torch.Size([64])\n",
      "res_blocks.0.branch2.norm_a.running_mean: torch.Size([64])\n",
      "res_blocks.0.branch2.norm_a.running_var: torch.Size([64])\n",
      "res_blocks.0.branch2.norm_a.num_batches_tracked: torch.Size([])\n",
      "res_blocks.0.branch2.conv_b.weight: torch.Size([64, 1, 3, 3, 3])\n",
      "res_blocks.0.branch2.norm_b.weight: torch.Size([64])\n",
      "res_blocks.0.branch2.norm_b.bias: torch.Size([64])\n",
      "res_blocks.0.branch2.norm_b.running_mean: torch.Size([64])\n",
      "res_blocks.0.branch2.norm_b.running_var: torch.Size([64])\n",
      "res_blocks.0.branch2.norm_b.num_batches_tracked: torch.Size([])\n",
      "res_blocks.0.branch2.conv_c.weight: torch.Size([256, 64, 1, 1, 1])\n",
      "res_blocks.0.branch2.norm_c.weight: torch.Size([256])\n",
      "res_blocks.0.branch2.norm_c.bias: torch.Size([256])\n",
      "res_blocks.0.branch2.norm_c.running_mean: torch.Size([256])\n",
      "res_blocks.0.branch2.norm_c.running_var: torch.Size([256])\n",
      "res_blocks.0.branch2.norm_c.num_batches_tracked: torch.Size([])\n",
      "res_blocks.1.branch2.conv_a.weight: torch.Size([64, 256, 1, 1, 1])\n",
      "res_blocks.1.branch2.norm_a.weight: torch.Size([64])\n",
      "res_blocks.1.branch2.norm_a.bias: torch.Size([64])\n",
      "res_blocks.1.branch2.norm_a.running_mean: torch.Size([64])\n",
      "res_blocks.1.branch2.norm_a.running_var: torch.Size([64])\n",
      "res_blocks.1.branch2.norm_a.num_batches_tracked: torch.Size([])\n",
      "res_blocks.1.branch2.conv_b.weight: torch.Size([64, 1, 3, 3, 3])\n",
      "res_blocks.1.branch2.norm_b.weight: torch.Size([64])\n",
      "res_blocks.1.branch2.norm_b.bias: torch.Size([64])\n",
      "res_blocks.1.branch2.norm_b.running_mean: torch.Size([64])\n",
      "res_blocks.1.branch2.norm_b.running_var: torch.Size([64])\n",
      "res_blocks.1.branch2.norm_b.num_batches_tracked: torch.Size([])\n",
      "res_blocks.1.branch2.conv_c.weight: torch.Size([256, 64, 1, 1, 1])\n",
      "res_blocks.1.branch2.norm_c.weight: torch.Size([256])\n",
      "res_blocks.1.branch2.norm_c.bias: torch.Size([256])\n",
      "res_blocks.1.branch2.norm_c.running_mean: torch.Size([256])\n",
      "res_blocks.1.branch2.norm_c.running_var: torch.Size([256])\n",
      "res_blocks.1.branch2.norm_c.num_batches_tracked: torch.Size([])\n",
      "res_blocks.2.branch2.conv_a.weight: torch.Size([64, 256, 1, 1, 1])\n",
      "res_blocks.2.branch2.norm_a.weight: torch.Size([64])\n",
      "res_blocks.2.branch2.norm_a.bias: torch.Size([64])\n",
      "res_blocks.2.branch2.norm_a.running_mean: torch.Size([64])\n",
      "res_blocks.2.branch2.norm_a.running_var: torch.Size([64])\n",
      "res_blocks.2.branch2.norm_a.num_batches_tracked: torch.Size([])\n",
      "res_blocks.2.branch2.conv_b.weight: torch.Size([64, 1, 3, 3, 3])\n",
      "res_blocks.2.branch2.norm_b.weight: torch.Size([64])\n",
      "res_blocks.2.branch2.norm_b.bias: torch.Size([64])\n",
      "res_blocks.2.branch2.norm_b.running_mean: torch.Size([64])\n",
      "res_blocks.2.branch2.norm_b.running_var: torch.Size([64])\n",
      "res_blocks.2.branch2.norm_b.num_batches_tracked: torch.Size([])\n",
      "res_blocks.2.branch2.conv_c.weight: torch.Size([256, 64, 1, 1, 1])\n",
      "res_blocks.2.branch2.norm_c.weight: torch.Size([256])\n",
      "res_blocks.2.branch2.norm_c.bias: torch.Size([256])\n",
      "res_blocks.2.branch2.norm_c.running_mean: torch.Size([256])\n",
      "res_blocks.2.branch2.norm_c.running_var: torch.Size([256])\n",
      "res_blocks.2.branch2.norm_c.num_batches_tracked: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "module = stage\n",
    "\n",
    "torch.save(module.state_dict(), 'module.pth')\n",
    "module_checkpoint = torch.load('module.pth')\n",
    "\n",
    "checkpoint_iter = iter(module_checkpoint.keys())\n",
    "for i in checkpoint_iter:\n",
    "    print(f'{i}: {module_checkpoint[i].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4321c6",
   "metadata": {},
   "source": [
    "Need to remove branch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6a600c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_res_stage(\n",
    "    *,\n",
    "    # Stage configs.\n",
    "    depth: int,\n",
    "    # Bottleneck Block configs.\n",
    "    dim_in: int,\n",
    "    dim_inner: int,\n",
    "    dim_out: int,\n",
    "    bottleneck: Callable,\n",
    "    # Conv configs.\n",
    "    conv_a_kernel_size = (3, 1, 1),\n",
    "    conv_a_stride: Tuple[int] = (2, 1, 1),\n",
    "    conv_a_padding = (1, 0, 0),\n",
    "    conv_a: Callable = nn.Conv3d,\n",
    "    conv_b_kernel_size: Tuple[int] = (1, 3, 3),\n",
    "    conv_b_stride: Tuple[int] = (1, 2, 2),\n",
    "    conv_b_padding: Tuple[int] = (0, 1, 1),\n",
    "    conv_b_num_groups: int = 1,\n",
    "    conv_b_dilation: Tuple[int] = (1, 1, 1),\n",
    "    conv_b: Callable = nn.Conv3d,\n",
    "    conv_c: Callable = nn.Conv3d,\n",
    "    # Norm configs.\n",
    "    norm: Callable = nn.BatchNorm3d,\n",
    "    norm_eps: float = 1e-5,\n",
    "    norm_momentum: float = 0.1,\n",
    "    # Activation configs.\n",
    "    activation: Callable = nn.ReLU,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create Residual Stage, which composes sequential blocks that make up a ResNet. These\n",
    "    blocks could be, for example, Residual blocks, Non-Local layers, or\n",
    "    Squeeze-Excitation layers.\n",
    "\n",
    "    ::\n",
    "\n",
    "\n",
    "                                        Input\n",
    "                                           ↓\n",
    "                                       ResBlock\n",
    "                                           ↓\n",
    "                                           .\n",
    "                                           .\n",
    "                                           .\n",
    "                                           ↓\n",
    "                                       ResBlock\n",
    "\n",
    "    Normalization examples include: BatchNorm3d and None (no normalization).\n",
    "    Activation examples include: ReLU, Softmax, Sigmoid, and None (no activation).\n",
    "    Bottleneck examples include: create_bottleneck_block.\n",
    "\n",
    "    Args:\n",
    "        depth (init): number of blocks to create.\n",
    "\n",
    "        dim_in (int): input channel size to the bottleneck block.\n",
    "        dim_inner (int): intermediate channel size of the bottleneck.\n",
    "        dim_out (int): output channel size of the bottleneck.\n",
    "        bottleneck (callable): a callable that constructs bottleneck block layer.\n",
    "            Examples include: create_bottleneck_block.\n",
    "\n",
    "        conv_a_kernel_size (tuple or list of tuple): convolutional kernel size(s)\n",
    "            for conv_a. If conv_a_kernel_size is a tuple, use it for all blocks in\n",
    "            the stage. If conv_a_kernel_size is a list of tuple, the kernel sizes\n",
    "            will be repeated until having same length of depth in the stage. For\n",
    "            example, for conv_a_kernel_size = [(3, 1, 1), (1, 1, 1)], the kernel\n",
    "            size for the first 6 blocks would be [(3, 1, 1), (1, 1, 1), (3, 1, 1),\n",
    "            (1, 1, 1), (3, 1, 1)].\n",
    "        conv_a_stride (tuple): convolutional stride size(s) for conv_a.\n",
    "        conv_a_padding (tuple or list of tuple): convolutional padding(s) for\n",
    "            conv_a. If conv_a_padding is a tuple, use it for all blocks in\n",
    "            the stage. If conv_a_padding is a list of tuple, the padding sizes\n",
    "            will be repeated until having same length of depth in the stage.\n",
    "        conv_a (callable): a callable that constructs the conv_a conv layer, examples\n",
    "            include nn.Conv3d, OctaveConv, etc\n",
    "        conv_b_kernel_size (tuple): convolutional kernel size(s) for conv_b.\n",
    "        conv_b_stride (tuple): convolutional stride size(s) for conv_b.\n",
    "        conv_b_padding (tuple): convolutional padding(s) for conv_b.\n",
    "        conv_b_num_groups (int): number of groups for groupwise convolution for\n",
    "            conv_b.\n",
    "        conv_b_dilation (tuple): dilation for 3D convolution for conv_b.\n",
    "        conv_b (callable): a callable that constructs the conv_b conv layer, examples\n",
    "            include nn.Conv3d, OctaveConv, etc\n",
    "        conv_c (callable): a callable that constructs the conv_c conv layer, examples\n",
    "            include nn.Conv3d, OctaveConv, etc\n",
    "\n",
    "        norm (callable): a callable that constructs normalization layer. Examples\n",
    "            include nn.BatchNorm3d, and None (not performing normalization).\n",
    "        norm_eps (float): normalization epsilon.\n",
    "        norm_momentum (float): normalization momentum.\n",
    "\n",
    "        activation (callable): a callable that constructs activation layer. Examples\n",
    "            include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None (not performing\n",
    "            activation).\n",
    "\n",
    "    Returns:\n",
    "        (nn.Module): resnet basic stage layer.\n",
    "    \"\"\"\n",
    "    res_blocks = []\n",
    "    if isinstance(conv_a_kernel_size[0], int):\n",
    "        conv_a_kernel_size = [conv_a_kernel_size]\n",
    "    if isinstance(conv_a_padding[0], int):\n",
    "        conv_a_padding = [conv_a_padding]\n",
    "    # Repeat conv_a kernels until having same length of depth in the stage.\n",
    "    conv_a_kernel_size = (conv_a_kernel_size * depth)[:depth]\n",
    "    conv_a_padding = (conv_a_padding * depth)[:depth]\n",
    "\n",
    "    for ind in range(depth):\n",
    "        block = create_res_block(\n",
    "            dim_in=dim_in if ind == 0 else dim_out,\n",
    "            dim_inner=dim_inner,\n",
    "            dim_out=dim_out,\n",
    "            bottleneck=bottleneck,\n",
    "            conv_a_kernel_size=conv_a_kernel_size[ind],\n",
    "            conv_a_stride=conv_a_stride if ind == 0 else (1, 1, 1),\n",
    "            conv_a_padding=conv_a_padding[ind],\n",
    "            conv_a=conv_a,\n",
    "            conv_b_kernel_size=conv_b_kernel_size,\n",
    "            conv_b_stride=conv_b_stride if ind == 0 else (1, 1, 1),\n",
    "            conv_b_padding=conv_b_padding,\n",
    "            conv_b_num_groups=conv_b_num_groups,\n",
    "            conv_b_dilation=conv_b_dilation,\n",
    "            conv_b=conv_b,\n",
    "            conv_c=conv_c,\n",
    "            norm=norm,\n",
    "            norm_eps=norm_eps,\n",
    "            norm_momentum=norm_momentum,\n",
    "            activation_bottleneck=activation,\n",
    "            activation_block=activation,\n",
    "        )\n",
    "        res_blocks.append(block)\n",
    "    return ResStage(res_blocks=nn.ModuleList(res_blocks))\n",
    "\n",
    "\n",
    "\n",
    "# Number of blocks for different stages given the model depth.\n",
    "_MODEL_STAGE_DEPTH = {50: (3, 4, 6, 3), 101: (3, 4, 23, 3), 152: (3, 8, 36, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3b9f80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage configs.\n",
    "depth: int = 3\n",
    "# Bottleneck Block configs.\n",
    "dim_in: int = 64\n",
    "dim_inner: int = 64\n",
    "dim_out: int = 256\n",
    "bottleneck: Callable\n",
    "# Conv configs.\n",
    "conv_a_kernel_size = (3, 1, 1)\n",
    "conv_a_stride: Tuple[int] = (2, 1, 1)\n",
    "conv_a_padding = (1, 0, 0)\n",
    "conv_a: Callable = nn.Conv3d,\n",
    "conv_b_kernel_size: Tuple[int] = (1, 3, 3)\n",
    "conv_b_stride: Tuple[int] = (1, 2, 2)\n",
    "conv_b_padding: Tuple[int] = (0, 1, 1)\n",
    "conv_b_num_groups: int = 1\n",
    "conv_b_dilation: Tuple[int] = (1, 1, 1)\n",
    "conv_b: Callable = nn.Conv3d\n",
    "conv_c: Callable = nn.Conv3d\n",
    "# Norm configs.\n",
    "norm: Callable = nn.BatchNorm3d\n",
    "norm_eps: float = 1e-5\n",
    "norm_momentum: float = 0.1\n",
    "# Activation configs.\n",
    "activation: Callable = nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2debee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_blocks = []\n",
    "if isinstance(conv_a_kernel_size[0], int):\n",
    "    conv_a_kernel_size = [conv_a_kernel_size]\n",
    "if isinstance(conv_a_padding[0], int):\n",
    "    conv_a_padding = [conv_a_padding]\n",
    "# Repeat conv_a kernels until having same length of depth in the stage.\n",
    "conv_a_kernel_size = (conv_a_kernel_size * depth)[:depth]\n",
    "conv_a_padding = (conv_a_padding * depth)[:depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "159b2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_res_block(\n",
    "    *,\n",
    "    # Bottleneck Block configs.\n",
    "    dim_in: int,\n",
    "    dim_inner: int,\n",
    "    dim_out: int,\n",
    "    bottleneck: Callable,\n",
    "    use_shortcut: bool = False,\n",
    "    branch_fusion: Callable = lambda x, y: x + y,\n",
    "    # Conv configs.\n",
    "    conv_a_kernel_size: Tuple[int] = (3, 1, 1),\n",
    "    conv_a_stride: Tuple[int] = (2, 1, 1),\n",
    "    conv_a_padding: Tuple[int] = (1, 0, 0),\n",
    "    conv_a: Callable = nn.Conv3d,\n",
    "    conv_b_kernel_size: Tuple[int] = (1, 3, 3),\n",
    "    conv_b_stride: Tuple[int] = (1, 2, 2),\n",
    "    conv_b_padding: Tuple[int] = (0, 1, 1),\n",
    "    conv_b_num_groups: int = 1,\n",
    "    conv_b_dilation: Tuple[int] = (1, 1, 1),\n",
    "    conv_b: Callable = nn.Conv3d,\n",
    "    conv_c: Callable = nn.Conv3d,\n",
    "    conv_skip: Callable = nn.Conv3d,\n",
    "    # Norm configs.\n",
    "    norm: Callable = nn.BatchNorm3d,\n",
    "    norm_eps: float = 1e-5,\n",
    "    norm_momentum: float = 0.1,\n",
    "    # Activation configs.\n",
    "    activation_bottleneck: Callable = nn.ReLU,\n",
    "    activation_block: Callable = nn.ReLU,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Residual block. Performs a summation between an identity shortcut in branch1 and a\n",
    "    main block in branch2. When the input and output dimensions are different, a\n",
    "    convolution followed by a normalization will be performed.\n",
    "\n",
    "    ::\n",
    "\n",
    "\n",
    "                                         Input\n",
    "                                           |-------+\n",
    "                                           ↓       |\n",
    "                                         Block     |\n",
    "                                           ↓       |\n",
    "                                       Summation ←-+\n",
    "                                           ↓\n",
    "                                       Activation\n",
    "\n",
    "    Normalization examples include: BatchNorm3d and None (no normalization).\n",
    "    Activation examples include: ReLU, Softmax, Sigmoid, and None (no activation).\n",
    "    Transform examples include: BottleneckBlock.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): input channel size to the bottleneck block.\n",
    "        dim_inner (int): intermediate channel size of the bottleneck.\n",
    "        dim_out (int): output channel size of the bottleneck.\n",
    "        bottleneck (callable): a callable that constructs bottleneck block layer.\n",
    "            Examples include: create_bottleneck_block.\n",
    "        use_shortcut (bool): If true, use conv and norm layers in skip connection.\n",
    "        branch_fusion (callable): a callable that constructs summation layer.\n",
    "            Examples include: lambda x, y: x + y, OctaveSum.\n",
    "\n",
    "        conv_a_kernel_size (tuple): convolutional kernel size(s) for conv_a.\n",
    "        conv_a_stride (tuple): convolutional stride size(s) for conv_a.\n",
    "        conv_a_padding (tuple): convolutional padding(s) for conv_a.\n",
    "        conv_a (callable): a callable that constructs the conv_a conv layer, examples\n",
    "            include nn.Conv3d, OctaveConv, etc\n",
    "        conv_b_kernel_size (tuple): convolutional kernel size(s) for conv_b.\n",
    "        conv_b_stride (tuple): convolutional stride size(s) for conv_b.\n",
    "        conv_b_padding (tuple): convolutional padding(s) for conv_b.\n",
    "        conv_b_num_groups (int): number of groups for groupwise convolution for\n",
    "            conv_b.\n",
    "        conv_b_dilation (tuple): dilation for 3D convolution for conv_b.\n",
    "        conv_b (callable): a callable that constructs the conv_b conv layer, examples\n",
    "            include nn.Conv3d, OctaveConv, etc\n",
    "        conv_c (callable): a callable that constructs the conv_c conv layer, examples\n",
    "            include nn.Conv3d, OctaveConv, etc\n",
    "        conv_skip (callable): a callable that constructs the conv_skip conv layer,\n",
    "        examples include nn.Conv3d, OctaveConv, etc\n",
    "\n",
    "        norm (callable): a callable that constructs normalization layer. Examples\n",
    "            include nn.BatchNorm3d, None (not performing normalization).\n",
    "        norm_eps (float): normalization epsilon.\n",
    "        norm_momentum (float): normalization momentum.\n",
    "\n",
    "        activation_bottleneck (callable): a callable that constructs activation layer in\n",
    "            bottleneck. Examples include: nn.ReLU, nn.Softmax, nn.Sigmoid, and None\n",
    "            (not performing activation).\n",
    "        activation_block (callable): a callable that constructs activation layer used\n",
    "            at the end of the block. Examples include: nn.ReLU, nn.Softmax, nn.Sigmoid,\n",
    "            and None (not performing activation).\n",
    "\n",
    "    Returns:\n",
    "        (nn.Module): resnet basic block layer.\n",
    "    \"\"\"\n",
    "    branch1_conv_stride = tuple(map(np.prod, zip(conv_a_stride, conv_b_stride)))\n",
    "    norm_model = None\n",
    "    if use_shortcut or (\n",
    "        norm is not None and (dim_in != dim_out or np.prod(branch1_conv_stride) != 1)\n",
    "    ):\n",
    "        norm_model = norm(num_features=dim_out, eps=norm_eps, momentum=norm_momentum)\n",
    "\n",
    "    return ResBlock(\n",
    "        branch1_conv=conv_skip(\n",
    "            dim_in,\n",
    "            dim_out,\n",
    "            kernel_size=(1, 1, 1),\n",
    "            stride=branch1_conv_stride,\n",
    "            bias=False,\n",
    "        )\n",
    "        if (dim_in != dim_out or np.prod(branch1_conv_stride) != 1) or use_shortcut\n",
    "        else None,\n",
    "        branch1_norm=norm_model,\n",
    "        branch2=bottleneck(\n",
    "            dim_in=dim_in,\n",
    "            dim_inner=dim_inner,\n",
    "            dim_out=dim_out,\n",
    "            conv_a_kernel_size=conv_a_kernel_size,\n",
    "            conv_a_stride=conv_a_stride,\n",
    "            conv_a_padding=conv_a_padding,\n",
    "            conv_a=conv_a,\n",
    "            conv_b_kernel_size=conv_b_kernel_size,\n",
    "            conv_b_stride=conv_b_stride,\n",
    "            conv_b_padding=conv_b_padding,\n",
    "            conv_b_num_groups=conv_b_num_groups,\n",
    "            conv_b_dilation=conv_b_dilation,\n",
    "            conv_b=conv_b,\n",
    "            conv_c=conv_c,\n",
    "            norm=norm,\n",
    "            norm_eps=norm_eps,\n",
    "            norm_momentum=norm_momentum,\n",
    "            activation=activation_bottleneck,\n",
    "        ),\n",
    "        activation=None if activation_block is None else activation_block(),\n",
    "        branch_fusion=branch_fusion,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "73eabae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth=depth\n",
    "dim_in=stage_dim_in\n",
    "dim_inner=stage_dim_inner\n",
    "dim_out=stage_dim_out\n",
    "bottleneck=bottleneck\n",
    "conv_a_kernel_size=stage_conv_a_kernel_size\n",
    "conv_a_stride=(1, 1, 1)\n",
    "conv_a_padding=[size // 2 for size in stage_conv_a_kernel_size]\n",
    "conv_b_kernel_size=stage_conv_b_kernel_size\n",
    "conv_b_stride=stage_conv_b_stride\n",
    "conv_b_padding=[size // 2 for size in stage_conv_b_kernel_size]\n",
    "conv_b_num_groups=(stage_dim_inner // stage_conv_b_width_per_group)\n",
    "conv_b_dilation=(1, 1, 1)\n",
    "norm=norm\n",
    "activation=activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7c3ad5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d00c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block. Performs a summation between an identity shortcut in branch1 and a\n",
    "    main block in branch2. When the input and output dimensions are different, a\n",
    "    convolution followed by a normalization will be performed.\n",
    "\n",
    "    ::\n",
    "\n",
    "\n",
    "                                         Input\n",
    "                                           |-------+\n",
    "                                           ↓       |\n",
    "                                         Block     |\n",
    "                                           ↓       |\n",
    "                                       Summation ←-+\n",
    "                                           ↓\n",
    "                                       Activation\n",
    "\n",
    "    The builder can be found in `create_res_block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        branch1_conv: nn.Module = None,\n",
    "        branch1_norm: nn.Module = None,\n",
    "        branch2: nn.Module = None,\n",
    "        activation: nn.Module = None,\n",
    "        branch_fusion: Callable = None,\n",
    "    ) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            branch1_conv (torch.nn.modules): convolutional module in branch1.\n",
    "            branch1_norm (torch.nn.modules): normalization module in branch1.\n",
    "            branch2 (torch.nn.modules): bottleneck block module in branch2.\n",
    "            activation (torch.nn.modules): activation module.\n",
    "            branch_fusion: (Callable): A callable or layer that combines branch1\n",
    "                and branch2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        set_attributes(self, locals())\n",
    "        assert self.branch2 is not None\n",
    "\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        if self.branch1_conv is None:\n",
    "            x = self.branch_fusion(x, self.branch2(x))\n",
    "        else:\n",
    "            shortcut = self.branch1_conv(x)\n",
    "            if self.branch1_norm is not None:\n",
    "                shortcut = self.branch1_norm(shortcut)\n",
    "            x = self.branch_fusion(shortcut, self.branch2(x))\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5811b236",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [127]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_res_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdim_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_inner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_inner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbottleneck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottleneck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_a_kernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a_kernel_size\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_a_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_a_padding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a_padding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_b_kernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_kernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_b_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_b_padding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_b_num_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_num_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_b_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_dilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_c\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation_bottleneck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [118]\u001b[0m, in \u001b[0;36mcreate_res_block\u001b[0;34m(dim_in, dim_inner, dim_out, bottleneck, use_shortcut, branch_fusion, conv_a_kernel_size, conv_a_stride, conv_a_padding, conv_a, conv_b_kernel_size, conv_b_stride, conv_b_padding, conv_b_num_groups, conv_b_dilation, conv_b, conv_c, conv_skip, norm, norm_eps, norm_momentum, activation_bottleneck, activation_block)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_shortcut \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     98\u001b[0m     norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (dim_in \u001b[38;5;241m!=\u001b[39m dim_out \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39mprod(branch1_conv_stride) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     99\u001b[0m ):\n\u001b[1;32m    100\u001b[0m     norm_model \u001b[38;5;241m=\u001b[39m norm(num_features\u001b[38;5;241m=\u001b[39mdim_out, eps\u001b[38;5;241m=\u001b[39mnorm_eps, momentum\u001b[38;5;241m=\u001b[39mnorm_momentum)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ResBlock(\n\u001b[1;32m    103\u001b[0m     branch1_conv\u001b[38;5;241m=\u001b[39mconv_skip(\n\u001b[1;32m    104\u001b[0m         dim_in,\n\u001b[1;32m    105\u001b[0m         dim_out,\n\u001b[1;32m    106\u001b[0m         kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    107\u001b[0m         stride\u001b[38;5;241m=\u001b[39mbranch1_conv_stride,\n\u001b[1;32m    108\u001b[0m         bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (dim_in \u001b[38;5;241m!=\u001b[39m dim_out \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39mprod(branch1_conv_stride) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m use_shortcut\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m     branch1_norm\u001b[38;5;241m=\u001b[39mnorm_model,\n\u001b[0;32m--> 113\u001b[0m     branch2\u001b[38;5;241m=\u001b[39m\u001b[43mbottleneck\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim_inner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_inner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_a_kernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a_kernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_a_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_a_padding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_b_kernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_kernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_b_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_b_padding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_b_num_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_num_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_b_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b_dilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_c\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation_bottleneck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    133\u001b[0m     activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m activation_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m activation_block(),\n\u001b[1;32m    134\u001b[0m     branch_fusion\u001b[38;5;241m=\u001b[39mbranch_fusion,\n\u001b[1;32m    135\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mmsign/lib/python3.8/site-packages/pytorchvideo/models/resnet.py:98\u001b[0m, in \u001b[0;36mcreate_bottleneck_block\u001b[0;34m(dim_in, dim_inner, dim_out, conv_a_kernel_size, conv_a_stride, conv_a_padding, conv_a, conv_b_kernel_size, conv_b_stride, conv_b_padding, conv_b_num_groups, conv_b_dilation, conv_b, conv_c, norm, norm_eps, norm_momentum, activation)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_bottleneck_block\u001b[39m(\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Convolution configs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     activation: Callable \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU,\n\u001b[1;32m     40\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Bottleneck block: a sequence of spatiotemporal Convolution, Normalization,\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    and Activations repeated in the following order:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m        (nn.Module): resnet bottleneck block.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     conv_a \u001b[38;5;241m=\u001b[39m \u001b[43mconv_a\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_inner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a_kernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_a_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     norm_a \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m norm(num_features\u001b[38;5;241m=\u001b[39mdim_inner, eps\u001b[38;5;241m=\u001b[39mnorm_eps, momentum\u001b[38;5;241m=\u001b[39mnorm_momentum)\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    111\u001b[0m     act_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m activation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m activation()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "ind = 0\n",
    "block = create_res_block(\n",
    "    dim_in=dim_in if ind == 0 else dim_out,\n",
    "    dim_inner=dim_inner,\n",
    "    dim_out=dim_out,\n",
    "    bottleneck=bottleneck,\n",
    "    conv_a_kernel_size=conv_a_kernel_size[ind],\n",
    "    conv_a_stride=conv_a_stride if ind == 0 else (1, 1, 1),\n",
    "    conv_a_padding=conv_a_padding[ind],\n",
    "    conv_a=conv_a,\n",
    "    conv_b_kernel_size=conv_b_kernel_size,\n",
    "    conv_b_stride=conv_b_stride if ind == 0 else (1, 1, 1),\n",
    "    conv_b_padding=conv_b_padding,\n",
    "    conv_b_num_groups=conv_b_num_groups,\n",
    "    conv_b_dilation=conv_b_dilation,\n",
    "    conv_b=conv_b,\n",
    "    conv_c=conv_c,\n",
    "    norm=norm,\n",
    "    norm_eps=norm_eps,\n",
    "    norm_momentum=norm_momentum,\n",
    "    activation_bottleneck=activation,\n",
    "    activation_block=activation,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmsign",
   "language": "python",
   "name": "mmsign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
