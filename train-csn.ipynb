{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5501fd6",
   "metadata": {},
   "source": [
    "# CSN-Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1b9f5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "764b6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from csn import csn152\n",
    "from torchvision import transforms\n",
    "from video_dataset import VideoFrameDataset, ImglistToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50bc00",
   "metadata": {},
   "source": [
    "## Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53d8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "except:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced16335",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fe3ffb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join(os.getcwd(), 'data/wlasl/rawframes') \n",
    "ann_file_train = os.path.join(os.getcwd(), 'data/wlasl/train_annotations.txt') \n",
    "ann_file_test = os.path.join(os.getcwd(), 'data/wlasl/test_annotations.txt')\n",
    "batch_size = 2\n",
    "\n",
    "# Setting up data transforms\n",
    "train_pipeline = transforms.Compose([\n",
    "        ImglistToTensor(), # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        transforms.Resize(256), # image batch, resize smaller edge to 256\n",
    "        transforms.RandomResizedCrop((224, 224)), # image batch, center crop to square 224x224\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "test_pipeline = transforms.Compose([\n",
    "        ImglistToTensor(), # list of PIL images to (FRAMES x CHANNELS x HEIGHT x WIDTH) tensor\n",
    "        transforms.Resize(256),  # image batch, resize smaller edge to 256\n",
    "        transforms.CenterCrop((224, 224)),  # image batch, center crop to square 224x224\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "# Setting up datasets\n",
    "train_dataset = VideoFrameDataset(\n",
    "    root_path=data_root,\n",
    "    annotationfile_path=ann_file_train,\n",
    "    num_segments=5,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=train_pipeline,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = VideoFrameDataset(\n",
    "    root_path=data_root,\n",
    "    annotationfile_path=ann_file_test,\n",
    "    num_segments=5,\n",
    "    frames_per_segment=1,\n",
    "    imagefile_template='img_{:05d}.jpg',\n",
    "    transform=test_pipeline,\n",
    "    test_mode=True\n",
    ")\n",
    "\n",
    "# Setting up dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=4,\n",
    "                                    pin_memory=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=2,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=4,\n",
    "                                    pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a0b6c1c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3, 224, 224])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing dataloader\n",
    "dataiter = iter(train_loader)\n",
    "get = next(dataiter)\n",
    "get[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441fbcd",
   "metadata": {},
   "source": [
    "## Set up model, loss and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b457e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSN model\n",
    "csn = csn152(num_classes=400)\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify loss function\n",
    "optimizer = torch.optim.Adam(csn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634aa03",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b2765c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.644 | Accuracy: 0.000\n",
      "Epoch: 1 \tTraining Loss: 12.320577\n",
      "Epoch: 2 \tTraining Loss: 6.218439\n",
      "Epoch: 3 \tTraining Loss: 5.693654\n",
      "Epoch: 4 \tTraining Loss: 5.565608\n",
      "Epoch: 5 \tTraining Loss: 5.368568\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "interval = 10 # For checkpoints and validation\n",
    "\n",
    "losses = []\n",
    "eval_accu = []\n",
    "eval_losses=[]\n",
    "\n",
    "csn.to(device)\n",
    "csn.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Reset train loss\n",
    "    train_loss = 0.0\n",
    "    for video_batch, targets in train_loader:\n",
    "        # Move data to device\n",
    "        video_batch, targets = video_batch.to(device), targets.to(device)\n",
    "        \n",
    "        # batch_size, channels, n_frames, h, w\n",
    "        predictions = csn(video_batch.view(video_batch.size(0), 3, 5, 224, 224))\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        # Backpropagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate batch loss\n",
    "        train_loss += loss.item()*video_batch.size(0)\n",
    "\n",
    "    # Check for interval to validate and save checkpoints\n",
    "    if epoch%interval==0:\n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        with torch.no_grad():\n",
    "            for video_batch, targets in test_loader:\n",
    "                video_batch, targets = video_batch.to(device), targets.to(device)\n",
    "\n",
    "                predictions = csn(video_batch.view(video_batch.size(0), 3, 5, 224, 224))\n",
    "\n",
    "                loss = loss_fn(predictions, targets)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                _, predicted = predictions.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        test_loss=running_loss/len(test_loader)\n",
    "        accu=100.*correct/total\n",
    "\n",
    "        eval_losses.append(test_loss)\n",
    "        eval_accu.append(accu)\n",
    "        print('Test Loss: %.3f | Accuracy: %.3f'%(test_loss,accu)) \n",
    "\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    losses.append(train_loss)\n",
    "    print(f'Epoch: {epoch+1} \\tTraining Loss: {train_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e64b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1ce4181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*5*2*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b9e9226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(0, 60)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "615a5be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1],\n",
       "         [ 2,  3]],\n",
       "\n",
       "        [[ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11]]],\n",
       "\n",
       "\n",
       "       [[[12, 13],\n",
       "         [14, 15]],\n",
       "\n",
       "        [[16, 17],\n",
       "         [18, 19]],\n",
       "\n",
       "        [[20, 21],\n",
       "         [22, 23]]],\n",
       "\n",
       "\n",
       "       [[[24, 25],\n",
       "         [26, 27]],\n",
       "\n",
       "        [[28, 29],\n",
       "         [30, 31]],\n",
       "\n",
       "        [[32, 33],\n",
       "         [34, 35]]],\n",
       "\n",
       "\n",
       "       [[[36, 37],\n",
       "         [38, 39]],\n",
       "\n",
       "        [[40, 41],\n",
       "         [42, 43]],\n",
       "\n",
       "        [[44, 45],\n",
       "         [46, 47]]],\n",
       "\n",
       "\n",
       "       [[[48, 49],\n",
       "         [50, 51]],\n",
       "\n",
       "        [[52, 53],\n",
       "         [54, 55]],\n",
       "\n",
       "        [[56, 57],\n",
       "         [58, 59]]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = x.reshape(5, 3, 2, 2)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0eaf81b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1],\n",
       "         [ 2,  3]],\n",
       "\n",
       "        [[ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15]],\n",
       "\n",
       "        [[16, 17],\n",
       "         [18, 19]]],\n",
       "\n",
       "\n",
       "       [[[20, 21],\n",
       "         [22, 23]],\n",
       "\n",
       "        [[24, 25],\n",
       "         [26, 27]],\n",
       "\n",
       "        [[28, 29],\n",
       "         [30, 31]],\n",
       "\n",
       "        [[32, 33],\n",
       "         [34, 35]],\n",
       "\n",
       "        [[36, 37],\n",
       "         [38, 39]]],\n",
       "\n",
       "\n",
       "       [[[40, 41],\n",
       "         [42, 43]],\n",
       "\n",
       "        [[44, 45],\n",
       "         [46, 47]],\n",
       "\n",
       "        [[48, 49],\n",
       "         [50, 51]],\n",
       "\n",
       "        [[52, 53],\n",
       "         [54, 55]],\n",
       "\n",
       "        [[56, 57],\n",
       "         [58, 59]]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape = video.reshape(3, 5, 2, 2)\n",
    "reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89cfdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first image rgb\n",
    "array([[[[ 0,  1],\n",
    "         [ 2,  3]],\n",
    "\n",
    "        [[ 4,  5],\n",
    "         [ 6,  7]],\n",
    "\n",
    "        [[ 8,  9],\n",
    "         [10, 11]]],"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmsign",
   "language": "python",
   "name": "mmsign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
